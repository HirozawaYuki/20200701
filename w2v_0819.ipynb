{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v_0819.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1UeIVI-mZWwWQCOHsmrL7xEkd2Ld89VgR",
      "authorship_tag": "ABX9TyMvpsW1EKgYy3xlm0JP5kCI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjHBqthJ6GLY"
      },
      "source": [
        "スポーツ名予測とマスク単語予測を行うネットワークの実験用.\n",
        "本プログラムではマスク単語を行う単語の条件を品詞情報によって決定する.\n",
        "学習後、似ている文章の特徴量を更に近づけ、似ていない文章の特徴量を遠ざける再学習を行う(Contrastive lossの使用)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkOcDuiUKDUa",
        "outputId": "ecc53499-e915-4e1f-ba24-84d0a45ec514"
      },
      "source": [
        "%ls ./drive/MyDrive/'Colab Notebooks'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access './drive/MyDrive/Colab Notebooks': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq_zrdQQNlRg",
        "outputId": "de4ecad9-75bc-4621-a960-f76cffb9bf2a"
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "0 upgraded, 21 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 3,877 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Fetched 3,877 kB in 1s (3,416 kB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 148486 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.14)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.14)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n",
            "The following packages will be REMOVED:\n",
            "  libnvidia-common-460{u} \n",
            "0 packages upgraded, 11 newly installed, 1 to remove and 40 not upgraded.\n",
            "Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n",
            "Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.3 MB in 2s (14.1 MB/s)\n",
            "(Reading database ... 148945 files and directories currently installed.)\n",
            "Removing libnvidia-common-460 (460.91.03-0ubuntu1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 148940 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "                            \n",
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JjN0rTba7lY",
        "outputId": "1cbaef53-52d0-492d-f7e8-1f607f1d0a23"
      },
      "source": [
        "# !pip install mecab-python3==0.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mecab-python3==0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/e9/bbf5fc790a2bedd96fbaf47a84afa060bfb0b3e0217e5f64b32bd4bbad69/mecab-python3-0.7.tar.gz (41kB)\n",
            "\r\u001b[K     |███████▉                        | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 20kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 30kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
            "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python3: filename=mecab_python3-0.7-cp37-cp37m-linux_x86_64.whl size=156587 sha256=54f7aae5adad170c69555f208642ac6f4c9f218cd1427afa34e0e78bde29dd53\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/07/3a/5f22ccc9f381f3bc01fa023202061cd1e0e9af855292f005dd\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6mO8LitQCcR",
        "outputId": "bb461f18-c199-4f2f-a44d-2e59ba4765b4"
      },
      "source": [
        "pip install unidic-lite"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4 MB 42 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658836 sha256=4bba3ca393db49e781eb3e8a3d00eb3b283311ab403316b832e3b435896c6007\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/69/b1/112140b599f2b13f609d485a99e357ba68df194d2079c5b1a2\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_E79aZ-PH8V"
      },
      "source": [
        "import MeCab\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from tensorflow.keras.layers import (Embedding, Dense, GlobalAveragePooling1D, Masking,\n",
        "                                     GlobalAveragePooling2D, Conv2D, Multiply,\n",
        "                                     Lambda, Input, LSTM, Bidirectional, concatenate, Dropout, Flatten, Reshape)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import copy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwWffxvfPRFz"
      },
      "source": [
        "dim_embedding = 16\n",
        "dim_z = 100 # LSTMの特徴ベクトル次元数\n",
        "epochs_w2v = 100\n",
        "epochs_lstm = 1000\n",
        "batchsize_w2v = 128\n",
        "batchsize_lstm = 128\n",
        "\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCqPUU_dTfMK",
        "outputId": "4bdea2cc-6754-46e0-9c8b-0ca3fefacaf7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5tVD92yPcFn",
        "outputId": "35944843-d265-4895-e913-d389c0de6878"
      },
      "source": [
        "%ls /drive/MyDrive/'Colab Notebooks'\n",
        "%ls /drive/MyDrive/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'chapter2.4to2.5.ipynb のコピー'\n",
            "'chapter2.4to2.5.ipynb のコピー (1)'\n",
            "'Chapter4-5~4-7.ipynb のコピー'\n",
            "'chapter5_1-2_配布用.ipynb のコピー'\n",
            "'chapter5_1-2_配布用.ipynb のコピー (1)'\n",
            "'chapter6-1_6-2.ipynb のコピー'\n",
            "'chapter6-3~7-4.ipynb のコピー'\n",
            " GAN_practice.ipynb\n",
            " Practice3.ipynb\n",
            " Untitled\n",
            " Untitled0.ipynb\n",
            " w2v_0518.ipynb\n",
            " w2v_0806.ipynb\n",
            " w2v_0819.ipynb\n",
            "'共通ゼミchapter3-1,3-2  のコピー'\n",
            "'共通ゼミ_ミツカワCP3-3,4.ipynb のコピー'\n",
            "'共通ゼミ_ミツカワCP3-3,4.ipynb のコピー (1)'\n",
            "'実践ゼミ_tweet取得プログラム '\n",
            " 実践ゼミ_tweet取得_文章特徴量取得プログラム\n",
            " \u001b[0m\u001b[01;34mトーク\u001b[0m/\n",
            " 20210621_Chapter4_廣澤_配布用.ipynb\n",
            " 2021_共通ゼミ_廣澤_配布用.pptx\n",
            " clustering.csv\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            " \u001b[01;34mColab_Notebooks\u001b[0m/\n",
            " Example_1.csv\n",
            " \u001b[01;34mpytorch-openpose\u001b[0m/\n",
            " pytorch_openpose.ipynb\n",
            "'○参考用語集(ver3).gdoc'\n",
            " 実践プログラミングゼミ４班中間発表.gslides\n",
            " 本_候補.docx\n",
            " 研究時間管理表.gsheet\n",
            " 領収書.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTZbDdXlQ0bG",
        "outputId": "bc770904-e5c8-445c-d6a8-8f76b1492303"
      },
      "source": [
        "data_tmp = pd.read_csv('/drive/MyDrive/Example_1.csv')\n",
        "print(data_tmp)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    番号 スポーツ                                                 記事\n",
            "0    1   野球  インターネットサイト「YouTube」の球団公式チャンネルが話題になっている。17日にはキャ...\n",
            "1    2   野球  阪神は新型コロナウイルスの感染拡大でシーズン開幕が延期される中、野球以外でファンに向けて何が...\n",
            "2    3   野球  企画発案から撮影に編集…。球団にとっても大きな負担となるが、矢野監督は「ファンがこういうのを...\n",
            "3    4   野球  「練習試合だったけど、森は4打数3安打。（公式記録に残らず）かわいそうだったね。でも彼が求め...\n",
            "4    5   野球  広島大瀬良大地投手（28）が、当初開幕予定だった中日戦で公式戦さながらに今年最長の7回を投げ...\n",
            "..  ..  ...                                                ...\n",
            "65  66  ゴルフ  16日、自身のインスタグラムを更新。河本は「ディフェンディングチャンピオンとして、またホステ...\n",
            "66  67  ゴルフ  大逆転での東京五輪出場を目指す河本は、今季から米ツアーに参戦していたが、メジャー初戦となる4...\n",
            "67  68  ゴルフ  全米プロゴルフ協会（PGA）は13日、開催中の米ツアー、ザ・プレーヤーズ選手権の第2ラウンド...\n",
            "68  69  ゴルフ  12日に始まったザ・プレーヤーズ選手権は、第1ラウンドはギャラリーを入れて開催した。3シーズ...\n",
            "69  70  ゴルフ  その好発進も幻と化した。第1日終了後に第2ラウンド以降は無観客で行うと発表していたPGAは、...\n",
            "\n",
            "[70 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoogYAD2Q_hH",
        "outputId": "1b62a745-5328-492c-cb29-c017d311b34f"
      },
      "source": [
        "# import os\n",
        "# os.chdir('/drive/MyDrive/')\n",
        "# path = os.getcwd()\n",
        "\n",
        "# print(path)\n",
        "# import csv\n",
        "# csvfile = open('/drive/MyDrive/Example_1.csv')\n",
        "\n",
        "# reader = csv.DictReader(csvfile)\n",
        "\n",
        "# for row in reader:\n",
        "#   print(row)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/drive/MyDrive\n",
            "OrderedDict([('\\ufeff番号', '1'), ('スポーツ', '野球'), ('記事', 'インターネットサイト「YouTube」の球団公式チャンネルが話題になっている。17日にはキャプテンの糸原が練習中に井上打撃コーチにドッキリを仕掛ける映像をアップ。18日には小型カメラを頭に装着した元外野手の緒方凌介広報が、新助っ人ボーアの強烈な打球キャッチに挑戦している（詳細は球団の公式チャンネルで）。井上コーチのドッキリ映像は、わずか2日で再生回数15万回を超えた。')])\n",
            "OrderedDict([('\\ufeff番号', '2'), ('スポーツ', '野球'), ('記事', '阪神は新型コロナウイルスの感染拡大でシーズン開幕が延期される中、野球以外でファンに向けて何ができるかをチームとして検討してきた。3月12日の練習前には監督、選手、関係者が室内練習場で緊急ミーティングを実施。そこで出てきたのは今年2月に開設した公式チャンネルの活用だった。')])\n",
            "OrderedDict([('\\ufeff番号', '3'), ('スポーツ', '野球'), ('記事', '企画発案から撮影に編集…。球団にとっても大きな負担となるが、矢野監督は「ファンがこういうのを見てみたいっていうのを含めて、何かを（SNSなどを通じて）発信していきたい」と話していた。ちなみにドッキリ企画の発案者は矢野監督。2軍戦の視察のために、その瞬間を見ることができなかったが、作戦成功にご満悦だった。')])\n",
            "OrderedDict([('\\ufeff番号', '4'), ('スポーツ', '野球'), ('記事', '「練習試合だったけど、森は4打数3安打。（公式記録に残らず）かわいそうだったね。でも彼が求めているものはシンプルにできているんじゃないか。先発した浜屋は、もっと大胆に投げてほしかったね」')])\n",
            "OrderedDict([('\\ufeff番号', '5'), ('スポーツ', '野球'), ('記事', '広島大瀬良大地投手（28）が、当初開幕予定だった中日戦で公式戦さながらに今年最長の7回を投げ、打席にも初めて立った。6回まで単打2本で無失点。7回に不運な当たりや浮いた球を捉えられて失点し、7回7安打4失点だった。それでも「僕の中では結果ほど悲観するほどではないと思っている」と前を向いた。')])\n",
            "OrderedDict([('\\ufeff番号', '6'), ('スポーツ', '野球'), ('記事', '1回は直球と得意球のカットボールで押すも、2回から組み立てを変えてカーブやフォークボールを多投。「あまりカットボールを使わずに真っすぐとカーブ、フォークで抑えられた。違ったバリエーション。配球にしてもいいものが見せられた。今後につながっていくんじゃないかなと思います」。2回はフォーク、スライダー、真っすぐといずれも違う決め球で阿部、平田、京田を3者連続三振。投球の幅が広がる可能性を感じた。')])\n",
            "OrderedDict([('\\ufeff番号', '7'), ('スポーツ', '野球'), ('記事', '前回13日のソフトバンク戦での5回4失点から修正と改善が見られ、2戦続けて101球とスタミナ面の不安はない。加えて、現時点で最短の開幕4月10日でも当たる中日の目先を変える餌をまくこともできた。バッテリーを組んだ会沢は「その意味合いも当然ある。前回の投球を踏まえて、大地といろいろ話をして組み立てられた」と振り返った。')])\n",
            "OrderedDict([('\\ufeff番号', '8'), ('スポーツ', '野球'), ('記事', '今年初の実戦打撃では5回1死一塁でバスターを決め、内野安打。実りある試合で弾みをつけた右腕は「あとは微調整や、やりたいことに集中できる」と先を見据えた。今後は1度登板機会を空け、次回は4月第1週の週末を予定する。そこから定められた開幕日に合わせていく。')])\n",
            "OrderedDict([('\\ufeff番号', '9'), ('スポーツ', '野球'), ('記事', '巨人阿部慎之助2軍監督が20日、41歳の誕生日を迎えた。ジャイアンツ球場で行われた2軍練習前には、育成の笠井を中心にバースデーソングで祝われた。')])\n",
            "OrderedDict([('\\ufeff番号', '10'), ('スポーツ', '野球'), ('記事', 'ブルペンデーで、ヤクルトの若手投手陣がアピールした。')])\n",
            "OrderedDict([('\\ufeff番号', '11'), ('スポーツ', '野球'), ('記事', '先発の2年目清水は2回を無失点。3番手のドラフト4位大西は2回を完璧に抑えた。9回は、ソフトバンクの育成から新加入した左腕長谷川がわずか7球で3者凡退。')])\n",
            "OrderedDict([('\\ufeff番号', '12'), ('スポーツ', '野球'), ('記事', '高津監督は、長谷川について「右打者に対して（の投球を）見たかった。らしさを出してくれている」と評価した。')])\n",
            "OrderedDict([('\\ufeff番号', '13'), ('スポーツ', '野球'), ('記事', '巨人新外国人のサンチェス投手が、21日の練習試合DeNA戦に先発する。')])\n",
            "OrderedDict([('\\ufeff番号', '14'), ('スポーツ', '野球'), ('記事', '約2週間前から杉内2軍投手コーチらのアドバイスを受け、ブルペンで10球ごとに新球へチェンジする方法を実践。日本のボールに慣れるための改善策の1つだ。')])\n",
            "OrderedDict([('\\ufeff番号', '15'), ('スポーツ', '野球'), ('記事', '日本流を落とし込みながら、投球では「これまでと同じアプローチでやっていくよ」と昨季韓国で17勝を挙げた力で打者を料理する。')])\n",
            "OrderedDict([('\\ufeff番号', '16'), ('スポーツ', '野球'), ('記事', 'プロ野球は本来の開幕日だった3月20日から練習試合が始まっている。')])\n",
            "OrderedDict([('\\ufeff番号', '17'), ('スポーツ', '野球'), ('記事', '21日は守護神から先発に転向した楽天松井の登板などに注目だ。')])\n",
            "OrderedDict([('\\ufeff番号', '18'), ('スポーツ', '野球'), ('記事', '阪神福留孝介外野手（42）が本来の開幕予定日だった20日、ヤクルトとの練習試合（神宮）でハッスル猛打賞を決めた。')])\n",
            "OrderedDict([('\\ufeff番号', '19'), ('スポーツ', '野球'), ('記事', '2回に左中間へ激走二塁打を放つと、その後も中前、右前と全方位に打ち分けた。新型コロナウイルスの感染拡大余波で開幕日が決まらず、モチベーション維持が難しい日々が続くが球界最年長は不動心。「今日が開幕」という強い気持ちを体現し、チームを叱咤（しった）した。')])\n",
            "OrderedDict([('\\ufeff番号', '20'), ('スポーツ', '野球'), ('記事', 'DeNA浜口遥大投手（25）が20日、結婚したことを発表した。\\n相手の恵理子さん（25）は東京出身の会社員で、女優仲里依紗に似た美人。大学時代に知り合い、約4年の交際期間を経て、昨年9月の恵理子さんの誕生日に横浜みなとみらいでプロポーズ。今月中旬に婚姻届を提出した。')])\n",
            "OrderedDict([('\\ufeff番号', '21'), ('スポーツ', '野球'), ('記事', '知人に描いてもらった2人の似顔絵を手にした左腕は「すごく明るくて前向きなところは、僕にとってもありがたいです。うまくいかない中でもポジティブに支えてくれました。得意料理？\\u3000何を作ってもおいしいです」とのろけた。近く同居を始める予定だという。')])\n",
            "OrderedDict([('\\ufeff番号', '22'), ('スポーツ', '野球'), ('記事', '浜口は神奈川大から16年ドラフト1位で入団。1年目に10勝を挙げチームの日本シリーズ進出に貢献したが、ここ2年は1ケタ勝利に終わった。4年目の今季も開幕ローテーション入りが確実な左腕は「妻の家族も含めて、みんなの期待を受けることになる。今年は開幕も遅れて、五輪もあったりしますし、やっぱり特別な1年にしたいです」と言葉に力を込めた。「ハマのハマちゃん」が自覚も新たに、さらなる飛躍を目指す。')])\n",
            "OrderedDict([('\\ufeff番号', '23'), ('スポーツ', 'サッカー'), ('記事', '日本サッカー協会（JFA）は20日、田嶋幸三会長（62）が14日に新型コロナウイルスを発症したことを受け、感染拡大防止策を新たに講じることを発表した。')])\n",
            "OrderedDict([('\\ufeff番号', '24'), ('スポーツ', 'サッカー'), ('記事', '保健所の指導の下、最長23日まで東京・文京区のJFAハウス内の消毒作業を実施。27日まで役職員は在宅勤務を継続し、来館者の立ち入りも禁止する。また、田嶋会長も参加していた14日の理事会出席者の行動履歴を確認したうえで保健所から指導を受けており、現時点で発熱や風邪の症状を訴える人はいないことも公表した。田嶋会長の症状は悪化はしておらず、検査と治療を続けている模様だ。')])\n",
            "OrderedDict([('\\ufeff番号', '25'), ('スポーツ', 'サッカー'), ('記事', 'Jリーグの村井満チェアマン（60）が19日、経営難に陥ったサガン鳥栖と協議中であることを認めた。')])\n",
            "OrderedDict([('\\ufeff番号', '26'), ('スポーツ', 'サッカー'), ('記事', 'ウェブによる実行委員会後の会見で「PL（損益計算書）上の問題とキャッシュフローの議論がありまして。このあたりを精査しているところ。現在、クラブと担当者レベルで意見交換している」と話した。')])\n",
            "OrderedDict([('\\ufeff番号', '27'), ('スポーツ', 'サッカー'), ('記事', '村井チェアマンが鳥栖経営危機の報告を受けたのは開幕（2月21日）直前で、Jリーグ事務局担当者によると「ライセンス制度のルールに基づいて1月末、定期的な話の中で資金繰りの話になった」という。その後、Jリーグ担当者と鳥栖の間で協議を重ねている。')])\n",
            "OrderedDict([('\\ufeff番号', '28'), ('スポーツ', 'サッカー'), ('記事', '鳥栖は昨季、5億円以上の赤字を出し、今季も資金難に苦しんでいる。仮に「リーグ戦安定開催融資制度」を適用しようにも、返却のめどが立たないと、Jリーグからの融資も難しくなる。場合によってはクラブライセンスにも影響し、2部か3部降格や除名などの可能性も出てくる。')])\n",
            "OrderedDict([('\\ufeff番号', '29'), ('スポーツ', 'サッカー'), ('記事', '北海道コンサドーレ札幌野々村芳和社長（47）が19日、新型コロナウイルスの影響でクラブが被る損失について、約5億円を見込んでいると明かした。「今年を乗り切れたとしても、来年以降は相当大変なことになる。クラブとして、せっかく成長曲線で来ていたけど、しょうがない」と話した。')])\n",
            "OrderedDict([('\\ufeff番号', '30'), ('スポーツ', 'サッカー'), ('記事', 'Jリーグ公式戦は中断中。現時点で札幌のホームゲームは、札幌ドームで週末に予定されていた2試合が延期されている。平日での札幌厚別開催に振り替えられる可能性もある。会場変更に伴う観客数の減少による収入減はもちろん、会場のアルコール消毒費、これまで高齢者も多くいたボランティア運営スタッフを採用せず、アルバイトスタッフにした時の人件費など、新たな支出が想定される。\\u300021日の鹿島との練習試合には、別メニューをのぞく全選手が帯同する。実戦感覚を失わないための道外遠征も、リーグ中断に伴い発生した支出だ。')])\n",
            "OrderedDict([('\\ufeff番号', '31'), ('スポーツ', 'サッカー'), ('記事', 'この日、J1の降格チームなしが決まった。だが今季の戦い方に変わりはない。「現場の目標はACL。そこに向かっていけばいい」と同社長。日程は過密となるが「連戦が増えれば増えるほど、いろんな選手にチャンスがあるわけだから、前向きに考えて欲しいよね。コロナのこの影響があったから、選手として一皮むけたとか、チャンスを得られたとかいう選手が出てくることがうれしい」と期待していた。')])\n",
            "OrderedDict([('\\ufeff番号', '32'), ('スポーツ', 'サッカー'), ('記事', 'J1湘南ベルマーレは、今季のJ1とJ2で降格なしとの特例が適用されることを「刺激」に変えた。')])\n",
            "OrderedDict([('\\ufeff番号', '33'), ('スポーツ', 'サッカー'), ('記事', '19日のJリーグ臨時実行委員会での決定から一夜明けた20日、チームは平塚市内で非公開練習を消化。昨季はプレーオフの末、J1残留を決めた経緯もあり、MF鈴木冬一（19）は「昨年、プレーオフの緊張感は経験した。今年は降格の危機感ではなく、逆に優勝争いできる緊張感をもっと味わいたい」と上位争いへの意識を高めた。')])\n",
            "OrderedDict([('\\ufeff番号', '34'), ('スポーツ', 'サッカー'), ('記事', 'またMF斉籐未月（21）は「毎年、下位や降格争いしてしまっているけれど、いつまでも下にいるのは面白くない。観客もサポーターも緊張感ある試合を求めていると思う。上位でそういう試合も増やしたい」と気を引き締めた。')])\n",
            "OrderedDict([('\\ufeff番号', '35'), ('スポーツ', 'サッカー'), ('記事', '前監督の退任、そして台風による練習場冠水などの苦境を乗り越え、J1残留を勝ち取った浮嶋敏監督（52）は「（降格なしは）昨年、ボクが引き継いだ状態の時だったら喜ぶかもしれないけれど。1試合しか終わっていないし、今だと喜びようもないかな」とジョークで反応。公式戦再開後の試合展開に変化があると指摘し「リーグ全体でいえば、よりアグレッシブに戦うチームが増える。何より得点が増える。終盤に采配、選手の気持ちがかなり違うと思います。より攻撃的にできる」と話していた。')])\n",
            "OrderedDict([('\\ufeff番号', '36'), ('スポーツ', 'サッカー'), ('記事', '新型コロナウイルスの感染拡大の影響で2月下旬から公式戦中断中のJリーグは19日に臨時実行委員会を実施し、今季はJ1とJ2で降格なしの特例の適用を決定した。')])\n",
            "OrderedDict([('\\ufeff番号', '37'), ('スポーツ', 'サッカー'), ('記事', 'J2、J3ともライセンス上問題のない上位2チームが自動昇格。今季18チームのJ1は来季20チームで臨み、22年シーズンは4チームを降格として18チームに戻すことになる見込みだ。')])\n",
            "OrderedDict([('\\ufeff番号', '38'), ('スポーツ', 'サッカー'), ('記事', 'Jリーグは4月3日の再開を目指しているが、安全面も考慮して4月17日や5月のゴールデンウイーク明けまでずれこむ可能性もある。五輪期間中や国際Aマッチデーなどに試合を行う変則日程となれば、主力が代表招集で不在になったり敵地での連戦が続くなど公平性を保てない場面も出てくる。村井チェアマンは「ある意味では不公平性を飲み込んででも試合は続けていこうという目線合わせをした。選手が目標に向かって頑張る姿を推奨したいし結果を残した選手に報いていきたい」と現時点で大会方式の変更を決めた理由を説明した。')])\n",
            "OrderedDict([('\\ufeff番号', '39'), ('スポーツ', 'サッカー'), ('記事', '公式戦として成立する消化試合数は、全日程の75％以上案が選択肢に挙がっている。賞金の分配法や22年シーズンの各カテゴリーのチーム数など細かな事案については、再開か再延期かも含め、25日の臨時実行委員会で協議する。')])\n",
            "OrderedDict([('\\ufeff番号', '40'), ('スポーツ', '相撲'), ('記事', '白鵬が朝乃山の大関とりの壁となって立ちはだかった。右を差し込み、左上手で相手に右を差させない厳しい相撲で押し出した。')])\n",
            "OrderedDict([('\\ufeff番号', '41'), ('スポーツ', '相撲'), ('記事', 'しかし、取組後は報道陣の呼びかけに応じず、取材エリアをなぞのスルー。再びトップに並び、14日目は同じ2敗の碧山との対戦が組まれた。勝負どころに向けての気合か、無言を貫く姿は気迫を漂わせた。')])\n",
            "OrderedDict([('\\ufeff番号', '42'), ('スポーツ', '相撲'), ('記事', '大関とりの関脇朝乃山（26＝高砂）が、横綱白鵬に押し出しで負けて3敗目を喫した。14日目は2敗でトップタイの横綱鶴竜と対戦。大関昇進目安の「三役で3場所33勝」に必要な12勝に向けて数字上は後がない状況だが、勝てば大関昇進の機運が高まる可能性はある。自身2度目となる優勝のためにも、白星で望みをつなげたい。白鵬、鶴竜と平幕碧山が2敗で首位に並んだ。')])\n",
            "OrderedDict([('\\ufeff番号', '43'), ('スポーツ', '相撲'), ('記事', '報道陣が大勢待ち受ける支度部屋外のミックスゾーンに、朝乃山はすがすがしい表情を浮かべて歩み寄ってきた。「今日から3日間が大事と思っていた。横綱戦も2回あると思っていたので、自分の相撲を取りきるつもりだった。でも取り切れなかったのは弱い自分がいた」。大関昇進の目安の今場所12勝に向けて、後がない状況にもかかわらず率直な言葉を吐き出した。')])\n",
            "OrderedDict([('\\ufeff番号', '44'), ('スポーツ', '相撲'), ('記事', '横綱に何もさせてもらえなかった。立ち合いで得意の右を差せず、右四つを許して上体を起こされると、一気に土俵際へ。逆転を狙って右に動きながら振り払うと、同時に土俵を割ったかのように見えたが、自分の左足がワンテンポ速く土俵の外に落ちた。「集中力、厳しさが足りなかった。土俵際で回っておけばと思った。そこも弱いところです」と弱点を自覚した。')])\n",
            "OrderedDict([('\\ufeff番号', '45'), ('スポーツ', '相撲'), ('記事', '大関昇進に向けて、試練の3番勝負の最初でつまずいた。14日目は不戦勝を除いて過去1勝1敗の鶴竜と対戦する。通常なら午前中に行われる審判部による翌日の取組編成会議が、この日は優勝争いを見据えて全取組終了後となったため、朝乃山が会場を引き揚げる際にはまだ14日目の対戦相手は分からず。それでも「先は考えずに1日一番、自分の相撲を取りきって頑張ります」と集中した。')])\n",
            "OrderedDict([('\\ufeff番号', '46'), ('スポーツ', '相撲'), ('記事', '場所前には行きつけの整骨院に通い、曲がっていた背骨を矯正した。そのかいあってか、今場所も力強さを見せている。泣いても笑っても、今場所は残り二番。ビシッと2連勝で締めくくり、夢の大関昇進をかなえる。')])\n",
            "OrderedDict([('\\ufeff番号', '47'), ('スポーツ', '相撲'), ('記事', '蜂窩（ほうか）織炎による発熱で途中休場し、11日目から再出場した西前頭15枚目千代丸（28＝九重）が、復帰後初めて報道陣の取材に対応し、自身の状態について説明した。この日は西前頭10枚目栃煌山を引き落としで破り、7勝目を挙げて幕内残留に大きく前進。11日目、12日目は報道陣が待つミックスゾーンに立ち止まらなかったが、この日は足を止めた。')])\n",
            "OrderedDict([('\\ufeff番号', '48'), ('スポーツ', '相撲'), ('記事', '一時は40度まで熱が上がった体調について「もう戻った。体は元気です」と話した。この日も右足に足袋、左のふくらはぎにテーピングを施したが「意外と（足に）痛みはない。休場する前も調子は良かった。その感覚も残っている」とアピールした。熱は36度5分まで下がったという。')])\n",
            "OrderedDict([('\\ufeff番号', '49'), ('スポーツ', '相撲'), ('記事', '休場が続けば十両陥落の危機だったが「リンパが腫れて痛かったから蜂窩（ほうか）織炎だと思った。いつでも出られるような気持ちでいた」と淡々。「一応、病院で点滴を打つ」と、足早に会場を引き揚げた。')])\n",
            "OrderedDict([('\\ufeff番号', '50'), ('スポーツ', '相撲'), ('記事', '1番でも多くの好取組を－。日本相撲協会の審判部は14日目の幕内取組を、13日目の全取組終了後に編成会議を開いて決めた。')])\n",
            "OrderedDict([('\\ufeff番号', '51'), ('スポーツ', '相撲'), ('記事', '通常、翌日の取組は前日午前中の取組編成会議で決めることになっている。ただ昨年から、優勝争いや三賞選考、勝ち越しや負け越しに絡むことなどを考慮し、千秋楽の取組は14日目の全取組終了後に決めることが、ほぼ慣例となっていた。これを今場所は、14日目の取組もギリギリまで優勝争いを見据え、結果が出た時点で取組を編成することになった。')])\n",
            "OrderedDict([('\\ufeff番号', '52'), ('スポーツ', '相撲'), ('記事', '12日目終了時点で単独トップに立っていた平幕下位の碧山（33＝春日野）が敗れ、トップは2敗で3人が並び、1差の3敗に3人がつけるという再び大混戦になった。この結果を受けて14日目には、結びの一番は想定通りに横綱鶴竜－関脇朝乃山戦が組まれたが、その一番前は横綱白鵬－碧山の2敗対決が組まれた。番付優先の通常なら白鵬－貴景勝の横綱、大関戦が組まれるが、6勝7敗の貴景勝の不振もあり、優勝争いを優先した格好だ。千秋楽は白鵬－鶴竜戦の横綱対決が確実に組まれると思われる。このため貴景勝は、横綱戦2つのうち白鵬戦が今場所、なくなることが確実となった。')])\n",
            "OrderedDict([('\\ufeff番号', '53'), ('スポーツ', '相撲'), ('記事', '審判部の柔軟ともいえる対応に八角理事長（元横綱北勝海）は「いい取組を作るために審判部がね（考えたこと）。横綱、大関が少ない（3人）ということもある。普通なら横綱、大関陣で優勝争いだから」と、いわゆる「割崩し」に理解を示した。今場所は史上初の無観客開催となっただけに、最後までより優勝争いを面白く－という意図がかいま見える判断だ。')])\n",
            "OrderedDict([('\\ufeff番号', '54'), ('スポーツ', '相撲'), ('記事', '1場所での十両復帰を目指したものの、6番相撲で負け越しが決まり、場所後の再十両の可能性が消えた東幕下2枚目豊ノ島（36＝時津風）が、今場所最後の7番相撲に登場。幕内上位などで過去15度の対戦（11勝4敗）がある西幕下6枚目の豊響（35＝境川）と対戦した。わずかに右をのぞかせたが、立ち合いから圧力負けし、その右を強烈におっつけられズルズル後退。真後ろにはたいたが、左足を踏み越し押し出しで敗れた。復活をかけた場所は2勝5敗で終わった。')])\n",
            "OrderedDict([('\\ufeff番号', '55'), ('スポーツ', '相撲'), ('記事', '何度も顔を合わせた相手との対戦を「幕内で何度も対戦があるから、ちょっと何か気負いすぎたかな、立ち合いが高かった。もうちょっと、いい相撲を取りたかった」と振り返った。場所全体を振り返り「やっぱり切れが、いろいろとね、落ちてきたなと思うし、感じますね」と素直に吐露した。')])\n",
            "OrderedDict([('\\ufeff番号', '56'), ('スポーツ', '相撲'), ('記事', '幕下陥落が決まった1月の初場所千秋楽。「自分の中では、やりきったという思いはある」と引退でほぼ固まっていた。それを翻意した裏には、一粒種の長女希歩ちゃん（7）の存在だ。幕下以下に落ち無給生活になることを、けなげにも7歳で分かっていたそうで「私が貸してあげる」と泣きながら相撲を続けることを訴えられたという。さらに、初場所では親や家族を場所に呼ぶことなく、関取の座を失った。「家族も両親も見に来させられなかった。それでいいのか、と思った」と、なえた気持ちを奮い立たせて臨んだ今場所の土俵だった。')])\n",
            "OrderedDict([('\\ufeff番号', '57'), ('スポーツ', '相撲'), ('記事', 'その今場所は、家族を会場に呼び寄せようにも無観客開催。「最後の姿」を見せることは来場所以降に持ち越しだ。だが、そのことに豊ノ島本人はこだわっていない。「最後だから（家族に）見せたいということには、こだわってない。見てほしいけど、だからといって…（その理由だけで続けると決断する）ことはないし、テレビでも見てるでしょう。そんな中途半端な気持ちでは…」と話す。現状では「幕下で負け越して、なかなか気持ちを（土俵に）持っていくのは難しい。これだけ長くやって、下に下がって負け越したことで、1つの決断をする時かな、とかいろいろな思いがある。（現役を）続ける気持ちに持っていけるか…」と苦悩する胸の内を明かした。冗談っぽく「（決断は）娘との闘いかな」と少しだけ笑い「とりあえず場所は終わったので、ゆっくり進退を考えたいと思います」と、こみ上げるものを抑えるように話した。')])\n",
            "OrderedDict([('\\ufeff番号', '58'), ('スポーツ', 'ゴルフ'), ('記事', '新型コロナウイルスの感染拡大のため多くのスポーツイベントが中止となるなか、米男子ゴルフのタイガー・ウッズ（米国）が16日、自身のツイッターを更新し「今はゴルフより大事なことがたくさんある」とコメントした。')])\n",
            "OrderedDict([('\\ufeff番号', '59'), ('スポーツ', 'ゴルフ'), ('記事', '昨年の優勝で復活を印象付けたマスターズ・トーナメントの延期が決まったが「安全が第一で、愛する人のために、ベストの選択をする必要がある」と続けた。')])\n",
            "OrderedDict([('\\ufeff番号', '60'), ('スポーツ', 'ゴルフ'), ('記事', '米女子プロゴルフ協会（LPGA）は20日、新型コロナウイルスの感染拡大によりロッテ選手権（ハワイ州）など4月に始まる米ツアー3大会の延期を発表した。2月後半のホンダLPGA（タイ）から9大会が休止となり、再開は早くても5月中旬となる。')])\n",
            "OrderedDict([('\\ufeff番号', '61'), ('スポーツ', 'ゴルフ'), ('記事', 'また4月2～5日からの延期が決まっていたメジャー第1戦のANAインスピレーション（カリフォルニア州）を9月10～13日に開催すると発表した。国内4大大会の日本女子プロ選手権コニカミノルタ杯（岡山）と同じ週。')])\n",
            "OrderedDict([('\\ufeff番号', '62'), ('スポーツ', 'ゴルフ'), ('記事', '日本ツアーも3月初めの第1戦から5週連続で中止が決定。世界ランキングで東京オリンピック（五輪）出場圏内にいる渋野日向子、鈴木愛はシーズン初戦を迎えられず、畑岡奈紗も米ツアーで2位に2度入った1月を最後に実戦を離れている。')])\n",
            "OrderedDict([('\\ufeff番号', '63'), ('スポーツ', 'ゴルフ'), ('記事', 'ゴルフの男女の世界ランキングは20日、担当者の共同声明で男子は15日付、女子は16日付を最後に当面凍結すると発表された。新型コロナウイルス感染拡大でポイントの対象となっている大会が中止となっていることに対応した。')])\n",
            "OrderedDict([('\\ufeff番号', '64'), ('スポーツ', 'ゴルフ'), ('記事', '東京オリンピック（五輪）の出場権決定に用いる五輪ランキングは世界ランクが基準。国際ゴルフ連盟（IGF）は「システムが東京五輪の出場資格を狙う全ての選手にとって公平であることは変わらない」と声明を出した。')])\n",
            "OrderedDict([('\\ufeff番号', '65'), ('スポーツ', 'ゴルフ'), ('記事', '国内女子ゴルフのツアー第4戦、アクサ・レディース（27～29日、宮崎・UMKCC）の中止が発表になったことを受け、昨年の同大会でプロ初優勝を飾った河本結（21＝リコー）が、胸の内を明かした。')])\n",
            "OrderedDict([('\\ufeff番号', '66'), ('スポーツ', 'ゴルフ'), ('記事', '16日、自身のインスタグラムを更新。河本は「ディフェンディングチャンピオンとして、またホステスプロとしてもとても楽しみにしていましたが、今の状況を考えますと、中止はやむを得ないと思います。1日でも早く事態が収束することを願っています。試合が再開した際に最高なプレーがお見せできるようしっかり準備しておきたいと思います」などとつづった。')])\n",
            "OrderedDict([('\\ufeff番号', '67'), ('スポーツ', 'ゴルフ'), ('記事', '大逆転での東京五輪出場を目指す河本は、今季から米ツアーに参戦していたが、メジャー初戦となる4月のANAインスピレーションも延期が決定。米疾病対策センターは今後8週間、50人以上集まるイベントの中止か延期を求めており、米ツアーは再開の見通しが立っていない。')])\n",
            "OrderedDict([('\\ufeff番号', '68'), ('スポーツ', 'ゴルフ'), ('記事', '全米プロゴルフ協会（PGA）は13日、開催中の米ツアー、ザ・プレーヤーズ選手権の第2ラウンド以降の中止と、4月のマスターズまでの4試合の中止を発表した。')])\n",
            "OrderedDict([('\\ufeff番号', '69'), ('スポーツ', 'ゴルフ'), ('記事', '12日に始まったザ・プレーヤーズ選手権は、第1ラウンドはギャラリーを入れて開催した。3シーズンぶりのツアー優勝を目指す松山英樹（27＝LEXUS）は、スタートの10番から4連続バーディー。コースレコードタイの63をマークし、9アンダーで暫定の単独首位に立っていた。')])\n",
            "OrderedDict([('\\ufeff番号', '70'), ('スポーツ', 'ゴルフ'), ('記事', 'その好発進も幻と化した。第1日終了後に第2ラウンド以降は無観客で行うと発表していたPGAは、わずか数時間で方針を変更。同大会から、今年最初のメジャー、マスターズ前週のバレロ・テキサス・オープン（4月2日開幕）までの全試合を中止すると発表した。')])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6qIKzFlTQtO",
        "outputId": "4300e5b7-2fe7-4732-fe27-323301c70a0f"
      },
      "source": [
        "data = data_tmp['記事']\n",
        "len_data = len(data)\n",
        "print(len_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVktGKOS0uUs",
        "outputId": "3bc03d76-4de1-4d6f-c076-5ebaf0f53e85"
      },
      "source": [
        "print(data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     インターネットサイト「YouTube」の球団公式チャンネルが話題になっている。17日にはキャ...\n",
            "1     阪神は新型コロナウイルスの感染拡大でシーズン開幕が延期される中、野球以外でファンに向けて何が...\n",
            "2     企画発案から撮影に編集…。球団にとっても大きな負担となるが、矢野監督は「ファンがこういうのを...\n",
            "3     「練習試合だったけど、森は4打数3安打。（公式記録に残らず）かわいそうだったね。でも彼が求め...\n",
            "4     広島大瀬良大地投手（28）が、当初開幕予定だった中日戦で公式戦さながらに今年最長の7回を投げ...\n",
            "                            ...                        \n",
            "65    16日、自身のインスタグラムを更新。河本は「ディフェンディングチャンピオンとして、またホステ...\n",
            "66    大逆転での東京五輪出場を目指す河本は、今季から米ツアーに参戦していたが、メジャー初戦となる4...\n",
            "67    全米プロゴルフ協会（PGA）は13日、開催中の米ツアー、ザ・プレーヤーズ選手権の第2ラウンド...\n",
            "68    12日に始まったザ・プレーヤーズ選手権は、第1ラウンドはギャラリーを入れて開催した。3シーズ...\n",
            "69    その好発進も幻と化した。第1日終了後に第2ラウンド以降は無観客で行うと発表していたPGAは、...\n",
            "Name: 記事, Length: 70, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt-EV4jQYAJ5"
      },
      "source": [
        "def make_dictionary(data, dictionary, pos_data):\n",
        "  vec_data = []\n",
        "  len_sentence = []\n",
        "\n",
        "  for row in data:\n",
        "    row_mecab = []\n",
        "    pos = []\n",
        "    # 各行(row)を形態素で分割\n",
        "    # m = MeCab.Tagger('-Owakati')\n",
        "    m = MeCab.Tagger('-chasen')\n",
        "    if not row == row:\n",
        "      print('error')\n",
        "      row = ' '\n",
        "    node = m.parseToNode(row)\n",
        "    while node:  # 文章の最後まで繰り返す\\\n",
        "      row_mecab.append(node.surface)  # 単語情報を抜き出す\n",
        "      pos.append(node.feature.split(',')[0])  # 品詞情報を抜き出す\n",
        "\n",
        "      node = node.next  # 次の単語\n",
        "\n",
        "\n",
        "    # print(row_mecab)\n",
        "    # print(pos)\n",
        "    vec_sentence = []\n",
        "    count = 0\n",
        "\n",
        "    for word, pos_inf in zip(row_mecab, pos):\n",
        "      # print(word)\n",
        "      # print(pos_inf)\n",
        "      #辞書に含まれていない単語を逐次追加\n",
        "      if word not in dictionary:\n",
        "        dictionary.append(word)\n",
        "        pos_data.append(pos_inf)\n",
        "\n",
        "      #各行を単語INDEXで表現\n",
        "      vec_sentence.append(dictionary.index(word))\n",
        "      count += 1\n",
        "    #データ全体の単語INDEXを取得\n",
        "    vec_data.append(vec_sentence)\n",
        "    len_sentence.append(count)\n",
        "\n",
        "  return dictionary, np.array(vec_data), np.array(len_sentence)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXGGJN_wZ6WR",
        "outputId": "75784558-f034-4f1e-c828-9ec49c1aaa93"
      },
      "source": [
        "  #辞書INDEX0番目は空白として予約する\n",
        "  dictionary = [' ']\n",
        "  pos_data = ['空白']\n",
        "\n",
        "  dictionary, vec_data, len_sentence = make_dictionary(data, dictionary, pos_data)\n",
        "\n",
        "  print('registered words:', len(dictionary))\n",
        "  print('registered part of speech:', len(pos_data))  # 辞書数と同じ長さでなければおかしい\n",
        "  print(vec_data.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'インターネット', 'サイト', '「', 'YouTube', '」', 'の', '球団', '公式', 'チャンネル', 'が', '話題', 'に', 'なっ', 'て', 'いる', '。', '17', '日', 'に', 'は', 'キャプテン', 'の', '糸原', 'が', '練習', '中', 'に', '井上', '打撃', 'コーチ', 'に', 'ドッキリ', 'を', '仕掛ける', '映像', 'を', 'アップ', '。', '18', '日', 'に', 'は', '小型', 'カメラ', 'を', '頭', 'に', '装着', 'し', 'た', '元', '外野', '手', 'の', '緒方', '凌', '介', '広報', 'が', '、', '新', '助っ人', 'ボーア', 'の', '強烈', 'な', '打球', 'キャッチ', 'に', '挑戦', 'し', 'て', 'いる', '（', '詳細', 'は', '球団', 'の', '公式', 'チャンネル', 'で', '）', '。', '井上', 'コーチ', 'の', 'ドッキリ', '映像', 'は', '、', 'わずか', '2', '日', 'で', '再生', '回数', '15', '万', '回', 'を', '超え', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '名詞', '接尾辞', '助詞', '助詞', '名詞', '助詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '接尾辞', '助詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '名詞', '助詞', '補助記号', '接頭辞', '名詞', '名詞', '助詞', '形状詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '助詞', '補助記号', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '補助記号', '副詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '阪神', 'は', '新型', 'コロナ', 'ウイルス', 'の', '感染', '拡大', 'で', 'シーズン', '開幕', 'が', '延期', 'さ', 'れる', '中', '、', '野球', '以外', 'で', 'ファン', 'に', '向け', 'て', '何', 'が', 'できる', 'か', 'を', 'チーム', 'と', 'し', 'て', '検討', 'し', 'て', 'き', 'た', '。', '3', '月', '12', '日', 'の', '練習', '前', 'に', 'は', '監督', '、', '選手', '、', '関係', '者', 'が', '室内', '練習', '場', 'で', '緊急', 'ミーティング', 'を', '実施', '。', 'そこ', 'で', '出', 'て', 'き', 'た', 'の', 'は', '今年', '2', '月', 'に', '開設', 'し', 'た', '公式', 'チャンネル', 'の', '活用', 'だっ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '代名詞', '助詞', '動詞', '助詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '助詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '代名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '助動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '企画', '発案', 'から', '撮影', 'に', '編集', '…', '。', '球団', 'に', 'とっ', 'て', 'も', '大きな', '負担', 'と', 'なる', 'が', '、', '矢野', '監督', 'は', '「', 'ファン', 'が', 'こう', 'いう', 'の', 'を', '見', 'て', 'み', 'たい', 'って', 'いう', 'の', 'を', '含め', 'て', '、', '何', 'か', 'を', '（', 'SNS', 'など', 'を', '通じ', 'て', '）', '発信', 'し', 'て', 'いき', 'たい', '」', 'と', '話し', 'て', 'い', 'た', '。', 'ちなみ', 'に', 'ドッキリ', '企画', 'の', '発案', '者', 'は', '矢野', '監督', '。', '2', '軍', '戦', 'の', '視察', 'の', 'ため', 'に', '、', 'その', '瞬間', 'を', '見る', 'こと', 'が', 'でき', 'なかっ', 'た', 'が', '、', '作戦', '成功', 'に', 'ご', '満悦', 'だっ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '補助記号', '補助記号', '名詞', '助詞', '動詞', '助詞', '助詞', '連体詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '副詞', '動詞', '助詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助詞', '動詞', '助詞', '助詞', '動詞', '助詞', '補助記号', '代名詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '補助記号', '名詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '名詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '接頭辞', '名詞', '助動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '「', '練習', '試合', 'だっ', 'た', 'けど', '、', '森', 'は', '4', '打数', '3', '安打', '。', '（', '公式', '記録', 'に', '残ら', 'ず', '）', 'かわいそう', 'だっ', 'た', 'ね', '。', 'で', 'も', '彼', 'が', '求め', 'て', 'いる', 'もの', 'は', 'シンプル', 'に', 'でき', 'て', 'いる', 'ん', 'じゃ', 'ない', 'か', '。', '先発', 'し', 'た', '浜屋', 'は', '、', 'もっと', '大胆', 'に', '投げ', 'て', 'ほしかっ', 'た', 'ね', '」', '']\n",
            "['BOS/EOS', '補助記号', '名詞', '名詞', '助動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '補助記号', '補助記号', '名詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '形状詞', '助動詞', '助動詞', '助詞', '補助記号', '助詞', '助詞', '代名詞', '助詞', '動詞', '助詞', '動詞', '名詞', '助詞', '形状詞', '助動詞', '動詞', '助詞', '動詞', '助詞', '助動詞', '形容詞', '助詞', '補助記号', '名詞', '動詞', '助動詞', '名詞', '助詞', '補助記号', '副詞', '形状詞', '助動詞', '動詞', '助詞', '形容詞', '助動詞', '助詞', '補助記号', 'BOS/EOS']\n",
            "['', '広島', '大瀬', '良', '大地', '投手', '（', '28', '）', 'が', '、', '当初', '開幕', '予定', 'だっ', 'た', '中日', '戦', 'で', '公式', '戦', 'さながら', 'に', '今年', '最長', 'の', '7', '回', 'を', '投げ', '、', '打席', 'に', 'も', '初めて', '立っ', 'た', '。', '6', '回', 'まで', '単打', '2', '本', 'で', '無', '失点', '。', '7', '回', 'に', '不運', 'な', '当たり', 'や', '浮い', 'た', '球', 'を', '捉え', 'られ', 'て', '失点', 'し', '、', '7', '回', '7', '安打', '4', '失点', 'だっ', 'た', '。', 'それ', 'で', 'も', '「', '僕', 'の', '中', 'で', 'は', '結果', 'ほど', '悲観', 'する', 'ほど', 'で', 'は', 'ない', 'と', '思っ', 'て', 'いる', '」', 'と', '前', 'を', '向い', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '名詞', '名詞', '名詞', '助動詞', '助動詞', '名詞', '接尾辞', '助詞', '名詞', '接尾辞', '副詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '助詞', '副詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '接尾辞', '助詞', '接頭辞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助動詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '助動詞', '助動詞', '補助記号', '代名詞', '助詞', '助詞', '補助記号', '代名詞', '助詞', '名詞', '助詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助詞', '助動詞', '助詞', '形容詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '1', '回', 'は', '直球', 'と', '得意', '球', 'の', 'カット', 'ボール', 'で', '押す', 'も', '、', '2', '回', 'から', '組み立て', 'を', '変え', 'て', 'カーブ', 'や', 'フォークボール', 'を', '多', '投', '。', '「', 'あまり', 'カット', 'ボール', 'を', '使わ', 'ず', 'に', '真っすぐ', 'と', 'カーブ', '、', 'フォーク', 'で', '抑え', 'られ', 'た', '。', '違っ', 'た', 'バリエーション', '。', '配球', 'に', 'し', 'て', 'も', 'いい', 'もの', 'が', '見せ', 'られ', 'た', '。', '今後', 'に', 'つながっ', 'て', 'いく', 'ん', 'じゃ', 'ない', 'か', 'な', 'と', '思い', 'ます', '」', '。', '2', '回', 'は', 'フォーク', '、', 'スライダー', '、', '真っすぐ', 'と', 'いずれ', 'も', '違う', '決め球', 'で', '阿部', '、', '平田', '、', '京田', 'を', '3', '者', '連続', '三振', '。', '投球', 'の', '幅', 'が', '広がる', '可能', '性', 'を', '感じ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '助詞', '名詞', '助詞', '形容詞', '名詞', '補助記号', '補助記号', '副詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '副詞', '助詞', '名詞', '補助記号', '名詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '動詞', '助動詞', '名詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '助詞', '形容詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '助動詞', '形容詞', '助詞', '助詞', '助詞', '動詞', '助動詞', '補助記号', '補助記号', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '補助記号', '副詞', '助詞', '代名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '助詞', '名詞', '接尾辞', '名詞', '名詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '形状詞', '接尾辞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '前回', '13', '日', 'の', 'ソフト', 'バンク', '戦', 'で', 'の', '5', '回', '4', '失点', 'から', '修正', 'と', '改善', 'が', '見', 'られ', '、', '2', '戦', '続け', 'て', '101', '球', 'と', 'スタミナ', '面', 'の', '不安', 'は', 'ない', '。', '加え', 'て', '、', '現', '時点', 'で', '最短', 'の', '開幕', '4', '月', '10', '日', 'で', 'も', '当たる', '中日', 'の', '目先', 'を', '変える', '餌', 'を', 'まく', 'こと', 'も', 'でき', 'た', '。', 'バッテリー', 'を', '組ん', 'だ', '会沢', 'は', '「', 'その', '意味', '合い', 'も', '当然', 'ある', '。', '前回', 'の', '投球', 'を', '踏まえ', 'て', '、', '大地', 'と', 'いろいろ', '話', 'を', 'し', 'て', '組み立て', 'られ', 'た', '」', 'と', '振り返っ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '接尾辞', '助詞', '助詞', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '接尾辞', '動詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '形容詞', '補助記号', '動詞', '助詞', '補助記号', '接頭辞', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '助詞', '動詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '補助記号', '連体詞', '名詞', '接尾辞', '助詞', '副詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '副詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '今年', '初', 'の', '実戦', '打撃', 'で', 'は', '5', '回', '1', '死', '一塁', 'で', 'バスター', 'を', '決め', '、', '内野', '安打', '。', '実り', 'ある', '試合', 'で', '弾み', 'を', 'つけ', 'た', '右腕', 'は', '「', 'あと', 'は', '微', '調整', 'や', '、', 'やり', 'たい', 'こと', 'に', '集中', 'できる', '」', 'と', '先', 'を', '見据え', 'た', '。', '今後', 'は', '1', '度', '登板', '機会', 'を', '空け', '、', '次回', 'は', '4', '月', '第', '1', '週', 'の', '週末', 'を', '予定', 'する', '。', 'そこ', 'から', '定め', 'られ', 'た', '開幕', '日', 'に', '合わせ', 'て', 'いく', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '助詞', '名詞', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '名詞', '補助記号', '名詞', '動詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '接頭辞', '名詞', '助詞', '補助記号', '動詞', '助動詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '接頭辞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '代名詞', '助詞', '動詞', '助動詞', '助動詞', '名詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '巨人', '阿部', '慎之助', '2', '軍', '監督', 'が', '20', '日', '、', '41', '歳', 'の', '誕生', '日', 'を', '迎え', 'た', '。', 'ジャイアンツ', '球場', 'で', '行わ', 'れ', 'た', '2', '軍', '練習', '前', 'に', 'は', '、', '育成', 'の', '笠井', 'を', '中心', 'に', 'バースデー', 'ソング', 'で', '祝わ', 'れ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '名詞', '名詞', '名詞', '名詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'ブルペン', 'デー', 'で', '、', 'ヤクルト', 'の', '若手', '投手', '陣', 'が', 'アピール', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '先発', 'の', '2', '年', '目', '清水', 'は', '2', '回', 'を', '無', '失点', '。', '3', '番手', 'の', 'ドラフト', '4', '位', '大西', 'は', '2', '回', 'を', '完璧', 'に', '抑え', 'た', '。', '9', '回', 'は', '、', 'ソフト', 'バンク', 'の', '育成', 'から', '新', '加入', 'し', 'た', '左腕', '長谷川', 'が', 'わずか', '7', '球', 'で', '3', '者', '凡退', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '名詞', '名詞', '接尾辞', '名詞', '助詞', '名詞', '名詞', '助詞', '接頭辞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '接尾辞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助動詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '接頭辞', '名詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '副詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '名詞', '補助記号', 'BOS/EOS']\n",
            "['', '高津', '監督', 'は', '、', '長谷川', 'に', 'つい', 'て', '「', '右', '打者', 'に', '対し', 'て', '（', 'の', '投球', 'を', '）', '見', 'たかっ', 'た', '。', 'らし', 'さ', 'を', '出し', 'て', 'くれ', 'て', 'いる', '」', 'と', '評価', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '助詞', '名詞', '助詞', '補助記号', '動詞', '助動詞', '助動詞', '補助記号', '接尾辞', '接尾辞', '助詞', '動詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '巨人', '新', '外国', '人', 'の', 'サンチェス', '投手', 'が', '、', '21', '日', 'の', '練習', '試合', 'DeNA', '戦', 'に', '先発', 'する', '。', '']\n",
            "['BOS/EOS', '名詞', '接頭辞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '約', '2', '週間', '前', 'から', '杉内', '2', '軍', '投手', 'コーチ', 'ら', 'の', 'アドバイス', 'を', '受け', '、', 'ブルペン', 'で', '10', '球', 'ごと', 'に', '新球', 'へ', 'チェンジ', 'する', '方法', 'を', '実践', '。', '日本', 'の', 'ボール', 'に', '慣れる', 'ため', 'の', '改善', '策', 'の', '1', 'つ', 'だ', '。', '']\n",
            "['BOS/EOS', '接頭辞', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '名詞', '動詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '日本', '流', 'を', '落とし込み', 'ながら', '、', '投球', 'で', 'は', '「', 'これ', 'まで', 'と', '同じ', 'アプローチ', 'で', 'やっ', 'て', 'いく', 'よ', '」', 'と', '昨季', '韓国', 'で', '17', '勝', 'を', '挙げ', 'た', '力', 'で', '打者', 'を', '料理', 'する', '。', '']\n",
            "['BOS/EOS', '名詞', '接尾辞', '助詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '助詞', '補助記号', '代名詞', '助詞', '助詞', '連体詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '補助記号', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'プロ', '野球', 'は', '本来', 'の', '開幕', '日', 'だっ', 'た', '3', '月', '20', '日', 'から', '練習', '試合', 'が', '始まっ', 'て', 'いる', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '助動詞', '助動詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '21', '日', 'は', '守護', '神', 'から', '先発', 'に', '転向', 'し', 'た', '楽天', '松井', 'の', '登板', 'など', 'に', '注目', 'だ', '。', '']\n",
            "['BOS/EOS', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '助詞', '助詞', '名詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '阪神', '福留', '孝介', '外野', '手', '（', '42', '）', 'が', '本来', 'の', '開幕', '予定', '日', 'だっ', 'た', '20', '日', '、', 'ヤクルト', 'と', 'の', '練習', '試合', '（', '神宮', '）', 'で', 'ハッスル', '猛打', '賞', 'を', '決め', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '名詞', '接尾辞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助動詞', '助動詞', '名詞', '接尾辞', '補助記号', '名詞', '助詞', '助詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '2', '回', 'に', '左中', '間', 'へ', '激走', '二塁', '打', 'を', '放つ', 'と', '、', 'その', '後', 'も', '中前', '、', '右前', 'と', '全', '方位', 'に', '打ち分け', 'た', '。', '新型', 'コロナ', 'ウイルス', 'の', '感染', '拡大', '余波', 'で', '開幕', '日', 'が', '決まら', 'ず', '、', 'モチベーション', '維持', 'が', '難しい', '日々', 'が', '続く', 'が', '球界', '最', '年長', 'は', '不動', '心', '。', '「', '今日', 'が', '開幕', '」', 'と', 'いう', '強い', '気持ち', 'を', '体現', 'し', '、', 'チーム', 'を', '叱咤', '（', 'しっ', 'た', '）', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '接尾辞', '助詞', '動詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '助詞', '接頭辞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '形容詞', '名詞', '助詞', '動詞', '助詞', '名詞', '接頭辞', '名詞', '助詞', '名詞', '接尾辞', '補助記号', '補助記号', '名詞', '助詞', '名詞', '補助記号', '助詞', '動詞', '形容詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '補助記号', '動詞', '助動詞', '補助記号', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'DeNA', '浜口', '遥', '大', '投手', '（', '25', '）', 'が', '20', '日', '、', '結婚', 'し', 'た', 'こと', 'を', '発表', 'し', 'た', '。', '相手', 'の', '恵理子', 'さん', '（', '25', '）', 'は', '東京', '出身', 'の', '会社', '員', 'で', '、', '女優', '仲', '里依', '紗', 'に', '似', 'た', '美人', '。', '大学', '時代', 'に', '知り合い', '、', '約', '4', '年', 'の', '交際', '期間', 'を', '経', 'て', '、', '昨年', '9', '月', 'の', '恵理子', 'さん', 'の', '誕生', '日', 'に', '横浜', 'みなと', 'みらい', 'で', 'プロポーズ', '。', '今月', '中旬', 'に', '婚姻', '届', 'を', '提出', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '接頭辞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助動詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '補助記号', '接頭辞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '知人', 'に', '描い', 'て', 'もらっ', 'た', '2', '人', 'の', '似顔', '絵', 'を', '手', 'に', 'し', 'た', '左腕', 'は', '「', 'すごく', '明るく', 'て', '前向き', 'な', 'ところ', 'は', '、', '僕', 'に', 'とっ', 'て', 'も', 'ありがたい', 'です', '。', 'うまく', 'いか', 'ない', '中', 'で', 'も', 'ポジティブ', 'に', '支え', 'て', 'くれ', 'まし', 'た', '。', '得意', '料理', '？', '\\u3000', '何', 'を', '作っ', 'て', 'も', 'おいしい', 'です', '」', 'と', 'のろけ', 'た', '。', '近く', '同居', 'を', '始める', '予定', 'だ', 'と', 'いう', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '補助記号', '形容詞', '形容詞', '助詞', '形状詞', '助動詞', '名詞', '助詞', '補助記号', '代名詞', '助詞', '動詞', '助詞', '助詞', '形容詞', '助動詞', '補助記号', '形容詞', '動詞', '助動詞', '名詞', '助詞', '助詞', '名詞', '助動詞', '動詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '名詞', '名詞', '補助記号', '空白', '代名詞', '助詞', '動詞', '助詞', '助詞', '形容詞', '助動詞', '補助記号', '助詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '名詞', '助動詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '浜口', 'は', '神奈川', '大', 'から', '16', '年', 'ドラフト', '1', '位', 'で', '入団', '。', '1', '年', '目', 'に', '10', '勝', 'を', '挙げ', 'チーム', 'の', '日本', 'シリーズ', '進出', 'に', '貢献', 'し', 'た', 'が', '、', 'ここ', '2', '年', 'は', '1', 'ケタ', '勝利', 'に', '終わっ', 'た', '。', '4', '年', '目', 'の', '今季', 'も', '開幕', 'ローテーション', '入り', 'が', '確実', 'な', '左腕', 'は', '「', '妻', 'の', '家族', 'も', '含め', 'て', '、', 'みんな', 'の', '期待', 'を', '受ける', 'こと', 'に', 'なる', '。', '今年', 'は', '開幕', 'も', '遅れ', 'て', '、', '五輪', 'も', 'あっ', 'たり', 'し', 'ます', 'し', '、', 'やっぱり', '特別', 'な', '1', '年', 'に', 'し', 'たい', 'です', '」', 'と', '言葉', 'に', '力', 'を', '込め', 'た', '。', '「', 'ハマ', 'の', 'ハマ', 'ちゃん', '」', 'が', '自覚', 'も', '新た', 'に', '、', 'さらなる', '飛躍', 'を', '目指す', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '補助記号', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '助詞', '補助記号', '代名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '形状詞', '助動詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '副詞', '形状詞', '助動詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '補助記号', '名詞', '助詞', '名詞', '接尾辞', '補助記号', '助詞', '名詞', '助詞', '形状詞', '助動詞', '補助記号', '連体詞', '名詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '日本', 'サッカー', '協会', '（', 'JFA', '）', 'は', '20', '日', '、', '田嶋', '幸三', '会長', '（', '62', '）', 'が', '14', '日', 'に', '新型', 'コロナ', 'ウイルス', 'を', '発症', 'し', 'た', 'こと', 'を', '受け', '、', '感染', '拡大', '防止', '策', 'を', '新た', 'に', '講じる', 'こと', 'を', '発表', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '助詞', '形状詞', '助動詞', '動詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '保健', '所', 'の', '指導', 'の', '下', '、', '最長', '23', '日', 'まで', '東京', '・', '文京', '区', 'の', 'JFA', 'ハウス', '内', 'の', '消毒', '作業', 'を', '実施', '。', '27', '日', 'まで', '役職', '員', 'は', '在宅', '勤務', 'を', '継続', 'し', '、', '来館', '者', 'の', '立ち入り', 'も', '禁止', 'する', '。', 'また', '、', '田嶋', '会長', 'も', '参加', 'し', 'て', 'い', 'た', '14', '日', 'の', '理事', '会', '出席', '者', 'の', '行動', '履歴', 'を', '確認', 'し', 'た', 'うえ', 'で', '保健', '所', 'から', '指導', 'を', '受け', 'て', 'おり', '、', '現', '時点', 'で', '発熱', 'や', '風邪', 'の', '症状', 'を', '訴える', '人', 'は', 'い', 'ない', 'こと', 'も', '公表', 'し', 'た', '。', '田嶋', '会長', 'の', '症状', 'は', '悪化', 'は', 'し', 'て', 'おら', 'ず', '、', '検査', 'と', '治療', 'を', '続け', 'て', 'いる', '模様', 'だ', '。', '']\n",
            "['BOS/EOS', '名詞', '接尾辞', '助詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '接尾辞', '助詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '接続詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '動詞', '助詞', '動詞', '助動詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '接頭辞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '名詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'J', 'リーグ', 'の', '村井', '満', 'チェアマン', '（', '60', '）', 'が', '19', '日', '、', '経営', '難', 'に', '陥っ', 'た', 'サガン', '鳥栖', 'と', '協議', '中', 'で', 'ある', 'こと', 'を', '認め', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助動詞', '動詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'ウェブ', 'に', 'よる', '実行', '委員', '会', '後', 'の', '会見', 'で', '「', 'PL', '（', '損益', '計算', '書', '）', '上', 'の', '問題', 'と', 'キャッシュ', 'フロー', 'の', '議論', 'が', 'あり', 'まし', 'て', '。', 'この', 'あたり', 'を', '精査', 'し', 'て', 'いる', 'ところ', '。', '現在', '、', 'クラブ', 'と', '担当', '者', 'レベル', 'で', '意見', '交換', 'し', 'て', 'いる', '」', 'と', '話し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '動詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '補助記号', '名詞', '補助記号', '名詞', '名詞', '接尾辞', '補助記号', '接尾辞', '助詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '動詞', '助詞', '動詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '助詞', '名詞', '接尾辞', '名詞', '助詞', '名詞', '名詞', '動詞', '助詞', '動詞', '補助記号', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '村井', 'チェアマン', 'が', '鳥栖', '経営', '危機', 'の', '報告', 'を', '受け', 'た', 'の', 'は', '開幕', '（', '2', '月', '21', '日', '）', '直前', 'で', '、', 'J', 'リーグ', '事務', '局', '担当', '者', 'に', 'よる', 'と', '「', 'ライセンス', '制度', 'の', 'ルール', 'に', '基づい', 'て', '1', '月', '末', '、', '定期', '的', 'な', '話', 'の', '中', 'で', '資金', '繰り', 'の', '話', 'に', 'なっ', 'た', '」', 'と', 'いう', '。', 'その', '後', '、', 'J', 'リーグ', '担当', '者', 'と', '鳥栖', 'の', '間', 'で', '協議', 'を', '重ね', 'て', 'いる', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '接尾辞', '補助記号', '名詞', '助詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '動詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '名詞', '接尾辞', '補助記号', '名詞', '接尾辞', '助動詞', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '助詞', '動詞', '補助記号', '連体詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '鳥栖', 'は', '昨季', '、', '5', '億', '円', '以上', 'の', '赤字', 'を', '出し', '、', '今季', 'も', '資金', '難', 'に', '苦しん', 'で', 'いる', '。', '仮', 'に', '「', 'リーグ', '戦', '安定', '開催', '融資', '制度', '」', 'を', '適用', 'しよう', 'に', 'も', '、', '返却', 'の', 'めど', 'が', '立た', 'ない', 'と', '、', 'J', 'リーグ', 'から', 'の', '融資', 'も', '難しく', 'なる', '。', '場合', 'に', 'よっ', 'て', 'は', 'クラブ', 'ライセンス', 'に', 'も', '影響', 'し', '、', '2', '部', 'か', '3', '部', '降格', 'や', '除名', 'など', 'の', '可能', '性', 'も', '出', 'て', 'くる', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '補助記号', '名詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '補助記号', '助詞', '名詞', '動詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '助詞', '名詞', '助詞', '形容詞', '動詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '助詞', '名詞', '名詞', '助詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '助詞', '形状詞', '接尾辞', '助詞', '動詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '北海道', 'コンサドーレ', '札幌', '野々村', '芳和', '社長', '（', '47', '）', 'が', '19', '日', '、', '新型', 'コロナ', 'ウイルス', 'の', '影響', 'で', 'クラブ', 'が', '被る', '損失', 'に', 'つい', 'て', '、', '約', '5', '億', '円', 'を', '見込ん', 'で', 'いる', 'と', '明かし', 'た', '。', '「', '今年', 'を', '乗り切れ', 'た', 'と', 'し', 'て', 'も', '、', '来年', '以降', 'は', '相当', '大変', 'な', 'こと', 'に', 'なる', '。', 'クラブ', 'と', 'し', 'て', '、', 'せっかく', '成長', '曲線', 'で', '来', 'て', 'い', 'た', 'けど', '、', 'しょう', 'が', 'ない', '」', 'と', '話し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '接頭辞', '名詞', '名詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '補助記号', '名詞', '助詞', '動詞', '助動詞', '助詞', '動詞', '助詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '副詞', '形状詞', '助動詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '補助記号', '副詞', '名詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '形容詞', '補助記号', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'J', 'リーグ', '公式', '戦', 'は', '中断', '中', '。', '現', '時点', 'で', '札幌', 'の', 'ホーム', 'ゲーム', 'は', '、', '札幌', 'ドーム', 'で', '週末', 'に', '予定', 'さ', 'れ', 'て', 'い', 'た', '2', '試合', 'が', '延期', 'さ', 'れ', 'て', 'いる', '。', '平日', 'で', 'の', '札幌', '厚別', '開催', 'に', '振り替え', 'られる', '可能', '性', 'も', 'ある', '。', '会場', '変更', 'に', '伴う', '観客', '数', 'の', '減少', 'に', 'よる', '収入', '減', 'は', 'もちろん', '、', '会場', 'の', 'アルコール', '消毒', '費', '、', 'これ', 'まで', '高齢', '者', 'も', '多く', 'い', 'た', 'ボランティア', '運営', 'スタッフ', 'を', '採用', 'せ', 'ず', '、', 'アルバイト', 'スタッフ', 'に', 'し', 'た', '時', 'の', '人件', '費', 'など', '、', '新た', 'な', '支出', 'が', '想定', 'さ', 'れる', '。', '\\u3000', '21', '日', 'の', '鹿島', 'と', 'の', '練習', '試合', 'に', 'は', '、', '別', 'メニュー', 'を', 'のぞく', '全', '選手', 'が', '帯同', 'する', '。', '実戦', '感覚', 'を', '失わ', 'ない', 'ため', 'の', '道外', '遠征', 'も', '、', 'リーグ', '中断', 'に', '伴い', '発生', 'し', 'た', '支出', 'だ', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '接尾辞', '補助記号', '接頭辞', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '助詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '形状詞', '接尾辞', '助詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '名詞', '助詞', '副詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '接尾辞', '補助記号', '代名詞', '助詞', '名詞', '接尾辞', '助詞', '形容詞', '動詞', '助動詞', '名詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '補助記号', '形状詞', '助動詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '空白', '名詞', '接尾辞', '助詞', '名詞', '助詞', '助詞', '名詞', '名詞', '助詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '接頭辞', '名詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '名詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '名詞', '動詞', '助動詞', '名詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'この', '日', '、', 'J', '1', 'の', '降格', 'チーム', 'なし', 'が', '決まっ', 'た', '。', 'だ', 'が', '今季', 'の', '戦い', '方', 'に', '変わり', 'は', 'ない', '。', '「', '現場', 'の', '目標', 'は', 'ACL', '。', 'そこ', 'に', '向かっ', 'て', 'いけ', 'ば', 'いい', '」', 'と', '同社', '長', '。', '日程', 'は', '過密', 'と', 'なる', 'が', '「', '連戦', 'が', '増えれ', 'ば', '増える', 'ほど', '、', 'いろんな', '選手', 'に', 'チャンス', 'が', 'ある', 'わけ', 'だ', 'から', '、', '前向き', 'に', '考え', 'て', '欲しい', 'よ', 'ね', '。', 'コロナ', 'の', 'この', '影響', 'が', 'あっ', 'た', 'から', '、', '選手', 'と', 'し', 'て', '一皮', 'むけ', 'た', 'と', 'か', '、', 'チャンス', 'を', '得', 'られ', 'た', 'と', 'か', 'いう', '選手', 'が', '出', 'て', 'くる', 'こと', 'が', 'うれしい', '」', 'と', '期待', 'し', 'て', 'い', 'た', '。', '']\n",
            "['BOS/EOS', '連体詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '助動詞', '助詞', '名詞', '助詞', '動詞', '接尾辞', '助詞', '名詞', '助詞', '形容詞', '補助記号', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '補助記号', '代名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '形容詞', '補助記号', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助動詞', '助詞', '補助記号', '形状詞', '助動詞', '動詞', '助詞', '形容詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '連体詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '名詞', '動詞', '助動詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助動詞', '助動詞', '助詞', '助詞', '動詞', '名詞', '助詞', '動詞', '助詞', '動詞', '名詞', '助詞', '形容詞', '補助記号', '助詞', '名詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'J', '1', '湘南', 'ベルマーレ', 'は', '、', '今季', 'の', 'J', '1', 'と', 'J', '2', 'で', '降格', 'なし', 'と', 'の', '特例', 'が', '適用', 'さ', 'れる', 'こと', 'を', '「', '刺激', '」', 'に', '変え', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '助詞', '補助記号', '名詞', '補助記号', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '19', '日', 'の', 'J', 'リーグ', '臨時', '実行', '委員', '会', 'で', 'の', '決定', 'から', '一夜', '明け', 'た', '20', '日', '、', 'チーム', 'は', '平塚', '市', '内', 'で', '非', '公開', '練習', 'を', '消化', '。', '昨季', 'は', 'プレー', 'オフ', 'の', '末', '、', 'J', '1', '残留', 'を', '決め', 'た', '経緯', 'も', 'あり', '、', 'MF', '鈴木', '冬', '一', '（', '19', '）', 'は', '「', '昨年', '、', 'プレー', 'オフ', 'の', '緊張', '感', 'は', '経験', 'し', 'た', '。', '今年', 'は', '降格', 'の', '危機', '感', 'で', 'は', 'なく', '、', '逆', 'に', '優勝', '争い', 'できる', '緊張', '感', 'を', 'もっと', '味わい', 'たい', '」', 'と', '上位', '争い', 'へ', 'の', '意識', 'を', '高め', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '助詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '接尾辞', '補助記号', '名詞', '助詞', '名詞', '名詞', '接尾辞', '助詞', '接頭辞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '助詞', '助詞', '形容詞', '補助記号', '名詞', '助動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '助詞', '副詞', '動詞', '助動詞', '補助記号', '助詞', '名詞', '名詞', '助詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'また', 'MF', '斉', '籐', '未', '月', '（', '21', '）', 'は', '「', '毎年', '、', '下位', 'や', '降格', '争い', 'し', 'て', 'しまっ', 'て', 'いる', 'けれど', '、', 'いつ', 'まで', 'も', '下', 'に', 'いる', 'の', 'は', '面白く', 'ない', '。', '観客', 'も', 'サポーター', 'も', '緊張', '感', 'ある', '試合', 'を', '求め', 'て', 'いる', 'と', '思う', '。', '上位', 'で', 'そう', 'いう', '試合', 'も', '増やし', 'たい', '」', 'と', '気', 'を', '引き締め', 'た', '。', '']\n",
            "['BOS/EOS', '接続詞', '名詞', '名詞', '名詞', '接頭辞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '名詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '動詞', '助詞', '動詞', '助詞', '動詞', '助詞', '補助記号', '代名詞', '助詞', '助詞', '名詞', '助詞', '動詞', '助詞', '助詞', '形容詞', '形容詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '動詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '副詞', '動詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '前', '監督', 'の', '退任', '、', 'そして', '台風', 'に', 'よる', '練習', '場', '冠水', 'など', 'の', '苦境', 'を', '乗り越え', '、', 'J', '1', '残留', 'を', '勝ち取っ', 'た', '浮', '嶋', '敏', '監督', '（', '52', '）', 'は', '「', '（', '降格', 'なし', 'は', '）', '昨年', '、', 'ボク', 'が', '引き継い', 'だ', '状態', 'の', '時', 'だっ', 'たら', '喜ぶ', 'か', 'も', 'しれ', 'ない', 'けれど', '。', '1', '試合', 'しか', '終わっ', 'て', 'い', 'ない', 'し', '、', '今', 'だ', 'と', '喜び', 'よう', 'も', 'ない', 'か', 'な', '」', 'と', 'ジョーク', 'で', '反応', '。', '公式', '戦', '再開', '後', 'の', '試合', '展開', 'に', '変化', 'が', 'ある', 'と', '指摘', 'し', '「', 'リーグ', '全体', 'で', 'いえ', 'ば', '、', 'より', 'アグレッシブ', 'に', '戦う', 'チーム', 'が', '増える', '。', '何', 'より', '得点', 'が', '増える', '。', '終盤', 'に', '采配', '、', '選手', 'の', '気持ち', 'が', 'かなり', '違う', 'と', '思い', 'ます', '。', 'より', '攻撃', '的', 'に', 'できる', '」', 'と', '話し', 'て', 'い', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '補助記号', '接続詞', '名詞', '助詞', '動詞', '名詞', '接尾辞', '名詞', '助詞', '助詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '補助記号', '名詞', '名詞', '助詞', '補助記号', '名詞', '補助記号', '代名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '助動詞', '助動詞', '動詞', '助詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '助動詞', '助詞', '動詞', '接尾辞', '助詞', '形容詞', '助詞', '助詞', '補助記号', '助詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '接尾辞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '副詞', '形状詞', '助動詞', '動詞', '名詞', '助詞', '動詞', '補助記号', '代名詞', '助詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '副詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '副詞', '名詞', '接尾辞', '助動詞', '動詞', '補助記号', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '新型', 'コロナ', 'ウイルス', 'の', '感染', '拡大', 'の', '影響', 'で', '2', '月', '下旬', 'から', '公式', '戦', '中断', '中', 'の', 'J', 'リーグ', 'は', '19', '日', 'に', '臨時', '実行', '委員', '会', 'を', '実施', 'し', '、', '今季', 'は', 'J', '1', 'と', 'J', '2', 'で', '降格', 'なし', 'の', '特例', 'の', '適用', 'を', '決定', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'J', '2', '、', 'J', '3', 'とも', 'ライセンス', '上', '問題', 'の', 'ない', '上位', '2', 'チーム', 'が', '自動', '昇格', '。', '今季', '18', 'チーム', 'の', 'J', '1', 'は', '来季', '20', 'チーム', 'で', '臨み', '、', '22', '年', 'シーズン', 'は', '4', 'チーム', 'を', '降格', 'と', 'し', 'て', '18', 'チーム', 'に', '戻す', 'こと', 'に', 'なる', '見込み', 'だ', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '補助記号', '名詞', '名詞', '接尾辞', '名詞', '接尾辞', '名詞', '助詞', '形容詞', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '名詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'J', 'リーグ', 'は', '4', '月', '3', '日', 'の', '再開', 'を', '目指し', 'て', 'いる', 'が', '、', '安全', '面', 'も', '考慮', 'し', 'て', '4', '月', '17', '日', 'や', '5', '月', 'の', 'ゴールデン', 'ウイーク', '明け', 'まで', 'ずれこむ', '可能', '性', 'も', 'ある', '。', '五輪', '期間', '中', 'や', '国際', 'A', 'マッチ', 'デー', 'など', 'に', '試合', 'を', '行う', '変則', '日程', 'と', 'なれ', 'ば', '、', '主力', 'が', '代表', '招集', 'で', '不在', 'に', 'なっ', 'たり', '敵地', 'で', 'の', '連戦', 'が', '続く', 'など', '公平', '性', 'を', '保て', 'ない', '場面', 'も', '出', 'て', 'くる', '。', '村井', 'チェアマン', 'は', '「', 'ある', '意味', 'で', 'は', '不', '公平', '性', 'を', '飲み込ん', 'で', 'で', 'も', '試合', 'は', '続け', 'て', 'いこう', 'と', 'いう', '目線', '合わせ', 'を', 'し', 'た', '。', '選手', 'が', '目標', 'に', '向かっ', 'て', '頑張る', '姿', 'を', '推奨', 'し', 'たい', 'し', '結果', 'を', '残し', 'た', '選手', 'に', '報い', 'て', 'いき', 'たい', '」', 'と', '現', '時点', 'で', '大会', '方式', 'の', '変更', 'を', '決め', 'た', '理由', 'を', '説明', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '動詞', '助詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '形状詞', '接尾辞', '助詞', '動詞', '補助記号', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '名詞', '助詞', '助詞', '名詞', '助詞', '動詞', '名詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '助詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '接尾辞', '助詞', '動詞', '助動詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '助詞', '接頭辞', '名詞', '接尾辞', '助詞', '動詞', '助詞', '助詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '動詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '助詞', '接頭辞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '公式', '戦', 'と', 'し', 'て', '成立', 'する', '消化', '試合', '数', 'は', '、', '全', '日程', 'の', '75', '％', '以上', '案', 'が', '選択', '肢', 'に', '挙がっ', 'て', 'いる', '。', '賞金', 'の', '分配', '法', 'や', '22', '年', 'シーズン', 'の', '各', 'カテゴリー', 'の', 'チーム', '数', 'など', '細か', 'な', '事案', 'に', 'つい', 'て', 'は', '、', '再開', 'か', '再', '延期', 'か', 'も', '含め', '、', '25', '日', 'の', '臨時', '実行', '委員', '会', 'で', '協議', 'する', '。', '']\n",
            "['BOS/EOS', '名詞', '接尾辞', '助詞', '動詞', '助詞', '名詞', '動詞', '名詞', '名詞', '名詞', '助詞', '補助記号', '接頭辞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '動詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '接頭辞', '名詞', '助詞', '名詞', '名詞', '助詞', '形状詞', '助動詞', '名詞', '助詞', '動詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '接頭辞', '名詞', '助詞', '助詞', '動詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '白鵬', 'が', '朝', '乃', '山', 'の', '大関', 'とり', 'の', '壁', 'と', 'なっ', 'て', '立ちはだかっ', 'た', '。', '右', 'を', '差し込み', '、', '左', '上手', 'で', '相手', 'に', '右', 'を', '差さ', 'せ', 'ない', '厳しい', '相撲', 'で', '押し出し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '動詞', '補助記号', '名詞', '形状詞', '助動詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '形容詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'しかし', '、', '取組', '後', 'は', '報道', '陣', 'の', '呼びかけ', 'に', '応じ', 'ず', '、', '取材', 'エリア', 'を', 'なぞ', 'の', 'スルー', '。', '再び', 'トップ', 'に', '並び', '、', '14', '日', '目', 'は', '同じ', '2', '敗', 'の', '碧山', 'と', 'の', '対戦', 'が', '組ま', 'れ', 'た', '。', '勝負', 'どころ', 'に', '向け', 'て', 'の', '気合', 'か', '、', '無言', 'を', '貫く', '姿', 'は', '気迫', 'を', '漂わ', 'せ', 'た', '。', '']\n",
            "['BOS/EOS', '接続詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '補助記号', '副詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '接尾辞', '接尾辞', '助詞', '連体詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助詞', '助詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '大関', 'とり', 'の', '関脇', '朝', '乃', '山', '（', '26', '＝', '高砂', '）', 'が', '、', '横綱', '白鵬', 'に', '押し出し', 'で', '負け', 'て', '3', '敗', '目', 'を', '喫し', 'た', '。', '14', '日', '目', 'は', '2', '敗', 'で', 'トップ', 'タイ', 'の', '横綱', '鶴竜', 'と', '対戦', '。', '大関', '昇進', '目安', 'の', '「', '三', '役', 'で', '3', '場所', '33', '勝', '」', 'に', '必要', 'な', '12', '勝', 'に', '向け', 'て', '数字', '上', 'は', '後', 'が', 'ない', '状況', 'だ', 'が', '、', '勝て', 'ば', '大関', '昇進', 'の', '機運', 'が', '高まる', '可能', '性', 'は', 'ある', '。', '自身', '2', '度', '目', 'と', 'なる', '優勝', 'の', 'ため', 'に', 'も', '、', '白星', 'で', '望み', 'を', 'つなげ', 'たい', '。', '白鵬', '、', '鶴竜', 'と', '平幕', '碧山', 'が', '2', '敗', 'で', '首位', 'に', '並ん', 'だ', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '接尾辞', '接尾辞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '接尾辞', '接尾辞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '補助記号', '助詞', '名詞', '助動詞', '名詞', '名詞', '助詞', '動詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '形容詞', '名詞', '助動詞', '助詞', '補助記号', '動詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '形状詞', '接尾辞', '助詞', '動詞', '補助記号', '名詞', '名詞', '名詞', '接尾辞', '助詞', '動詞', '名詞', '助詞', '名詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '報道', '陣', 'が', '大勢', '待ち受ける', '支度', '部屋', '外', 'の', 'ミックス', 'ゾーン', 'に', '、', '朝', '乃', '山', 'は', 'すがすがしい', '表情', 'を', '浮かべ', 'て', '歩み寄っ', 'て', 'き', 'た', '。', '「', '今日', 'から', '3', '日間', 'が', '大事', 'と', '思っ', 'て', 'い', 'た', '。', '横綱', '戦', 'も', '2', '回', 'ある', 'と', '思っ', 'て', 'い', 'た', 'の', 'で', '、', '自分', 'の', '相撲', 'を', '取り', 'きる', 'つもり', 'だっ', 'た', '。', 'で', 'も', '取り', '切れ', 'なかっ', 'た', 'の', 'は', '弱い', '自分', 'が', 'い', 'た', '」', '。', '大関', '昇進', 'の', '目安', 'の', '今', '場所', '12', '勝', 'に', '向け', 'て', '、', '後', 'が', 'ない', '状況', 'に', 'も', 'かかわら', 'ず', '率直', 'な', '言葉', 'を', '吐き出し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '動詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '形容詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '形状詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '動詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '動詞', '名詞', '助動詞', '助動詞', '補助記号', '助詞', '助詞', '動詞', '動詞', '助動詞', '助動詞', '助詞', '助詞', '形容詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '形容詞', '名詞', '助詞', '助詞', '動詞', '助動詞', '形状詞', '助動詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '横綱', 'に', '何', 'も', 'さ', 'せ', 'て', 'もらえ', 'なかっ', 'た', '。', '立ち合い', 'で', '得意', 'の', '右', 'を', '差せ', 'ず', '、', '右', '四', 'つ', 'を', '許し', 'て', '上体', 'を', '起こさ', 'れる', 'と', '、', '一気', 'に', '土俵', '際', 'へ', '。', '逆転', 'を', '狙っ', 'て', '右', 'に', '動き', 'ながら', '振り払う', 'と', '、', '同時', 'に', '土俵', 'を', '割っ', 'た', 'か', 'の', 'よう', 'に', '見え', 'た', 'が', '、', '自分', 'の', '左足', 'が', 'ワン', 'テンポ', '速く', '土俵', 'の', '外', 'に', '落ち', 'た', '。', '「', '集中', '力', '、', '厳し', 'さ', 'が', '足り', 'なかっ', 'た', '。', '土俵', '際', 'で', '回っ', 'て', 'おけ', 'ば', 'と', '思っ', 'た', '。', 'そこ', 'も', '弱い', 'ところ', 'です', '」', 'と', '弱点', 'を', '自覚', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '代名詞', '助詞', '動詞', '助動詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '接尾辞', '助詞', '動詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '助詞', '形状詞', '助動詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '形容詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '補助記号', '名詞', '接尾辞', '補助記号', '形容詞', '接尾辞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '助詞', '動詞', '助動詞', '補助記号', '代名詞', '助詞', '形容詞', '名詞', '助動詞', '補助記号', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '大関', '昇進', 'に', '向け', 'て', '、', '試練', 'の', '3', '番', '勝負', 'の', '最初', 'で', 'つまずい', 'た', '。', '14', '日', '目', 'は', '不', '戦勝', 'を', '除い', 'て', '過去', '1', '勝', '1', '敗', 'の', '鶴竜', 'と', '対戦', 'する', '。', '通常', 'なら', '午前', '中', 'に', '行わ', 'れる', '審判', '部', 'に', 'よる', '翌日', 'の', '取組', '編成', '会議', 'が', '、', 'この', '日', 'は', '優勝', '争い', 'を', '見据え', 'て', '全', '取組', '終了', '後', 'と', 'なっ', 'た', 'ため', '、', '朝', '乃', '山', 'が', '会場', 'を', '引き揚げる', '際', 'に', 'は', 'まだ', '14', '日', '目', 'の', '対戦', '相手', 'は', '分から', 'ず', '。', 'それ', 'で', 'も', '「', '先', 'は', '考え', 'ず', 'に', '1', '日', '一番', '、', '自分', 'の', '相撲', 'を', '取り', 'きっ', 'て', '頑張り', 'ます', '」', 'と', '集中', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '接尾辞', '接尾辞', '助詞', '接頭辞', '名詞', '助詞', '動詞', '助詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '助動詞', '名詞', '接尾辞', '助詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助詞', '接頭辞', '名詞', '名詞', '接尾辞', '助詞', '動詞', '助動詞', '名詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '助詞', '副詞', '名詞', '接尾辞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '代名詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助動詞', '助詞', '名詞', '接尾辞', '副詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '場所', '前', 'に', 'は', '行きつけ', 'の', '整骨', '院', 'に', '通い', '、', '曲がっ', 'て', 'い', 'た', '背骨', 'を', '矯正', 'し', 'た', '。', 'その', 'かい', 'あっ', 'て', 'か', '、', '今', '場所', 'も', '力強', 'さ', 'を', '見せ', 'て', 'いる', '。', '泣い', 'て', 'も', '笑っ', 'て', 'も', '、', '今', '場所', 'は', '残り', '二', '番', '。', 'ビシッ', 'と', '2', '連勝', 'で', '締めくくり', '、', '夢', 'の', '大関', '昇進', 'を', 'かなえる', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '助詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '動詞', '補助記号', '動詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '連体詞', '名詞', '動詞', '助詞', '助詞', '補助記号', '接頭辞', '名詞', '助詞', '形容詞', '接尾辞', '助詞', '動詞', '助詞', '動詞', '補助記号', '動詞', '助詞', '助詞', '動詞', '助詞', '助詞', '補助記号', '接頭辞', '名詞', '助詞', '名詞', '名詞', '名詞', '補助記号', '副詞', '助詞', '名詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '蜂窩', '（', 'ほうか', '）', '織', '炎', 'に', 'よる', '発熱', 'で', '途中', '休場', 'し', '、', '11', '日', '目', 'から', '再', '出場', 'し', 'た', '西', '前頭', '15', '枚', '目', '千代', '丸', '（', '28', '＝', '九重', '）', 'が', '、', '復帰', '後', '初めて', '報道', '陣', 'の', '取材', 'に', '対応', 'し', '、', '自身', 'の', '状態', 'に', 'つい', 'て', '説明', 'し', 'た', '。', 'この', '日', 'は', '西', '前頭', '10', '枚', '目', '栃煌山', 'を', '引き落とし', 'で', '破り', '、', '7', '勝目', 'を', '挙げ', 'て', '幕内', '残留', 'に', '大きく', '前進', '。', '11', '日', '目', '、', '12', '日', '目', 'は', '報道', '陣', 'が', '待つ', 'ミックス', 'ゾーン', 'に', '立ち止まら', 'なかっ', 'た', 'が', '、', 'この', '日', 'は', '足', 'を', '止め', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '補助記号', '名詞', '補助記号', '接尾辞', '接尾辞', '助詞', '動詞', '名詞', '助詞', '名詞', '名詞', '動詞', '補助記号', '名詞', '接尾辞', '接尾辞', '助詞', '接頭辞', '名詞', '動詞', '助動詞', '名詞', '名詞', '名詞', '接尾辞', '接尾辞', '名詞', '接尾辞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '名詞', '接尾辞', '副詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '名詞', '名詞', '接尾辞', '接尾辞', '名詞', '助詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助詞', '名詞', '名詞', '助詞', '形容詞', '名詞', '補助記号', '名詞', '接尾辞', '接尾辞', '補助記号', '名詞', '接尾辞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '動詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '一時', 'は', '40', '度', 'まで', '熱', 'が', '上がっ', 'た', '体調', 'に', 'つい', 'て', '「', 'もう', '戻っ', 'た', '。', '体', 'は', '元気', 'です', '」', 'と', '話し', 'た', '。', 'この', '日', 'も', '右足', 'に', '足袋', '、', '左', 'の', 'ふくらはぎ', 'に', 'テーピング', 'を', '施し', 'た', 'が', '「', '意外', 'と', '（', '足', 'に', '）', '痛み', 'は', 'ない', '。', '休場', 'する', '前', 'も', '調子', 'は', '良かっ', 'た', '。', 'その', '感覚', 'も', '残っ', 'て', 'いる', '」', 'と', 'アピール', 'し', 'た', '。', '熱', 'は', '36', '度', '5', '分', 'まで', '下がっ', 'た', 'と', 'いう', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '副詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '助動詞', '補助記号', '助詞', '動詞', '助動詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '形状詞', '助詞', '補助記号', '名詞', '助詞', '補助記号', '名詞', '助詞', '形容詞', '補助記号', '名詞', '動詞', '名詞', '助詞', '名詞', '助詞', '形容詞', '助動詞', '補助記号', '連体詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '助詞', '名詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', '休場', 'が', '続け', 'ば', '十', '両', '陥落', 'の', '危機', 'だっ', 'た', 'が', '「', 'リンパ', 'が', '腫れ', 'て', '痛かっ', 'た', 'から', '蜂窩', '（', 'ほうか', '）', '織', '炎', 'だ', 'と', '思っ', 'た', '。', 'いつ', 'で', 'も', '出', 'られる', 'よう', 'な', '気持ち', 'で', 'い', 'た', '」', 'と', '淡々', '。', '「', '一応', '、', '病院', 'で', '点滴', 'を', '打つ', '」', 'と', '、', '足早', 'に', '会場', 'を', '引き揚げ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '動詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '形容詞', '助動詞', '助詞', '名詞', '補助記号', '名詞', '補助記号', '接尾辞', '接尾辞', '助動詞', '助詞', '動詞', '助動詞', '補助記号', '代名詞', '助詞', '助詞', '動詞', '助動詞', '形状詞', '助動詞', '名詞', '助動詞', '動詞', '助動詞', '補助記号', '助詞', '形状詞', '補助記号', '補助記号', '名詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '補助記号', '助詞', '補助記号', '形状詞', '助動詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '1', '番', 'で', 'も', '多く', 'の', '好', '取組', 'を', '－', '。', '日本', '相撲', '協会', 'の', '審判', '部', 'は', '14', '日', '目', 'の', '幕内', '取組', 'を', '、', '13', '日', '目', 'の', '全', '取組', '終了', '後', 'に', '編成', '会議', 'を', '開い', 'て', '決め', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '助詞', '名詞', '助詞', '接頭辞', '名詞', '助詞', '補助記号', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '補助記号', '名詞', '接尾辞', '接尾辞', '助詞', '接頭辞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '通常', '、', '翌日', 'の', '取組', 'は', '前日', '午前', '中', 'の', '取組', '編成', '会議', 'で', '決める', 'こと', 'に', 'なっ', 'て', 'いる', '。', 'ただ', '昨年', 'から', '、', '優勝', '争い', 'や', '三', '賞', '選考', '、', '勝ち越し', 'や', '負け越し', 'に', '絡む', 'こと', 'など', 'を', '考慮', 'し', '、', '千秋', '楽', 'の', '取組', 'は', '14', '日', '目', 'の', '全', '取組', '終了', '後', 'に', '決める', 'こと', 'が', '、', 'ほぼ', '慣例', 'と', 'なっ', 'て', 'い', 'た', '。', 'これ', 'を', '今', '場所', 'は', '、', '14', '日', '目', 'の', '取組', 'も', 'ギリギリ', 'まで', '優勝', '争い', 'を', '見据え', '、', '結果', 'が', '出', 'た', '時点', 'で', '取組', 'を', '編成', 'する', 'こと', 'に', 'なっ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '接続詞', '名詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '助詞', '名詞', '接尾辞', '接尾辞', '助詞', '接頭辞', '名詞', '名詞', '接尾辞', '助詞', '動詞', '名詞', '助詞', '補助記号', '副詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '代名詞', '助詞', '接頭辞', '名詞', '助詞', '補助記号', '名詞', '接尾辞', '接尾辞', '助詞', '名詞', '助詞', '副詞', '助詞', '名詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '12', '日', '目', '終了', '時点', 'で', '単独', 'トップ', 'に', '立っ', 'て', 'い', 'た', '平幕', '下位', 'の', '碧山', '（', '33', '＝', '春日野', '）', 'が', '敗れ', '、', 'トップ', 'は', '2', '敗', 'で', '3', '人', 'が', '並び', '、', '1', '差', 'の', '3', '敗', 'に', '3', '人', 'が', 'つける', 'と', 'いう', '再び', '大', '混戦', 'に', 'なっ', 'た', '。', 'この', '結果', 'を', '受け', 'て', '14', '日', '目', 'に', 'は', '、', '結び', 'の', '一', '番', 'は', '想定', '通り', 'に', '横綱', '鶴竜', '－', '関脇', '朝', '乃', '山', '戦', 'が', '組ま', 'れ', 'た', 'が', '、', 'その', '一番', '前', 'は', '横綱', '白鵬', '－', '碧山', 'の', '2', '敗', '対決', 'が', '組ま', 'れ', 'た', '。', '番付', '優先', 'の', '通常', 'なら', '白鵬', '－', '貴', '景勝', 'の', '横綱', '、', '大関', '戦', 'が', '組ま', 'れる', 'が', '、', '6', '勝', '7', '敗', 'の', '貴', '景勝', 'の', '不振', 'も', 'あり', '、', '優勝', '争い', 'を', '優先', 'し', 'た', '格好', 'だ', '。', '千秋', '楽', 'は', '白鵬', '－', '鶴竜', '戦', 'の', '横綱', '対決', 'が', '確実', 'に', '組ま', 'れる', 'と', '思わ', 'れる', '。', 'この', 'ため', '貴', '景勝', 'は', '、', '横綱', '戦', '2', 'つ', 'の', 'うち', '白鵬', '戦', 'が', '今', '場所', '、', 'なくなる', 'こと', 'が', '確実', 'と', 'なっ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '接尾辞', '接尾辞', '名詞', '名詞', '助詞', '形状詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '接尾辞', '助詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '接尾辞', '助詞', '動詞', '助詞', '動詞', '副詞', '接頭辞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '連体詞', '名詞', '助詞', '動詞', '助詞', '名詞', '接尾辞', '接尾辞', '助詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '動詞', '助動詞', '助動詞', '助詞', '補助記号', '連体詞', '副詞', '名詞', '助詞', '名詞', '名詞', '補助記号', '名詞', '助詞', '名詞', '接尾辞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助動詞', '名詞', '補助記号', '接頭辞', '名詞', '助詞', '名詞', '補助記号', '名詞', '接尾辞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '名詞', '名詞', '接尾辞', '助詞', '接頭辞', '名詞', '助詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '助動詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '形状詞', '助動詞', '動詞', '助動詞', '助詞', '動詞', '助動詞', '補助記号', '連体詞', '名詞', '接頭辞', '名詞', '助詞', '補助記号', '名詞', '接尾辞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '接尾辞', '助詞', '接頭辞', '名詞', '補助記号', '動詞', '名詞', '助詞', '形状詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '審判', '部', 'の', '柔軟', 'と', 'も', 'いえる', '対応', 'に', '八角', '理事', '長', '（', '元', '横綱', '北勝海', '）', 'は', '「', 'いい', '取組', 'を', '作る', 'ため', 'に', '審判', '部', 'が', 'ね', '（', '考え', 'た', 'こと', '）', '。', '横綱', '、', '大関', 'が', '少ない', '（', '3', '人', '）', 'と', 'いう', 'こと', 'も', 'ある', '。', '普通', 'なら', '横綱', '、', '大関', '陣', 'で', '優勝', '争い', 'だ', 'から', '」', 'と', '、', 'いわゆる', '「', '割', '崩し', '」', 'に', '理解', 'を', '示し', 'た', '。', '今', '場所', 'は', '史上', '初', 'の', '無', '観客', '開催', 'と', 'なっ', 'た', 'だけ', 'に', '、', '最後', 'まで', 'より', '優勝', '争い', 'を', '面白く', '－', 'と', 'いう', '意図', 'が', 'かいま', '見える', '判断', 'だ', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '形状詞', '助詞', '助詞', '動詞', '名詞', '助詞', '名詞', '名詞', '接尾辞', '補助記号', '名詞', '名詞', '名詞', '補助記号', '助詞', '補助記号', '形容詞', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '名詞', '助詞', '助詞', '補助記号', '動詞', '助動詞', '名詞', '補助記号', '補助記号', '名詞', '補助記号', '名詞', '助詞', '形容詞', '補助記号', '名詞', '接尾辞', '補助記号', '助詞', '動詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助動詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '助動詞', '助詞', '補助記号', '助詞', '補助記号', '連体詞', '補助記号', '名詞', '動詞', '補助記号', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '接頭辞', '名詞', '助詞', '名詞', '名詞', '助詞', '接頭辞', '名詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '助詞', '名詞', '名詞', '助詞', '形容詞', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '動詞', '名詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '1', '場所', 'で', 'の', '十', '両', '復帰', 'を', '目指し', 'た', 'もの', 'の', '、', '6', '番', '相撲', 'で', '負け越し', 'が', '決まり', '、', '場所', '後', 'の', '再', '十両', 'の', '可能', '性', 'が', '消え', 'た', '東', '幕下', '2', '枚', '目', '豊ノ島', '（', '36', '＝', '時津風', '）', 'が', '、', '今', '場所', '最後', 'の', '7', '番', '相撲', 'に', '登場', '。', '幕内', '上位', 'など', 'で', '過去', '15', '度', 'の', '対戦', '（', '11', '勝', '4', '敗', '）', 'が', 'ある', '西', '幕下', '6', '枚', '目', 'の', '豊', '響', '（', '35', '＝', '境川', '）', 'と', '対戦', 'し', 'た', '。', 'わずか', 'に', '右', 'を', 'のぞか', 'せ', 'た', 'が', '、', '立ち合い', 'から', '圧', '力負け', 'し', '、', 'その', '右', 'を', '強烈', 'に', 'おっつけ', 'られ', 'ズルズル', '後退', '。', '真後ろ', 'に', 'はたい', 'た', 'が', '、', '左足', 'を', '踏み越し', '押し出し', 'で', '敗れ', 'た', '。', '復活', 'を', 'かけ', 'た', '場所', 'は', '2', '勝', '5', '敗', 'で', '終わっ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '接尾辞', '助詞', '接頭辞', '名詞', '助詞', '形状詞', '接尾辞', '助詞', '動詞', '助動詞', '名詞', '名詞', '名詞', '接尾辞', '接尾辞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '接頭辞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '接尾辞', '補助記号', '助詞', '動詞', '名詞', '名詞', '名詞', '接尾辞', '接尾辞', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '動詞', '助動詞', '補助記号', '形状詞', '助動詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '動詞', '補助記号', '連体詞', '名詞', '助詞', '形状詞', '助動詞', '動詞', '助動詞', '副詞', '名詞', '補助記号', '名詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '名詞', '名詞', '接尾辞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '何', '度', 'も', '顔', 'を', '合わせ', 'た', '相手', 'と', 'の', '対戦', 'を', '「', '幕内', 'で', '何', '度', 'も', '対戦', 'が', 'ある', 'から', '、', 'ちょっと', '何', 'か', '気負い', 'すぎ', 'た', 'か', 'な', '、', '立ち合い', 'が', '高かっ', 'た', '。', 'もう', 'ちょっと', '、', 'いい', '相撲', 'を', '取り', 'たかっ', 'た', '」', 'と', '振り返っ', 'た', '。', '場所', '全体', 'を', '振り返り', '「', 'やっぱり', '切れ', 'が', '、', 'いろいろ', 'と', 'ね', '、', '落ち', 'て', 'き', 'た', 'な', 'と', '思う', 'し', '、', '感じ', 'ます', 'ね', '」', 'と', '素直', 'に', '吐露', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '助詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '副詞', '代名詞', '助詞', '動詞', '動詞', '助動詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '形容詞', '助動詞', '補助記号', '副詞', '副詞', '補助記号', '形容詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '助詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '補助記号', '副詞', '名詞', '助詞', '補助記号', '副詞', '助詞', '助詞', '補助記号', '動詞', '助詞', '動詞', '助動詞', '助詞', '助詞', '動詞', '助詞', '補助記号', '動詞', '助動詞', '助詞', '補助記号', '助詞', '形状詞', '助動詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '幕下', '陥落', 'が', '決まっ', 'た', '1', '月', 'の', '初', '場所', '千秋', '楽', '。', '「', '自分', 'の', '中', 'で', 'は', '、', 'やり', 'きっ', 'た', 'と', 'いう', '思い', 'は', 'ある', '」', 'と', '引退', 'で', 'ほぼ', '固まっ', 'て', 'い', 'た', '。', 'それ', 'を', '翻意', 'し', 'た', '裏', 'に', 'は', '、', '一', '粒', '種', 'の', '長女', '希', '歩', 'ちゃん', '（', '7', '）', 'の', '存在', 'だ', '。', '幕下', '以下', 'に', '落ち', '無給', '生活', 'に', 'なる', 'こと', 'を', '、', 'けなげ', 'に', 'も', '7', '歳', 'で', '分かっ', 'て', 'い', 'た', 'そう', 'で', '「', '私', 'が', '貸し', 'て', 'あげる', '」', 'と', '泣き', 'ながら', '相撲', 'を', '続ける', 'こと', 'を', '訴え', 'られ', 'た', 'と', 'いう', '。', 'さらに', '、', '初', '場所', 'で', 'は', '親', 'や', '家族', 'を', '場所', 'に', '呼ぶ', 'こと', 'なく', '、', '関取', 'の', '座', 'を', '失っ', 'た', '。', '「', '家族', 'も', '両親', 'も', '見', 'に', '来', 'させ', 'られ', 'なかっ', 'た', '。', 'それ', 'で', 'いい', 'の', 'か', '、', 'と', '思っ', 'た', '」', 'と', '、', 'なえ', 'た', '気持ち', 'を', '奮い立た', 'せ', 'て', '臨ん', 'だ', '今', '場所', 'の', '土俵', 'だっ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '補助記号', '補助記号', '名詞', '助詞', '名詞', '助詞', '助詞', '補助記号', '動詞', '動詞', '助動詞', '助詞', '動詞', '名詞', '助詞', '動詞', '補助記号', '助詞', '名詞', '助詞', '副詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '代名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '助詞', '助詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '接頭辞', '名詞', '接尾辞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '助動詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '名詞', '名詞', '助詞', '動詞', '名詞', '助詞', '補助記号', '形状詞', '助動詞', '助詞', '名詞', '接尾辞', '助詞', '動詞', '助詞', '動詞', '助動詞', '名詞', '助動詞', '補助記号', '代名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '助詞', '動詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '助詞', '動詞', '補助記号', '接続詞', '補助記号', '名詞', '名詞', '助詞', '助詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '形容詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助動詞', '助動詞', '助動詞', '補助記号', '代名詞', '助詞', '形容詞', '助詞', '助詞', '補助記号', '助詞', '動詞', '助動詞', '補助記号', '助詞', '補助記号', '動詞', '助動詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '助動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'その', '今', '場所', 'は', '、', '家族', 'を', '会場', 'に', '呼び寄せよう', 'に', 'も', '無', '観客', '開催', '。', '「', '最後', 'の', '姿', '」', 'を', '見せる', 'こと', 'は', '来場', '所', '以降', 'に', '持ち越し', 'だ', '。', 'だ', 'が', '、', 'その', 'こと', 'に', '豊ノ島', '本人', 'は', 'こだわっ', 'て', 'い', 'ない', '。', '「', '最後', 'だ', 'から', '（', '家族', 'に', '）', '見せ', 'たい', 'と', 'いう', 'こと', 'に', 'は', '、', 'こだわっ', 'て', 'ない', '。', '見', 'て', 'ほしい', 'けど', '、', 'だ', 'から', 'と', 'いっ', 'て', '…', '（', 'その', '理由', 'だけ', 'で', '続ける', 'と', '決断', 'する', '）', 'こと', 'は', 'ない', 'し', '、', 'テレビ', 'で', 'も', '見', 'てる', 'でしょう', '。', 'そんな', '中途', '半端', 'な', '気持ち', 'で', 'は', '…', '」', 'と', '話す', '。', '現状', 'で', 'は', '「', '幕下', 'で', '負け越し', 'て', '、', 'なかなか', '気持ち', 'を', '（', '土俵', 'に', '）', '持っ', 'て', 'いく', 'の', 'は', '難しい', '。', 'これ', 'だけ', '長く', 'やっ', 'て', '、', '下', 'に', '下がっ', 'て', '負け越し', 'た', 'こと', 'で', '、', '1', 'つ', 'の', '決断', 'を', 'する', '時', 'か', 'な', '、', 'と', 'か', 'いろいろ', 'な', '思い', 'が', 'ある', '。', '（', '現役', 'を', '）', '続ける', '気持ち', 'に', '持っ', 'て', 'いける', 'か', '…', '」', 'と', '苦悩', 'する', '胸', 'の', '内', 'を', '明かし', 'た', '。', '冗談', 'っぽく', '「', '（', '決断', 'は', '）', '娘', 'と', 'の', '闘い', 'か', 'な', '」', 'と', '少し', 'だけ', '笑い', '「', 'とりあえず', '場所', 'は', '終わっ', 'た', 'の', 'で', '、', 'ゆっくり', '進退', 'を', '考え', 'たい', 'と', '思い', 'ます', '」', 'と', '、', 'こみ上げる', 'もの', 'を', '抑える', 'よう', 'に', '話し', 'た', '。', '']\n",
            "['BOS/EOS', '連体詞', '名詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '助詞', '接頭辞', '名詞', '名詞', '補助記号', '補助記号', '名詞', '助詞', '名詞', '補助記号', '助詞', '動詞', '名詞', '助詞', '名詞', '接尾辞', '名詞', '助詞', '名詞', '助動詞', '補助記号', '助動詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '補助記号', '名詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '補助記号', '動詞', '助動詞', '助詞', '動詞', '名詞', '助詞', '助詞', '補助記号', '動詞', '助動詞', '助動詞', '補助記号', '動詞', '助詞', '形容詞', '助詞', '補助記号', '助動詞', '助詞', '助詞', '動詞', '助詞', '補助記号', '補助記号', '連体詞', '名詞', '助詞', '助詞', '動詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '助詞', '形容詞', '助詞', '補助記号', '名詞', '助詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '連体詞', '名詞', '名詞', '助動詞', '名詞', '助詞', '助詞', '補助記号', '補助記号', '助詞', '動詞', '補助記号', '名詞', '助詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '補助記号', '副詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '補助記号', '動詞', '助詞', '動詞', '助詞', '助詞', '形容詞', '補助記号', '代名詞', '助詞', '形容詞', '動詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '名詞', '助動詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '助詞', '補助記号', '助詞', '助詞', '形状詞', '助動詞', '名詞', '助詞', '動詞', '補助記号', '補助記号', '名詞', '助詞', '補助記号', '動詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助詞', '補助記号', '補助記号', '助詞', '名詞', '動詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '接尾辞', '補助記号', '補助記号', '名詞', '助詞', '補助記号', '名詞', '助詞', '助詞', '名詞', '助詞', '助詞', '補助記号', '助詞', '副詞', '助詞', '名詞', '補助記号', '副詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '助動詞', '補助記号', '副詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '動詞', '助動詞', '補助記号', '助詞', '補助記号', '動詞', '名詞', '助詞', '動詞', '形状詞', '助動詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '新型', 'コロナ', 'ウイルス', 'の', '感染', '拡大', 'の', 'ため', '多く', 'の', 'スポーツ', 'イベント', 'が', '中止', 'と', 'なる', 'なか', '、', '米', '男子', 'ゴルフ', 'の', 'タイガー', '・', 'ウッズ', '（', '米国', '）', 'が', '16', '日', '、', '自身', 'の', 'ツイッター', 'を', '更新', 'し', '「', '今', 'は', 'ゴルフ', 'より', '大事', 'な', 'こと', 'が', 'たくさん', 'ある', '」', 'と', 'コメント', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '形状詞', '助動詞', '名詞', '助詞', '副詞', '動詞', '補助記号', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '昨年', 'の', '優勝', 'で', '復活', 'を', '印象', '付け', 'た', 'マスターズ', '・', 'トーナメント', 'の', '延期', 'が', '決まっ', 'た', 'が', '「', '安全', 'が', '第', '一', 'で', '、', '愛する', '人', 'の', 'ため', 'に', '、', 'ベスト', 'の', '選択', 'を', 'する', '必要', 'が', 'ある', '」', 'と', '続け', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '接頭辞', '名詞', '助詞', '補助記号', '動詞', '名詞', '助詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '動詞', '補助記号', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '米', '女子', 'プロ', 'ゴルフ', '協会', '（', 'LPGA', '）', 'は', '20', '日', '、', '新型', 'コロナ', 'ウイルス', 'の', '感染', '拡大', 'に', 'より', 'ロッテ', '選手', '権', '（', 'ハワイ', '州', '）', 'など', '4', '月', 'に', '始まる', '米', 'ツアー', '3', '大会', 'の', '延期', 'を', '発表', 'し', 'た', '。', '2', '月', '後半', 'の', 'ホンダ', 'LPGA', '（', 'タイ', '）', 'から', '9', '大会', 'が', '休止', 'と', 'なり', '、', '再開', 'は', '早く', 'て', 'も', '5', '月', '中旬', 'と', 'なる', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '名詞', '名詞', '接尾辞', '補助記号', '名詞', '名詞', '補助記号', '助詞', '名詞', '名詞', '助詞', '動詞', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '形容詞', '助詞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'また', '4', '月', '2', '～', '5', '日', 'から', 'の', '延期', 'が', '決まっ', 'て', 'い', 'た', 'メジャー', '第', '1', '戦', 'の', 'ANA', 'インスピレーション', '（', 'カリフォルニア', '州', '）', 'を', '9', '月', '10', '～', '13', '日', 'に', '開催', 'する', 'と', '発表', 'し', 'た', '。', '国内', '4', '大', '大会', 'の', '日本', '女子', 'プロ', '選手', '権', 'コニカ', 'ミノルタ', '杯', '（', '岡山', '）', 'と', '同じ', '週', '。', '']\n",
            "['BOS/EOS', '接続詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '接尾辞', '助詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '名詞', '接頭辞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '補助記号', '名詞', '名詞', '補助記号', '助詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '動詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '接頭辞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '連体詞', '名詞', '補助記号', 'BOS/EOS']\n",
            "['', '日本', 'ツアー', 'も', '3', '月', '初め', 'の', '第', '1', '戦', 'から', '5', '週', '連続', 'で', '中止', 'が', '決定', '。', '世界', 'ランキング', 'で', '東京', 'オリンピック', '（', '五輪', '）', '出場', '圏', '内', 'に', 'いる', '渋', '野', '日向子', '、', '鈴木', '愛', 'は', 'シーズン', '初戦', 'を', '迎え', 'られ', 'ず', '、', '畑', '岡', '奈', '紗', 'も', '米', 'ツアー', 'で', '2', '位', 'に', '2', '度', '入っ', 'た', '1', '月', 'を', '最後', 'に', '実戦', 'を', '離れ', 'て', 'いる', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '接頭辞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '接尾辞', '接尾辞', '助詞', '動詞', '名詞', '接尾辞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '助動詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'ゴルフ', 'の', '男女', 'の', '世界', 'ランキング', 'は', '20', '日', '、', '担当', '者', 'の', '共同', '声明', 'で', '男子', 'は', '15', '日', '付', '、', '女子', 'は', '16', '日', '付', 'を', '最後', 'に', '当面', '凍結', 'する', 'と', '発表', 'さ', 'れ', 'た', '。', '新型', 'コロナ', 'ウイルス', '感染', '拡大', 'で', 'ポイント', 'の', '対象', 'と', 'なっ', 'て', 'いる', '大会', 'が', '中止', 'と', 'なっ', 'て', 'いる', 'こと', 'に', '対応', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '助詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '接尾辞', '接尾辞', '補助記号', '名詞', '助詞', '名詞', '接尾辞', '接尾辞', '助詞', '名詞', '助詞', '副詞', '名詞', '動詞', '助詞', '名詞', '動詞', '助動詞', '助動詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '東京', 'オリンピック', '（', '五輪', '）', 'の', '出場', '権', '決定', 'に', '用いる', '五輪', 'ランキング', 'は', '世界', 'ランク', 'が', '基準', '。', '国際', 'ゴルフ', '連盟', '（', 'IGF', '）', 'は', '「', 'システム', 'が', '東京', '五輪', 'の', '出場', '資格', 'を', '狙う', '全て', 'の', '選手', 'に', 'とっ', 'て', '公平', 'で', 'ある', 'こと', 'は', '変わら', 'ない', '」', 'と', '声明', 'を', '出し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '接尾辞', '名詞', '助詞', '動詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '助動詞', '動詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '国内', '女子', 'ゴルフ', 'の', 'ツアー', '第', '4', '戦', '、', 'アクサ', '・', 'レディース', '（', '27', '～', '29', '日', '、', '宮崎', '・', 'UMKCC', '）', 'の', '中止', 'が', '発表', 'に', 'なっ', 'た', 'こと', 'を', '受け', '、', '昨年', 'の', '同', '大会', 'で', 'プロ', '初', '優勝', 'を', '飾っ', 'た', '河本', '結', '（', '21', '＝', 'リコー', '）', 'が', '、', '胸', 'の', '内', 'を', '明かし', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '助詞', '名詞', '接頭辞', '名詞', '接尾辞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '名詞', '接尾辞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '動詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '助動詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '16', '日', '、', '自身', 'の', 'インス', 'タグ', 'ラム', 'を', '更新', '。', '河本', 'は', '「', 'ディフェンディング', 'チャンピオン', 'と', 'し', 'て', '、', 'また', 'ホステス', 'プロ', 'と', 'し', 'て', 'も', 'とても', '楽しみ', 'に', 'し', 'て', 'い', 'まし', 'た', 'が', '、', '今', 'の', '状況', 'を', '考え', 'ます', 'と', '、', '中止', 'は', 'やむ', 'を', '得', 'ない', 'と', '思い', 'ます', '。', '1', '日', 'で', 'も', '早く', '事態', 'が', '収束', 'する', 'こと', 'を', '願っ', 'て', 'い', 'ます', '。', '試合', 'が', '再開', 'し', 'た', '際', 'に', '最高', 'な', 'プレー', 'が', 'お', '見せ', 'できる', 'よう', 'しっかり', '準備', 'し', 'て', 'おき', 'たい', 'と', '思い', 'ます', '」', 'など', 'と', 'つづっ', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '接尾辞', '補助記号', '名詞', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '接続詞', '名詞', '名詞', '助詞', '動詞', '助詞', '助詞', '副詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '接尾辞', '助詞', '助詞', '形容詞', '名詞', '助詞', '名詞', '動詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '動詞', '助動詞', '名詞', '助詞', '名詞', '助動詞', '名詞', '助詞', '接頭辞', '動詞', '動詞', '形状詞', '副詞', '名詞', '動詞', '助詞', '動詞', '助動詞', '助詞', '動詞', '助動詞', '補助記号', '助詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '大', '逆転', 'で', 'の', '東京', '五輪', '出場', 'を', '目指す', '河本', 'は', '、', '今季', 'から', '米', 'ツアー', 'に', '参戦', 'し', 'て', 'い', 'た', 'が', '、', 'メジャー', '初戦', 'と', 'なる', '4', '月', 'の', 'ANA', 'インスピレーション', 'も', '延期', 'が', '決定', '。', '米', '疾病', '対策', 'センター', 'は', '今後', '8', '週間', '、', '50', '人', '以上', '集まる', 'イベント', 'の', '中止', 'か', '延期', 'を', '求め', 'て', 'おり', '、', '米', 'ツアー', 'は', '再開', 'の', '見通し', 'が', '立っ', 'て', 'い', 'ない', '。', '']\n",
            "['BOS/EOS', '接頭辞', '名詞', '助詞', '助詞', '名詞', '名詞', '名詞', '助詞', '動詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '動詞', '助詞', '動詞', '助動詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '動詞', '名詞', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '接尾辞', '名詞', '動詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '全米', 'プロ', 'ゴルフ', '協会', '（', 'PGA', '）', 'は', '13', '日', '、', '開催', '中', 'の', '米', 'ツアー', '、', 'ザ', '・', 'プレーヤーズ', '選手', '権', 'の', '第', '2', 'ラウンド', '以降', 'の', '中止', 'と', '、', '4', '月', 'の', 'マスターズ', 'まで', 'の', '4', '試合', 'の', '中止', 'を', '発表', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '接尾辞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '名詞', '接尾辞', '助詞', '接頭辞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', '12', '日', 'に', '始まっ', 'た', 'ザ', '・', 'プレーヤーズ', '選手', '権', 'は', '、', '第', '1', 'ラウンド', 'は', 'ギャラリー', 'を', '入れ', 'て', '開催', 'し', 'た', '。', '3', 'シーズン', 'ぶり', 'の', 'ツアー', '優勝', 'を', '目指す', '松山', '英樹', '（', '27', '＝', 'LEXUS', '）', 'は', '、', 'スタート', 'の', '10', '番', 'から', '4', '連続', 'バーディー', '。', 'コース', 'レコード', 'タイ', 'の', '63', 'を', 'マーク', 'し', '、', '9', 'アンダー', 'で', '暫定', 'の', '単独', '首位', 'に', '立っ', 'て', 'い', 'た', '。', '']\n",
            "['BOS/EOS', '名詞', '接尾辞', '助詞', '動詞', '助動詞', '名詞', '補助記号', '名詞', '名詞', '接尾辞', '助詞', '補助記号', '接頭辞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', '名詞', '名詞', '接尾辞', '助詞', '名詞', '名詞', '助詞', '動詞', '名詞', '名詞', '補助記号', '名詞', '補助記号', '名詞', '補助記号', '助詞', '補助記号', '名詞', '助詞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '動詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '助詞', '形状詞', '名詞', '助詞', '動詞', '助詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "['', 'その', '好', '発進', 'も', '幻', 'と', '化し', 'た', '。', '第', '1', '日', '終了', '後', 'に', '第', '2', 'ラウンド', '以降', 'は', '無', '観客', 'で', '行う', 'と', '発表', 'し', 'て', 'い', 'た', 'PGA', 'は', '、', 'わずか', '数', '時間', 'で', '方針', 'を', '変更', '。', '同', '大会', 'から', '、', '今年', '最初', 'の', 'メジャー', '、', 'マスターズ', '前週', 'の', 'バレロ・テキサス・オープン', '（', '4', '月', '2', '日', '開幕', '）', 'まで', 'の', '全', '試合', 'を', '中止', 'する', 'と', '発表', 'し', 'た', '。', '']\n",
            "['BOS/EOS', '連体詞', '接頭辞', '名詞', '助詞', '名詞', '助詞', '動詞', '助動詞', '補助記号', '接頭辞', '名詞', '接尾辞', '名詞', '接尾辞', '助詞', '接頭辞', '名詞', '名詞', '名詞', '助詞', '接頭辞', '名詞', '助詞', '動詞', '助詞', '名詞', '動詞', '助詞', '動詞', '助動詞', '名詞', '助詞', '補助記号', '副詞', '名詞', '名詞', '助詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '助詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '接尾辞', '名詞', '補助記号', '助詞', '助詞', '接頭辞', '名詞', '助詞', '名詞', '動詞', '助詞', '名詞', '動詞', '助動詞', '補助記号', 'BOS/EOS']\n",
            "registered words: 1395\n",
            "registered part of speech: 1395\n",
            "(70,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vaTfdo0zfNT",
        "outputId": "4943d7b7-32f9-4276-e6cb-be925ae79dbf"
      },
      "source": [
        "print(len(dictionary))\n",
        "print(dictionary)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1395\n",
            "[' ', '', 'インターネット', 'サイト', '「', 'YouTube', '」', 'の', '球団', '公式', 'チャンネル', 'が', '話題', 'に', 'なっ', 'て', 'いる', '。', '17', '日', 'は', 'キャプテン', '糸原', '練習', '中', '井上', '打撃', 'コーチ', 'ドッキリ', 'を', '仕掛ける', '映像', 'アップ', '18', '小型', 'カメラ', '頭', '装着', 'し', 'た', '元', '外野', '手', '緒方', '凌', '介', '広報', '、', '新', '助っ人', 'ボーア', '強烈', 'な', '打球', 'キャッチ', '挑戦', '（', '詳細', 'で', '）', 'わずか', '2', '再生', '回数', '15', '万', '回', '超え', '阪神', '新型', 'コロナ', 'ウイルス', '感染', '拡大', 'シーズン', '開幕', '延期', 'さ', 'れる', '野球', '以外', 'ファン', '向け', '何', 'できる', 'か', 'チーム', 'と', '検討', 'き', '3', '月', '12', '前', '監督', '選手', '関係', '者', '室内', '場', '緊急', 'ミーティング', '実施', 'そこ', '出', '今年', '開設', '活用', 'だっ', '企画', '発案', 'から', '撮影', '編集', '…', 'とっ', 'も', '大きな', '負担', 'なる', '矢野', 'こう', 'いう', '見', 'み', 'たい', 'って', '含め', 'SNS', 'など', '通じ', '発信', 'いき', '話し', 'い', 'ちなみ', '軍', '戦', '視察', 'ため', 'その', '瞬間', '見る', 'こと', 'でき', 'なかっ', '作戦', '成功', 'ご', '満悦', '試合', 'けど', '森', '4', '打数', '安打', '記録', '残ら', 'ず', 'かわいそう', 'ね', '彼', '求め', 'もの', 'シンプル', 'ん', 'じゃ', 'ない', '先発', '浜屋', 'もっと', '大胆', '投げ', 'ほしかっ', '広島', '大瀬', '良', '大地', '投手', '28', '当初', '予定', '中日', 'さながら', '最長', '7', '打席', '初めて', '立っ', '6', 'まで', '単打', '本', '無', '失点', '不運', '当たり', 'や', '浮い', '球', '捉え', 'られ', 'それ', '僕', '結果', 'ほど', '悲観', 'する', '思っ', '向い', '1', '直球', '得意', 'カット', 'ボール', '押す', '組み立て', '変え', 'カーブ', 'フォークボール', '多', '投', 'あまり', '使わ', '真っすぐ', 'フォーク', '抑え', '違っ', 'バリエーション', '配球', 'いい', '見せ', '今後', 'つながっ', 'いく', '思い', 'ます', 'スライダー', 'いずれ', '違う', '決め球', '阿部', '平田', '京田', '連続', '三振', '投球', '幅', '広がる', '可能', '性', '感じ', '前回', '13', 'ソフト', 'バンク', '5', '修正', '改善', '続け', '101', 'スタミナ', '面', '不安', '加え', '現', '時点', '最短', '10', '当たる', '目先', '変える', '餌', 'まく', 'バッテリー', '組ん', 'だ', '会沢', '意味', '合い', '当然', 'ある', '踏まえ', 'いろいろ', '話', '振り返っ', '初', '実戦', '死', '一塁', 'バスター', '決め', '内野', '実り', '弾み', 'つけ', '右腕', 'あと', '微', '調整', 'やり', '集中', '先', '見据え', '度', '登板', '機会', '空け', '次回', '第', '週', '週末', '定め', '合わせ', '巨人', '慎之助', '20', '41', '歳', '誕生', '迎え', 'ジャイアンツ', '球場', '行わ', 'れ', '育成', '笠井', '中心', 'バースデー', 'ソング', '祝わ', 'ブルペン', 'デー', 'ヤクルト', '若手', '陣', 'アピール', '年', '目', '清水', '番手', 'ドラフト', '位', '大西', '完璧', '9', '加入', '左腕', '長谷川', '凡退', '高津', 'つい', '右', '打者', '対し', 'たかっ', 'らし', '出し', 'くれ', '評価', '外国', '人', 'サンチェス', '21', 'DeNA', '約', '週間', '杉内', 'ら', 'アドバイス', '受け', 'ごと', '新球', 'へ', 'チェンジ', '方法', '実践', '日本', '慣れる', '策', 'つ', '流', '落とし込み', 'ながら', 'これ', '同じ', 'アプローチ', 'やっ', 'よ', '昨季', '韓国', '勝', '挙げ', '力', '料理', 'プロ', '本来', '始まっ', '守護', '神', '転向', '楽天', '松井', '注目', '福留', '孝介', '42', '神宮', 'ハッスル', '猛打', '賞', '左中', '間', '激走', '二塁', '打', '放つ', '後', '中前', '右前', '全', '方位', '打ち分け', '余波', '決まら', 'モチベーション', '維持', '難しい', '日々', '続く', '球界', '最', '年長', '不動', '心', '今日', '強い', '気持ち', '体現', '叱咤', 'しっ', '浜口', '遥', '大', '25', '結婚', '発表', '相手', '恵理子', 'さん', '東京', '出身', '会社', '員', '女優', '仲', '里依', '紗', '似', '美人', '大学', '時代', '知り合い', '交際', '期間', '経', '昨年', '横浜', 'みなと', 'みらい', 'プロポーズ', '今月', '中旬', '婚姻', '届', '提出', '知人', '描い', 'もらっ', '似顔', '絵', 'すごく', '明るく', '前向き', 'ところ', 'ありがたい', 'です', 'うまく', 'いか', 'ポジティブ', '支え', 'まし', '？', '\\u3000', '作っ', 'おいしい', 'のろけ', '近く', '同居', '始める', '神奈川', '16', '入団', 'シリーズ', '進出', '貢献', 'ここ', 'ケタ', '勝利', '終わっ', '今季', 'ローテーション', '入り', '確実', '妻', '家族', 'みんな', '期待', '受ける', '遅れ', '五輪', 'あっ', 'たり', 'やっぱり', '特別', '言葉', '込め', 'ハマ', 'ちゃん', '自覚', '新た', 'さらなる', '飛躍', '目指す', 'サッカー', '協会', 'JFA', '田嶋', '幸三', '会長', '62', '14', '発症', '防止', '講じる', '保健', '所', '指導', '下', '23', '・', '文京', '区', 'ハウス', '内', '消毒', '作業', '27', '役職', '在宅', '勤務', '継続', '来館', '立ち入り', '禁止', 'また', '参加', '理事', '会', '出席', '行動', '履歴', '確認', 'うえ', 'おり', '発熱', '風邪', '症状', '訴える', '公表', '悪化', 'おら', '検査', '治療', '模様', 'J', 'リーグ', '村井', '満', 'チェアマン', '60', '19', '経営', '難', '陥っ', 'サガン', '鳥栖', '協議', '認め', 'ウェブ', 'よる', '実行', '委員', '会見', 'PL', '損益', '計算', '書', '上', '問題', 'キャッシュ', 'フロー', '議論', 'あり', 'この', 'あたり', '精査', '現在', 'クラブ', '担当', 'レベル', '意見', '交換', '危機', '報告', '直前', '事務', '局', 'ライセンス', '制度', 'ルール', '基づい', '末', '定期', '的', '資金', '繰り', '重ね', '億', '円', '以上', '赤字', '苦しん', '仮', '安定', '開催', '融資', '適用', 'しよう', '返却', 'めど', '立た', '難しく', '場合', 'よっ', '影響', '部', '降格', '除名', 'くる', '北海道', 'コンサドーレ', '札幌', '野々村', '芳和', '社長', '47', '被る', '損失', '見込ん', '明かし', '乗り切れ', '来年', '以降', '相当', '大変', 'せっかく', '成長', '曲線', '来', 'しょう', '中断', 'ホーム', 'ゲーム', 'ドーム', '平日', '厚別', '振り替え', 'られる', '会場', '変更', '伴う', '観客', '数', '減少', '収入', '減', 'もちろん', 'アルコール', '費', '高齢', '多く', 'ボランティア', '運営', 'スタッフ', '採用', 'せ', 'アルバイト', '時', '人件', '支出', '想定', '鹿島', '別', 'メニュー', 'のぞく', '帯同', '感覚', '失わ', '道外', '遠征', '伴い', '発生', 'なし', '決まっ', '戦い', '方', '変わり', '現場', '目標', 'ACL', '向かっ', 'いけ', 'ば', '同社', '長', '日程', '過密', '連戦', '増えれ', '増える', 'いろんな', 'チャンス', 'わけ', '考え', '欲しい', '一皮', 'むけ', '得', 'うれしい', '湘南', 'ベルマーレ', '特例', '刺激', '臨時', '決定', '一夜', '明け', '平塚', '市', '非', '公開', '消化', 'プレー', 'オフ', '残留', '経緯', 'MF', '鈴木', '冬', '一', '緊張', '感', '経験', 'なく', '逆', '優勝', '争い', '味わい', '上位', '意識', '高め', '斉', '籐', '未', '毎年', '下位', 'しまっ', 'けれど', 'いつ', '面白く', 'サポーター', '思う', 'そう', '増やし', '気', '引き締め', '退任', 'そして', '台風', '冠水', '苦境', '乗り越え', '勝ち取っ', '浮', '嶋', '敏', '52', 'ボク', '引き継い', '状態', 'たら', '喜ぶ', 'しれ', 'しか', '今', '喜び', 'よう', 'ジョーク', '反応', '再開', '展開', '変化', '指摘', '全体', 'いえ', 'より', 'アグレッシブ', '戦う', '得点', '終盤', '采配', 'かなり', '攻撃', '下旬', 'とも', '自動', '昇格', '来季', '臨み', '22', '戻す', '見込み', '目指し', '安全', '考慮', 'ゴールデン', 'ウイーク', 'ずれこむ', '国際', 'A', 'マッチ', '行う', '変則', 'なれ', '主力', '代表', '招集', '不在', '敵地', '公平', '保て', '場面', '不', '飲み込ん', 'いこう', '目線', '頑張る', '姿', '推奨', '残し', '報い', '大会', '方式', '理由', '説明', '成立', '75', '％', '案', '選択', '肢', '挙がっ', '賞金', '分配', '法', '各', 'カテゴリー', '細か', '事案', '再', '白鵬', '朝', '乃', '山', '大関', 'とり', '壁', '立ちはだかっ', '差し込み', '左', '上手', '差さ', '厳しい', '相撲', '押し出し', 'しかし', '取組', '報道', '呼びかけ', '応じ', '取材', 'エリア', 'なぞ', 'スルー', '再び', 'トップ', '並び', '敗', '碧山', '対戦', '組ま', '勝負', 'どころ', '気合', '無言', '貫く', '気迫', '漂わ', '関脇', '26', '＝', '高砂', '横綱', '負け', '喫し', 'タイ', '鶴竜', '昇進', '目安', '三', '役', '場所', '33', '必要', '数字', '状況', '勝て', '機運', '高まる', '自身', '白星', '望み', 'つなげ', '平幕', '首位', '並ん', '大勢', '待ち受ける', '支度', '部屋', '外', 'ミックス', 'ゾーン', 'すがすがしい', '表情', '浮かべ', '歩み寄っ', '日間', '大事', '自分', '取り', 'きる', 'つもり', '切れ', '弱い', 'かかわら', '率直', '吐き出し', 'もらえ', '立ち合い', '差せ', '四', '許し', '上体', '起こさ', '一気', '土俵', '際', '逆転', '狙っ', '動き', '振り払う', '同時', '割っ', '見え', '左足', 'ワン', 'テンポ', '速く', '落ち', '厳し', '足り', '回っ', 'おけ', '弱点', '試練', '番', '最初', 'つまずい', '戦勝', '除い', '過去', '通常', 'なら', '午前', '審判', '翌日', '編成', '会議', '終了', '引き揚げる', 'まだ', '分から', '一番', 'きっ', '頑張り', '行きつけ', '整骨', '院', '通い', '曲がっ', '背骨', '矯正', 'かい', '力強', '泣い', '笑っ', '残り', '二', 'ビシッ', '連勝', '締めくくり', '夢', 'かなえる', '蜂窩', 'ほうか', '織', '炎', '途中', '休場', '11', '出場', '西', '前頭', '枚', '千代', '丸', '九重', '復帰', '対応', '栃煌山', '引き落とし', '破り', '勝目', '幕内', '大きく', '前進', '待つ', '立ち止まら', '足', '止め', '一時', '40', '熱', '上がっ', '体調', 'もう', '戻っ', '体', '元気', '右足', '足袋', 'ふくらはぎ', 'テーピング', '施し', '意外', '痛み', '調子', '良かっ', '残っ', '36', '分', '下がっ', '十', '両', '陥落', 'リンパ', '腫れ', '痛かっ', '淡々', '一応', '病院', '点滴', '打つ', '足早', '引き揚げ', '好', '－', '開い', '前日', '決める', 'ただ', '選考', '勝ち越し', '負け越し', '絡む', '千秋', '楽', 'ほぼ', '慣例', 'ギリギリ', '単独', '春日野', '敗れ', '差', 'つける', '混戦', '結び', '通り', '対決', '番付', '優先', '貴', '景勝', '不振', '格好', '思わ', 'うち', 'なくなる', '柔軟', 'いえる', '八角', '北勝海', '作る', '少ない', '普通', 'いわゆる', '割', '崩し', '理解', '示し', '史上', 'だけ', '最後', '意図', 'かいま', '見える', '判断', '決まり', '十両', '消え', '東', '幕下', '豊ノ島', '時津風', '登場', '豊', '響', '35', '境川', 'のぞか', '圧', '力負け', 'おっつけ', 'ズルズル', '後退', '真後ろ', 'はたい', '踏み越し', '復活', 'かけ', '顔', 'ちょっと', '気負い', 'すぎ', '高かっ', '振り返り', '素直', '吐露', '引退', '固まっ', '翻意', '裏', '粒', '種', '長女', '希', '歩', '存在', '以下', '無給', '生活', 'けなげ', '分かっ', '私', '貸し', 'あげる', '泣き', '続ける', '訴え', 'さらに', '親', '呼ぶ', '関取', '座', '失っ', '両親', 'させ', 'なえ', '奮い立た', '臨ん', '呼び寄せよう', '見せる', '来場', '持ち越し', '本人', 'こだわっ', 'ほしい', 'いっ', '決断', 'テレビ', 'てる', 'でしょう', 'そんな', '中途', '半端', '話す', '現状', 'なかなか', '持っ', '長く', '現役', 'いける', '苦悩', '胸', '冗談', 'っぽく', '娘', '闘い', '少し', '笑い', 'とりあえず', 'ゆっくり', '進退', 'こみ上げる', '抑える', 'スポーツ', 'イベント', '中止', 'なか', '米', '男子', 'ゴルフ', 'タイガー', 'ウッズ', '米国', 'ツイッター', '更新', 'たくさん', 'コメント', '印象', '付け', 'マスターズ', 'トーナメント', '愛する', 'ベスト', '女子', 'LPGA', 'ロッテ', '権', 'ハワイ', '州', '始まる', 'ツアー', '後半', 'ホンダ', '休止', 'なり', '早く', '～', 'メジャー', 'ANA', 'インスピレーション', 'カリフォルニア', '国内', 'コニカ', 'ミノルタ', '杯', '岡山', '初め', '世界', 'ランキング', 'オリンピック', '圏', '渋', '野', '日向子', '愛', '初戦', '畑', '岡', '奈', '入っ', '離れ', '男女', '共同', '声明', '付', '当面', '凍結', 'ポイント', '対象', '用いる', 'ランク', '基準', '連盟', 'IGF', 'システム', '資格', '狙う', '全て', '変わら', 'アクサ', 'レディース', '29', '宮崎', 'UMKCC', '同', '飾っ', '河本', '結', 'リコー', 'インス', 'タグ', 'ラム', 'ディフェンディング', 'チャンピオン', 'ホステス', 'とても', '楽しみ', 'やむ', '事態', '収束', '願っ', '最高', 'お', 'しっかり', '準備', 'おき', 'つづっ', '参戦', '疾病', '対策', 'センター', '8', '50', '集まる', '見通し', '全米', 'PGA', 'ザ', 'プレーヤーズ', 'ラウンド', 'ギャラリー', '入れ', 'ぶり', '松山', '英樹', 'LEXUS', 'スタート', 'バーディー', 'コース', 'レコード', '63', 'マーク', 'アンダー', '暫定', '発進', '幻', '化し', '時間', '方針', '前週', 'バレロ・テキサス・オープン']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlVuQjKi_1Dg",
        "outputId": "89661d62-29dd-436c-c393-a4dbc614dfc0"
      },
      "source": [
        "\n",
        "# Word2Vec学習用データセットの作成\n",
        "\n",
        "window_size = 5\n",
        "input_data = []\n",
        "output = []\n",
        "\n",
        "for row in vec_data:\n",
        " \n",
        "    # Lは各文の形態素数\n",
        "    L = len(row)\n",
        "    \n",
        "    for i in range(L):\n",
        "        begin = i - window_size\n",
        "        end = i + window_size + 1\n",
        "        input_seq = []\n",
        "    \n",
        "        # 当該単語の周辺の単語の辞書番号を格納(文章がない部分は0パディング)\n",
        "        for j in range(begin, end):\n",
        "            if (j < 0):\n",
        "                input_seq.append(0)\n",
        "            elif(0 <= j < L and j != i):\n",
        "                input_seq.append(row[j])\n",
        "            elif(j >= L):\n",
        "                input_seq.append(0)\n",
        "\n",
        "        output.append(row[i])\n",
        "        input_data.append(input_seq)\n",
        "\n",
        "\n",
        "x_train = np.array(input_data)\n",
        "# outputをone-hot化．クラス数は辞書数にmask用単語の領域を引いた(-1した)ものと同じ\n",
        "y_train = np.array(to_categorical(output, len(dictionary)))\n",
        "\n",
        "print('Shape of x_train:', x_train.shape)\n",
        "print('Shape of y_train:', y_train.shape)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_train: (5820, 10)\n",
            "Shape of y_train: (5820, 1395)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05qyKlt7TUUm",
        "outputId": "a9860613-10bc-4b68-a697-d3763a417aac"
      },
      "source": [
        "print(x_train[0:20])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  0  2  3  4  5  6]\n",
            " [ 0  0  0  0  1  3  4  5  6  7]\n",
            " [ 0  0  0  1  2  4  5  6  7  8]\n",
            " [ 0  0  1  2  3  5  6  7  8  9]\n",
            " [ 0  1  2  3  4  6  7  8  9 10]\n",
            " [ 1  2  3  4  5  7  8  9 10 11]\n",
            " [ 2  3  4  5  6  8  9 10 11 12]\n",
            " [ 3  4  5  6  7  9 10 11 12 13]\n",
            " [ 4  5  6  7  8 10 11 12 13 14]\n",
            " [ 5  6  7  8  9 11 12 13 14 15]\n",
            " [ 6  7  8  9 10 12 13 14 15 16]\n",
            " [ 7  8  9 10 11 13 14 15 16 17]\n",
            " [ 8  9 10 11 12 14 15 16 17 18]\n",
            " [ 9 10 11 12 13 15 16 17 18 19]\n",
            " [10 11 12 13 14 16 17 18 19 13]\n",
            " [11 12 13 14 15 17 18 19 13 20]\n",
            " [12 13 14 15 16 18 19 13 20 21]\n",
            " [13 14 15 16 17 19 13 20 21  7]\n",
            " [14 15 16 17 18 13 20 21  7 22]\n",
            " [15 16 17 18 19 20 21  7 22 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC_LYtX9de_R",
        "outputId": "fb6f24a6-d107-43a7-95d0-f1953d6c8675"
      },
      "source": [
        "# Word2Vecで学習(CBOW)\n",
        "# Embeddingの重みを単語の特徴量として利用する\n",
        "\n",
        "num_word = y_train.shape[1] #辞書の単語数\n",
        "\n",
        "# 形態素の平均ベクトルから中央の形態素を予測\n",
        "model = Sequential([\n",
        "    layers.Embedding(num_word, dim_embedding, input_length=window_size*2),\n",
        "    layers.Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim_embedding,)),\n",
        "    layers.Dense(num_word, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 16)            22320     \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1395)              23715     \n",
            "=================================================================\n",
            "Total params: 46,035\n",
            "Trainable params: 46,035\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLC5rmKSfA4p",
        "outputId": "f1d4798d-2f62-4536-91db-093287db45c6"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#学習\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=epochs_w2v)\n",
        "\n",
        "w2v = model.get_weights()[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "46/46 [==============================] - 1s 8ms/step - loss: 7.2222 - accuracy: 0.0412\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 7.1586 - accuracy: 0.0371\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 7.0249 - accuracy: 0.0383\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.8104 - accuracy: 0.0380\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.5398 - accuracy: 0.0380\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.2575 - accuracy: 0.0380\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.0174 - accuracy: 0.0380\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.8588 - accuracy: 0.0385\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.7805 - accuracy: 0.0418\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.7481 - accuracy: 0.0469\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.7324 - accuracy: 0.0543\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.7214 - accuracy: 0.0617\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 5.7110 - accuracy: 0.0715\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.7011 - accuracy: 0.0646\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.6916 - accuracy: 0.0632\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.6820 - accuracy: 0.0629\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.6723 - accuracy: 0.0629\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.6631 - accuracy: 0.0629\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.6544 - accuracy: 0.0629\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.6452 - accuracy: 0.0632\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.6370 - accuracy: 0.0629\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.6289 - accuracy: 0.0629\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.6209 - accuracy: 0.0629\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.6134 - accuracy: 0.0631\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.6060 - accuracy: 0.0631\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5986 - accuracy: 0.0632\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5913 - accuracy: 0.0631\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5843 - accuracy: 0.0637\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.5774 - accuracy: 0.0634\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5708 - accuracy: 0.0637\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5638 - accuracy: 0.0644\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5571 - accuracy: 0.0639\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5504 - accuracy: 0.0646\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5438 - accuracy: 0.0646\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.5369 - accuracy: 0.0648\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.5305 - accuracy: 0.0651\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5234 - accuracy: 0.0656\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5163 - accuracy: 0.0660\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5093 - accuracy: 0.0662\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.5024 - accuracy: 0.0660\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.4949 - accuracy: 0.0680\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.4873 - accuracy: 0.0679\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.4797 - accuracy: 0.0672\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.4717 - accuracy: 0.0684\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.4636 - accuracy: 0.0684\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.4549 - accuracy: 0.0704\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.4463 - accuracy: 0.0723\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.4373 - accuracy: 0.0742\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.4278 - accuracy: 0.0773\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.4185 - accuracy: 0.0784\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.4083 - accuracy: 0.0808\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.3983 - accuracy: 0.0811\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.3877 - accuracy: 0.0825\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.3767 - accuracy: 0.0823\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.3655 - accuracy: 0.0835\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.3540 - accuracy: 0.0854\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.3418 - accuracy: 0.0883\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.3298 - accuracy: 0.0888\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.3175 - accuracy: 0.0893\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.3048 - accuracy: 0.0918\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.2918 - accuracy: 0.0935\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.2787 - accuracy: 0.0936\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.2653 - accuracy: 0.0943\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.2517 - accuracy: 0.0943\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.2379 - accuracy: 0.0967\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.2241 - accuracy: 0.0979\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.2100 - accuracy: 0.0986\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.1959 - accuracy: 0.1007\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.1815 - accuracy: 0.1017\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.1670 - accuracy: 0.1019\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 5.1523 - accuracy: 0.1021\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.1376 - accuracy: 0.1052\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.1229 - accuracy: 0.1048\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.1081 - accuracy: 0.1076\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.0933 - accuracy: 0.1081\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.0781 - accuracy: 0.1091\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.0629 - accuracy: 0.1096\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.0476 - accuracy: 0.1110\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.0325 - accuracy: 0.1129\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.0170 - accuracy: 0.1124\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.0016 - accuracy: 0.1149\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.9860 - accuracy: 0.1162\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.9704 - accuracy: 0.1174\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.9547 - accuracy: 0.1189\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.9388 - accuracy: 0.1199\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.9231 - accuracy: 0.1206\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.9069 - accuracy: 0.1239\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.8910 - accuracy: 0.1251\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.8748 - accuracy: 0.1256\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.8586 - accuracy: 0.1275\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.8423 - accuracy: 0.1280\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.8259 - accuracy: 0.1290\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.8095 - accuracy: 0.1313\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.7931 - accuracy: 0.1320\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.7764 - accuracy: 0.1326\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.7594 - accuracy: 0.1352\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 4.7427 - accuracy: 0.1363\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.7258 - accuracy: 0.1373\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.7088 - accuracy: 0.1381\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.6920 - accuracy: 0.1397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5ACNm8spSHT",
        "outputId": "4bfac5f2-0afe-497a-fc7e-5b96aa9ffb35"
      },
      "source": [
        "print(w2v.shape)\n",
        "print(dictionary[1394])\n",
        "print(w2v[1394])\n",
        "print(w2v.shape[1])\n",
        "print(len(w2v))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1395, 16)\n",
            "バレロ・テキサス・オープン\n",
            "[-0.25565928 -0.07600416  0.9439205  -0.5157413  -0.21602893 -0.28802186\n",
            " -0.9567261   0.39101714 -1.2062737  -0.40406984 -0.13029867  0.80743325\n",
            " -0.29056594  0.84673685  0.9850685   0.8826857 ]\n",
            "16\n",
            "1395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNIuICkQQJIJ",
        "outputId": "06e9f52f-7497-443d-f145-127dfd683410"
      },
      "source": [
        "data_cls = data_tmp['スポーツ']\n",
        "print(data_cls)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0      野球\n",
            "1      野球\n",
            "2      野球\n",
            "3      野球\n",
            "4      野球\n",
            "     ... \n",
            "65    ゴルフ\n",
            "66    ゴルフ\n",
            "67    ゴルフ\n",
            "68    ゴルフ\n",
            "69    ゴルフ\n",
            "Name: スポーツ, Length: 70, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJXrGmmHfxEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a3f061-8224-490d-e7ee-2820beaee7f6"
      },
      "source": [
        "# スポーツ名に対するINDEX番号の付与\n",
        "dic_class = []\n",
        "y_train_1 = []\n",
        "\n",
        "for i in range(len(data_cls)):\n",
        "  if not data_cls[i] in dic_class:\n",
        "    dic_class.append(data_cls[i])\n",
        "\n",
        "for i in range(len(dic_class)):\n",
        "  if not dic_class[i] == dic_class[i]:\n",
        "    dic_class[i] = ' '\n",
        "\n",
        "for i in range(len(data_cls)):\n",
        "  for j in range(len(dic_class)):\n",
        "    if data_cls[i] == dic_class[j] or (data_cls[i] != data_cls[i] and dic_class[j] == ' '):\n",
        "      label_num = j\n",
        "  y_train_1.append(label_num)\n",
        "\n",
        "print('Correct class vector for class 2', y_train_1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct class vector for class 2 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj5RBgXx68zb",
        "outputId": "b0827b5f-cdf7-4a1f-8e68-67a3cb3b74ad"
      },
      "source": [
        "\n",
        "\n",
        "# Paddingで文章長をそろえ、データの形を確認する\n",
        "input_w2v = pad_sequences(vec_data, dtype='int32')\n",
        "# input_copy_w2v = pad_sequences(vec_data, dtype='float32')   # input_w2vの値渡しでのコピー方法が分からなかったので\n",
        "input_copy_w2v = input_w2v.copy()\n",
        "\n",
        "y_train_1 = np.array(to_categorical(y_train_1))\n",
        "\n",
        "timesteps = input_w2v.shape[1]\n",
        "\n",
        "#距離の教師信号(0の配列)\n",
        "# y_distance = np.array([[0.]]*y_train_1.shape[0])\n",
        "\n",
        "print(input_w2v[7])\n",
        "print(y_train_1.shape)\n",
        "print(timesteps)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   1 105 286   7 287  26  58  20 256\n",
            "  66 210 288 289  58 290  29 291  47 292 155  17 293 281 150  58 294  29\n",
            " 295  39 296  20   4 297  20 298 299 197  47 300 125 143  13 301  84   6\n",
            "  87 302  29 303  39  17 232  20 210 304 305 306  29 307  47 308  20 153\n",
            "  91 309 210 310   7 311  29 181 207  17 103 111 312 201  39  75  19  13\n",
            " 313  15 234  17   1]\n",
            "(70, 4)\n",
            "239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxr0koxhiuPP",
        "outputId": "629cb7f2-7304-4005-b66e-4d89dfe6b08e"
      },
      "source": [
        "\n",
        "# import random\n",
        "\n",
        "# teach_seq = []\n",
        "\n",
        "# for i in range(len(data_cls)):\n",
        "#   # print(i)\n",
        "#   while True:\n",
        "#     swap_index = random.randrange(0, timesteps-1, 1) \n",
        "#     # print(swap_index)\n",
        "#     if (input_copy_w2v[i][swap_index] != 0.0):\n",
        "#       # print(input_copy_w2v[i][swap_index])\n",
        "#       break\n",
        "#   # print(input_copy_w2v[i][swap_index])\n",
        "#   teach_seq.append(input_copy_w2v[i][swap_index])\n",
        "#   input_copy_w2v[i][swap_index] = 0.0\n",
        "\n",
        "# teach_seq = np.array(teach_seq)\n",
        "# print(teach_seq.shape)\n",
        "# print(teach_seq[0])\n",
        "# print(w2v[int(teach_seq[0])]) # 指定されたseq番号の単語特徴量\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70,)\n",
            "45.0\n",
            "[ 2.755957   -2.7813385  -1.6191074   0.41452047 -1.570642    2.2770116\n",
            "  0.5740424   1.4648542  -1.5034803   0.9143562  -0.01284696 -0.8284912\n",
            " -0.01225564 -2.14295    -2.086923   -0.33675575]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-urwMyI3JV5t",
        "outputId": "9b60aa02-1c0a-431a-de6a-647105a6fa58"
      },
      "source": [
        "print(pos_data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['空白', 'BOS/EOS', '名詞', '名詞', '補助記号', '名詞', '補助記号', '助詞', '名詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '動詞', '補助記号', '名詞', '接尾辞', '助詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '助詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '補助記号', '接頭辞', '名詞', '名詞', '形状詞', '助動詞', '名詞', '名詞', '名詞', '補助記号', '名詞', '助詞', '補助記号', '副詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '名詞', '動詞', '代名詞', '動詞', '助詞', '名詞', '助詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '代名詞', '動詞', '名詞', '名詞', '名詞', '助動詞', '名詞', '名詞', '助詞', '名詞', '名詞', '補助記号', '動詞', '助詞', '連体詞', '名詞', '動詞', '名詞', '副詞', '動詞', '動詞', '動詞', '助動詞', '助詞', '動詞', '名詞', '助詞', '動詞', '名詞', '動詞', '動詞', '動詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '連体詞', '名詞', '動詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '接頭辞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '助動詞', '形状詞', '助詞', '代名詞', '動詞', '名詞', '形状詞', '助詞', '助動詞', '形容詞', '名詞', '名詞', '副詞', '形状詞', '動詞', '形容詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '副詞', '名詞', '名詞', '名詞', '副詞', '動詞', '名詞', '助詞', '名詞', '接尾辞', '接頭辞', '名詞', '名詞', '名詞', '助詞', '動詞', '名詞', '動詞', '助動詞', '代名詞', '代名詞', '名詞', '助詞', '名詞', '動詞', '動詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '形容詞', '名詞', '副詞', '動詞', '副詞', '名詞', '動詞', '動詞', '名詞', '名詞', '形容詞', '動詞', '名詞', '動詞', '動詞', '動詞', '助動詞', '名詞', '代名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '形状詞', '接尾辞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '接頭辞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '動詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '接尾辞', '副詞', '動詞', '動詞', '副詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '接頭辞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '動詞', '名詞', '接頭辞', '名詞', '名詞', '動詞', '動詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '動詞', '名詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '助動詞', '接尾辞', '動詞', '動詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '接頭辞', '名詞', '名詞', '接尾辞', '名詞', '動詞', '接尾辞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '接尾辞', '接尾辞', '動詞', '助詞', '代名詞', '連体詞', '名詞', '動詞', '助詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '接尾辞', '動詞', '名詞', '名詞', '名詞', '接頭辞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '形容詞', '名詞', '動詞', '名詞', '接頭辞', '名詞', '名詞', '接尾辞', '名詞', '形容詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '接頭辞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '動詞', '名詞', '名詞', '形容詞', '形容詞', '形状詞', '名詞', '形容詞', '助動詞', '形容詞', '動詞', '名詞', '動詞', '助動詞', '補助記号', '空白', '動詞', '形容詞', '動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '代名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '形状詞', '名詞', '名詞', '名詞', '名詞', '動詞', '動詞', '名詞', '動詞', '助詞', '副詞', '形状詞', '名詞', '動詞', '名詞', '接尾辞', '名詞', '形状詞', '連体詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '補助記号', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接続詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '動詞', '連体詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '接尾辞', '名詞', '接尾辞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '形容詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '動詞', '動詞', '名詞', '名詞', '副詞', '形状詞', '副詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '助動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '副詞', '名詞', '接尾辞', '名詞', '形容詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '動詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '動詞', '動詞', '助詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '動詞', '動詞', '連体詞', '名詞', '名詞', '動詞', '形容詞', '名詞', '動詞', '動詞', '形容詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '接頭辞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '形容詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '接頭辞', '名詞', '名詞', '動詞', '助詞', '代名詞', '形容詞', '名詞', '動詞', '副詞', '動詞', '名詞', '動詞', '名詞', '接続詞', '名詞', '名詞', '名詞', '動詞', '動詞', '名詞', '名詞', '名詞', '名詞', '代名詞', '動詞', '名詞', '助動詞', '動詞', '動詞', '助詞', '名詞', '動詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '副詞', '形状詞', '動詞', '名詞', '名詞', '名詞', '副詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '接頭辞', '動詞', '動詞', '名詞', '動詞', '名詞', '名詞', '動詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '動詞', '名詞', '名詞', '名詞', '接頭辞', '名詞', '形状詞', '名詞', '接頭辞', '名詞', '名詞', '助詞', '名詞', '名詞', '名詞', '名詞', '動詞', '動詞', '名詞', '形状詞', '動詞', '形容詞', '名詞', '動詞', '接続詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '副詞', '名詞', '動詞', '接尾辞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '補助記号', '名詞', '名詞', '動詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '形容詞', '名詞', '動詞', '動詞', '名詞', '形状詞', '名詞', '動詞', '動詞', '名詞', '動詞', '形容詞', '動詞', '形状詞', '動詞', '動詞', '名詞', '動詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '動詞', '動詞', '名詞', '動詞', '動詞', '名詞', '名詞', '名詞', '形容詞', '動詞', '形容詞', '動詞', '動詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '助動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '副詞', '動詞', '副詞', '動詞', '動詞', '名詞', '名詞', '接尾辞', '動詞', '動詞', '名詞', '名詞', '名詞', '形容詞', '動詞', '動詞', '名詞', '名詞', '副詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '接尾辞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '形容詞', '名詞', '動詞', '動詞', '名詞', '動詞', '名詞', '名詞', '名詞', '動詞', '名詞', '副詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '形状詞', '名詞', '名詞', '形容詞', '動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '形容詞', '形状詞', '名詞', '名詞', '名詞', '動詞', '形状詞', '動詞', '接頭辞', '補助記号', '動詞', '名詞', '動詞', '接続詞', '名詞', '名詞', '名詞', '動詞', '名詞', '接尾辞', '副詞', '名詞', '副詞', '形状詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接頭辞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '形状詞', '動詞', '名詞', '名詞', '動詞', '形容詞', '名詞', '連体詞', '名詞', '動詞', '名詞', '動詞', '名詞', '助詞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '副詞', '名詞', '名詞', '動詞', '動詞', '名詞', '動詞', '名詞', '副詞', '動詞', '動詞', '形容詞', '動詞', '形状詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接頭辞', '名詞', '名詞', '名詞', '名詞', '名詞', '形状詞', '動詞', '代名詞', '動詞', '動詞', '動詞', '動詞', '動詞', '接続詞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '助動詞', '動詞', '動詞', '動詞', '動詞', '動詞', '名詞', '名詞', '名詞', '動詞', '形容詞', '動詞', '名詞', '名詞', '助動詞', '助動詞', '連体詞', '名詞', '名詞', '動詞', '名詞', '副詞', '動詞', '形容詞', '名詞', '動詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '副詞', '名詞', '副詞', '副詞', '名詞', '動詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '副詞', '名詞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '動詞', '形容詞', '補助記号', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '接尾辞', '名詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '動詞', '名詞', '名詞', '名詞', '接尾辞', '副詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '副詞', '名詞', '動詞', '名詞', '名詞', '動詞', '名詞', '接頭辞', '副詞', '名詞', '動詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '接尾辞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '名詞', '動詞', '名詞', '名詞', '名詞', '名詞']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m7fzL5ZVL4n",
        "outputId": "e469b5b6-3dd3-457b-a57d-3bab5df9a5e4"
      },
      "source": [
        "pos_class = np.unique(pos_data)\n",
        "print(pos_class)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['BOS/EOS' '代名詞' '副詞' '助動詞' '助詞' '動詞' '名詞' '形容詞' '形状詞' '接尾辞' '接続詞' '接頭辞'\n",
            " '空白' '補助記号' '連体詞']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkO8sTCKJ3-y",
        "outputId": "93305c68-602d-4b00-88de-da92a94ccc90"
      },
      "source": [
        "# mask単語を決定する際にmaskしない品詞の設定\n",
        "\n",
        "pos_flag = []\n",
        "ban_pos = ['助詞', '助動詞', '補助記号', '空白', 'BOS/EOS']\n",
        "\n",
        "for i in range(len(pos_data)):\n",
        "  # マスク禁止品詞の場合は0を、そうでない場合は1を挿入\n",
        "  if(pos_data[i] in ban_pos):\n",
        "    pos_flag.append(0)\n",
        "  else:\n",
        "    pos_flag.append(1)\n",
        "\n",
        "pos_flag = np.array(pos_flag)\n",
        "print(pos_flag)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuNOBVagzS7N",
        "outputId": "219653b5-5da8-4d3a-b7aa-70b81cf5b6c9"
      },
      "source": [
        "print(input_copy_w2v.shape)\n",
        "print(input_copy_w2v[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70, 239)\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10\n",
            " 11 12 13 14 15 16 17 18 19 13 20 21  7 22 11 23 24 13 25 26 27 13 28 29\n",
            " 30 31 29 32 17 33 19 13 20 34 35 29 36 13 37 38 39 40 41 42  7 43 44 45\n",
            " 46 11 47 48 49 50  7 51 52 53 54 13 55 38 15 16 56 57 20  8  7  9 10 58\n",
            " 59 17 25 27  7 28 31 20 47 60 61 19 58 62 63 64 65 66 29 67 39 17  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEVU6ZeKzwTv",
        "outputId": "aed69e27-fdb9-4566-d10f-24a689906cfa"
      },
      "source": [
        "# print(pos_flag[int(input_copy_w2v[0][235])])\n",
        "print(pos_flag.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1395,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqratHqS0GmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a7cf6e-d7a5-421d-b652-65acf1fb3bef"
      },
      "source": [
        "# マスク単語変更アルゴリズムの計算量を減らすための前処理\n",
        "\n",
        "available_word_count = []  # 各文章でマスクできる条件を満たす単語の総数を保管\n",
        "available_word_index = []  # 各文章で何番目の添え字の単語が使用可能かを保管(単語番号ではないことに注意)\n",
        "\n",
        "for i in range(input_copy_w2v.shape[0]):\n",
        "  each_available_word_index = []\n",
        "  for j in range(input_copy_w2v.shape[1]):\n",
        "    if pos_flag[int(input_copy_w2v[i][j])] == 1:  # マスク可能な単語ならば\n",
        "      each_available_word_index.append(j)  # その文章で何番目の単語なのかを保存\n",
        "  available_word_index.append(each_available_word_index)\n",
        "  available_word_count.append(len(each_available_word_index))\n",
        "\n",
        "available_word_count = np.array(available_word_count)  # マスク可能単語のリストをnumpy配列化\n",
        "available_word_index = pad_sequences(available_word_index, dtype='int32', padding='post')  # マスク可能単語の添え字保管リストを(末尾)パディングして長さをそろえる\n",
        "available_word_index = np.array(available_word_index)\n",
        "\n",
        "print(data_cls.size)\n",
        "line_inf = np.arange(data_cls.size)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIrAbQH3QFiP",
        "outputId": "1fedbe23-a092-4151-ecf7-7c3b42a66a4e"
      },
      "source": [
        "print(available_word_index[0])\n",
        "print(available_word_index.shape[1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[135 136 138 141 142 143 145 147 149 151 152 155 157 159 160 162 163 164\n",
            " 166 168 169 171 173 174 177 178 180 182 183 185 186 187 189 190 191 192\n",
            " 195 196 197 199 201 202 204 205 207 209 211 213 214 218 219 221 222 225\n",
            " 226 227 229 230 231 232 233 235   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_yLEa88TH6Y"
      },
      "source": [
        "# mask_index=random_matrix % available_word_count\n",
        "# print(ww)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZxgwYG94-Sv",
        "outputId": "66f676f2-d3d7-4113-e0c1-0ed2bd9c7454"
      },
      "source": [
        "print(available_word_count)\n",
        "print(len(available_word_index))\n",
        "print(available_word_index[0])\n",
        "print(available_word_index[0][0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 62  49  47  26  55  57  56  49  26   8  35  16  14  29  18  13  11  20\n",
            "  45  53  35  68  28  74  18  33  47  47  43  89  55  17  60  35  77  34\n",
            "  36  90  43  20  33  69  54  52  70  35  70  42  27  26  61 111  56  84\n",
            "  37  82 102  33  21  45  41  47  40  32  33  52  45  28  43  45]\n",
            "70\n",
            "[135 136 138 141 142 143 145 147 149 151 152 155 157 159 160 162 163 164\n",
            " 166 168 169 171 173 174 177 178 180 182 183 185 186 187 189 190 191 192\n",
            " 195 196 197 199 201 202 204 205 207 209 211 213 214 218 219 221 222 225\n",
            " 226 227 229 230 231 232 233 235   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0]\n",
            "135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G58a9vzZ5uky"
      },
      "source": [
        "def make_input_copy_data(input_copy_w2v, num_sentence, mask_candidate, num_dic, line_inf):\n",
        "\n",
        "  random_matrix = np.random.randint(mask_candidate, mask_candidate*3, num_sentence)\n",
        "  mask_index=random_matrix % available_word_count\n",
        "  # print(mask_index)\n",
        "  # mask_word_feature = w2v[input_copy_w2v[line_inf, available_word_index[line_inf, mask_index]]]  # maskする単語の特徴量(16次元)を保持\n",
        "  # teach_onehot = np.array(to_categorical(input_copy_w2v[line_inf, available_word_index[line_inf, mask_index]], num_dic))\n",
        "  teach_onehot = np.array(to_categorical(input_copy_w2v[line_inf, available_word_index[line_inf, mask_index]], num_dic))  # 教師データの作成(入れ替える前の単語をonehotベクトルで表す)\n",
        "  input_copy_w2v[line_inf, available_word_index[line_inf, mask_index]] = 0\n",
        "\n",
        "  return input_copy_w2v, teach_onehot"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nan1AO47PF0S",
        "outputId": "99a17574-aa45-4315-fdb5-63c3ecba09dc"
      },
      "source": [
        "import random, time\n",
        "\n",
        "t1 = time.time()\n",
        "input_copy_w2v, teach_onehot = make_input_copy_data(input_copy_w2v, len(data_cls), available_word_index.shape[0], len(dictionary), line_inf)\n",
        "t2 = time.time()\n",
        "run_time = t2-t1\n",
        "print('経過時間', run_time)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "経過時間 0.011852741241455078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzRuqB85wVZH",
        "outputId": "200dc753-dbe2-42d0-acb8-ba8d4390da41"
      },
      "source": [
        "print(input_copy_w2v[0])\n",
        "print(input_w2v[0])\n",
        "print(teach_onehot[0][16])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10\n",
            " 11 12 13 14 15 16 17 18 19 13 20 21  7 22 11 23 24 13 25 26 27 13 28 29\n",
            " 30 31 29 32 17 33 19 13 20 34 35 29 36 13 37 38 39 40 41 42  7 43 44 45\n",
            " 46 11 47 48 49 50  7 51 52 53 54 13 55 38 15 16 56 57 20  8  7  9 10 58\n",
            " 59 17 25 27  7 28 31 20 47 60 61 19 58 62 63 64  0 66 29 67 39 17  1]\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10\n",
            " 11 12 13 14 15 16 17 18 19 13 20 21  7 22 11 23 24 13 25 26 27 13 28 29\n",
            " 30 31 29 32 17 33 19 13 20 34 35 29 36 13 37 38 39 40 41 42  7 43 44 45\n",
            " 46 11 47 48 49 50  7 51 52 53 54 13 55 38 15 16 56 57 20  8  7  9 10 58\n",
            " 59 17 25 27  7 28 31 20 47 60 61 19 58 62 63 64 65 66 29 67 39 17  1]\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaqH2COrPlL1",
        "outputId": "6328eeeb-a0d1-4490-a394-0e54d5f045ad"
      },
      "source": [
        "# run_time2 = 1833.300451\n",
        "# print('経過時間:', int(run_time2%60), '分', int(run_time2), '秒')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "経過時間: 33 分 1833 秒\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibk88iEEHFNU",
        "outputId": "59909d47-dff2-4889-942d-7415a5f06e05"
      },
      "source": [
        "print(teach_onehot[0][34])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wndWWzG9J9_",
        "outputId": "6bb2218f-1cec-4c50-9440-366cf1dc9c55"
      },
      "source": [
        "print(input_copy_w2v[0])\n",
        "print(input_w2v[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10\n",
            " 11 12 13 14 15 16 17 18 19 13 20 21  7 22 11 23 24 13 25 26 27 13 28 29\n",
            " 30 31 29  0 17 33 19 13 20 34 35 29 36 13 37 38 39 40 41 42  7 43 44 45\n",
            " 46 11 47 48 49 50  7 51 52 53 54 13 55 38 15 16 56 57 20  8  7  9 10 58\n",
            " 59 17 25 27  7 28 31 20 47 60 61 19 58 62 63 64 65 66 29 67 39 17  1]\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10\n",
            " 11 12 13 14 15 16 17 18 19 13 20 21  7 22 11 23 24 13 25 26 27 13 28 29\n",
            " 30 31 29 32 17 33 19 13 20 34 35 29 36 13 37 38 39 40 41 42  7 43 44 45\n",
            " 46 11 47 48 49 50  7 51 52 53 54 13 55 38 15 16 56 57 20  8  7  9 10 58\n",
            " 59 17 25 27  7 28 31 20 47 60 61 19 58 62 63 64 65 66 29 67 39 17  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMPPKtBtJIgI",
        "outputId": "e9259715-c2ea-460b-cc17-7c35febd5b38"
      },
      "source": [
        "print(w2v[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.9453039e-03 -1.1314447e+00  1.1540352e+00 -1.0684234e+00\n",
            " -1.0498395e+00  4.3507019e-01 -1.1887598e+00  1.1632371e+00\n",
            "  2.1059680e+00 -1.1592200e+00 -4.8026356e-01  5.6137449e-01\n",
            " -1.1221852e+00  1.1547741e+00 -1.7144275e+00  1.0402098e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU19XBsj-HLP",
        "outputId": "c5d9958c-9dac-4021-cf17-fda4655aed5f"
      },
      "source": [
        "\n",
        "cls_num = len(dic_class)\n",
        "\n",
        "input_dim_w2v = len(w2v)\n",
        "output_dim_w2v = w2v.shape[1]\n",
        "\n",
        "inputs = Input(shape=(timesteps, ))\n",
        "\n",
        "# inputsをw2vでベクトル化\n",
        "\n",
        "embed = Embedding(input_dim_w2v, \n",
        "                  output_dim_w2v, \n",
        "                  input_length=timesteps, \n",
        "                  weights=[w2v], \n",
        "                  mask_zero=True, \n",
        "                  trainable=False, \n",
        "                  name='layer_0')(inputs)\n",
        "\n",
        "# カスタムレイヤー\n",
        "# embed_dim = LambdaExpandDims(timesteps = timesteps, dim_embedding = dim_embedding)(embed)\n",
        "embed_dim = Lambda(lambda x: K.expand_dims(x, 1), output_shape=(1, timesteps, dim_embedding, ))(embed)\n",
        "\n",
        "'''------------記事を基にしたスポーツ名予測---------------------------------'''\n",
        "c0x = GlobalAveragePooling2D(name='layer_2x')(embed_dim)\n",
        "c1x = Dense(dim_embedding//2, activation=\"relu\", name='layer_3x')(c0x)\n",
        "c2x = Dense(dim_embedding, activation=\"sigmoid\", name='layer_4x')(c1x)\n",
        "c2x = Reshape((1, dim_embedding), input_shape=(dim_embedding,))(c2x)\n",
        "\n",
        "cx = Multiply(name='layer_5x')([embed, c2x])\n",
        "\n",
        "sx = Conv2D(1, 1, activation='sigmoid', name='layer_6x')(embed_dim)\n",
        "sx = Multiply(name='layer_7x')([embed, sx])\n",
        "\n",
        "sx = Lambda(lambda x: tf.squeeze(x, 1), name='layer_8x')(sx)\n",
        "# sx = my_func.Lambda.Squeeze()(sx)\n",
        "\n",
        "attx = layers.add([cx, sx])\n",
        "\n",
        "encodedx = Bidirectional(\n",
        "    LSTM(dim_z, \n",
        "         batch_input_shape=(None, timesteps, dim_embedding), \n",
        "         activation=\"tanh\", \n",
        "         recurrent_activation=\"sigmoid\", \n",
        "         return_sequences=False\n",
        "         ),\n",
        "    name='bidirectionalx')(attx)\n",
        "\n",
        "encodedx = Dropout(0.2)(encodedx)\n",
        "\n",
        "out = Dense(units=cls_num, activation='softmax', name='out')(encodedx)\n",
        "\n",
        "out_2 = Dense(units=num_word, activation='softmax', name='out_2')(encodedx)  # 16次元の出力で、活性化関数はsoftmax\n",
        "\n",
        "LSTM_AE = Model(inputs, [out, out_2], name='LSTM_AE')\n",
        "\n",
        "LSTM_AE.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999), \n",
        "                loss={ 'out':'categorical_crossentropy', \n",
        "                       'out_2':'categorical_crossentropy'}, \n",
        "                metrics=['acc'])\n",
        "\n",
        "LSTM_AE.summary()\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"LSTM_AE\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 239)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "layer_0 (Embedding)             (None, 239, 16)      22320       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1, 239, 16)   0           layer_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_2x (GlobalAveragePooling2 (None, 16)           0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_3x (Dense)                (None, 8)            136         layer_2x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_4x (Dense)                (None, 16)           144         layer_3x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_6x (Conv2D)               (None, 1, 239, 1)    17          lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 1, 16)        0           layer_4x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_7x (Multiply)             (None, 1, 239, 16)   0           layer_0[0][0]                    \n",
            "                                                                 layer_6x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_5x (Multiply)             (None, 239, 16)      0           layer_0[0][0]                    \n",
            "                                                                 reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_8x (Lambda)               (None, 239, 16)      0           layer_7x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 239, 16)      0           layer_5x[0][0]                   \n",
            "                                                                 layer_8x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectionalx (Bidirectional)  (None, 200)          93600       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 200)          0           bidirectionalx[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "out (Dense)                     (None, 4)            804         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "out_2 (Dense)                   (None, 1395)         280395      dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 397,416\n",
            "Trainable params: 375,096\n",
            "Non-trainable params: 22,320\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoCgAcNvQ8Av",
        "outputId": "d322f702-f78c-41da-ddc5-1ebbdd63e041"
      },
      "source": [
        "# print(teach_seq[0])\n",
        "print(input_copy_w2v[0])\n",
        "\n",
        "\n",
        "print(input_w2v[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10\n",
            " 11 12 13 14 15 16 17 18 19 13 20 21  7 22 11 23 24 13 25 26 27 13 28 29\n",
            " 30 31 29 32 17 33 19 13 20 34 35 29 36 13 37 38 39 40 41 42  7 43 44 45\n",
            " 46 11 47 48 49 50  7 51 52 53 54 13 55 38 15 16 56 57 20  8  7  9 10 58\n",
            " 59 17 25 27  7 28 31 20 47 60 61 19 58 62 63 64  0 66 29 67 39 17  1]\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10\n",
            " 11 12 13 14 15 16 17 18 19 13 20 21  7 22 11 23 24 13 25 26 27 13 28 29\n",
            " 30 31 29 32 17 33 19 13 20 34 35 29 36 13 37 38 39 40 41 42  7 43 44 45\n",
            " 46 11 47 48 49 50  7 51 52 53 54 13 55 38 15 16 56 57 20  8  7  9 10 58\n",
            " 59 17 25 27  7 28 31 20 47 60 61 19 58 62 63 64 65 66 29 67 39 17  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUN0e96_DjE5",
        "outputId": "bb82bcd5-eda7-4513-c6c1-6300d00ed4b5"
      },
      "source": [
        "path_weight = './my_model.h5'\n",
        "\n",
        "best_saver = tf.keras.callbacks.ModelCheckpoint(filepath=path_weight, monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "loss_list = []\n",
        "out_loss_list = []\n",
        "out_2_loss_list = []\n",
        "out_acc_list = []\n",
        "out_2_acc_list = []\n",
        "\n",
        "for i in range(epochs_lstm):\n",
        "  print('epochs:', i+1)\n",
        "  result = LSTM_AE.fit(input_copy_w2v, \n",
        "                      [y_train_1, teach_onehot], \n",
        "                      epochs=1, \n",
        "                      batch_size=batchsize_lstm, \n",
        "                      shuffle=True, \n",
        "                      callbacks=[best_saver]\n",
        "                      )\n",
        "\n",
        "  loss_list.extend(result.history['loss'])\n",
        "  out_loss_list.extend(result.history['out_loss'])\n",
        "  out_2_loss_list.extend(result.history['out_2_loss'])\n",
        "  out_acc_list.extend(result.history['out_acc'])\n",
        "  out_2_acc_list.extend(result.history['out_2_acc'])\n",
        "   \n",
        "  \n",
        "  input_copy_w2v = input_w2v.copy()\n",
        "  # print(input_copy_w2v[24])\n",
        "  input_copy_w2v, teach_onehot = make_input_copy_data(input_copy_w2v, len(data_cls), available_word_index.shape[0], len(dictionary), line_inf)\n",
        "  # print(teach_onehot)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs: 1\n",
            "1/1 [==============================] - 10s 10s/step - loss: 8.6358 - out_loss: 1.4054 - out_2_loss: 7.2303 - out_acc: 0.1429 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from inf to 8.63576, saving model to ./my_model.h5\n",
            "epochs: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 685ms/step - loss: 8.6016 - out_loss: 1.3844 - out_2_loss: 7.2172 - out_acc: 0.2429 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 8.63576 to 8.60160, saving model to ./my_model.h5\n",
            "epochs: 3\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 8.5694 - out_loss: 1.3528 - out_2_loss: 7.2166 - out_acc: 0.3429 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 8.60160 to 8.56942, saving model to ./my_model.h5\n",
            "epochs: 4\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 8.5643 - out_loss: 1.3503 - out_2_loss: 7.2140 - out_acc: 0.3714 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 8.56942 to 8.56433, saving model to ./my_model.h5\n",
            "epochs: 5\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 8.5518 - out_loss: 1.3336 - out_2_loss: 7.2182 - out_acc: 0.3571 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 8.56433 to 8.55177, saving model to ./my_model.h5\n",
            "epochs: 6\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 8.5094 - out_loss: 1.3090 - out_2_loss: 7.2004 - out_acc: 0.4143 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 8.55177 to 8.50937, saving model to ./my_model.h5\n",
            "epochs: 7\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 8.4856 - out_loss: 1.2926 - out_2_loss: 7.1930 - out_acc: 0.4714 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 8.50937 to 8.48558, saving model to ./my_model.h5\n",
            "epochs: 8\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 8.4694 - out_loss: 1.2704 - out_2_loss: 7.1989 - out_acc: 0.4714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 8.48558 to 8.46935, saving model to ./my_model.h5\n",
            "epochs: 9\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 8.4240 - out_loss: 1.2726 - out_2_loss: 7.1514 - out_acc: 0.5286 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 8.46935 to 8.42400, saving model to ./my_model.h5\n",
            "epochs: 10\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 8.4176 - out_loss: 1.2335 - out_2_loss: 7.1841 - out_acc: 0.5286 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 8.42400 to 8.41756, saving model to ./my_model.h5\n",
            "epochs: 11\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 8.3626 - out_loss: 1.2199 - out_2_loss: 7.1427 - out_acc: 0.5143 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 8.41756 to 8.36264, saving model to ./my_model.h5\n",
            "epochs: 12\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 8.3686 - out_loss: 1.2041 - out_2_loss: 7.1645 - out_acc: 0.5429 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 8.36264\n",
            "epochs: 13\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 8.3140 - out_loss: 1.1456 - out_2_loss: 7.1685 - out_acc: 0.6000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 8.36264 to 8.31404, saving model to ./my_model.h5\n",
            "epochs: 14\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 8.3011 - out_loss: 1.1333 - out_2_loss: 7.1678 - out_acc: 0.6714 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 8.31404 to 8.30107, saving model to ./my_model.h5\n",
            "epochs: 15\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 8.1654 - out_loss: 1.0946 - out_2_loss: 7.0707 - out_acc: 0.6286 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 8.30107 to 8.16537, saving model to ./my_model.h5\n",
            "epochs: 16\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 8.1434 - out_loss: 1.0781 - out_2_loss: 7.0653 - out_acc: 0.6143 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 8.16537 to 8.14342, saving model to ./my_model.h5\n",
            "epochs: 17\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 8.0750 - out_loss: 1.0387 - out_2_loss: 7.0364 - out_acc: 0.6000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 8.14342 to 8.07504, saving model to ./my_model.h5\n",
            "epochs: 18\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 7.9556 - out_loss: 1.0092 - out_2_loss: 6.9464 - out_acc: 0.6429 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 8.07504 to 7.95555, saving model to ./my_model.h5\n",
            "epochs: 19\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 7.9686 - out_loss: 0.9730 - out_2_loss: 6.9955 - out_acc: 0.6571 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.95555\n",
            "epochs: 20\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 7.9069 - out_loss: 0.9305 - out_2_loss: 6.9765 - out_acc: 0.6429 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 7.95555 to 7.90695, saving model to ./my_model.h5\n",
            "epochs: 21\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 7.7874 - out_loss: 0.8939 - out_2_loss: 6.8935 - out_acc: 0.6571 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 7.90695 to 7.78740, saving model to ./my_model.h5\n",
            "epochs: 22\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 7.7183 - out_loss: 0.8122 - out_2_loss: 6.9061 - out_acc: 0.7000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 7.78740 to 7.71833, saving model to ./my_model.h5\n",
            "epochs: 23\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 7.7011 - out_loss: 0.7629 - out_2_loss: 6.9382 - out_acc: 0.6857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 7.71833 to 7.70111, saving model to ./my_model.h5\n",
            "epochs: 24\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 7.8026 - out_loss: 0.7900 - out_2_loss: 7.0126 - out_acc: 0.6429 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.70111\n",
            "epochs: 25\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 7.8216 - out_loss: 0.9305 - out_2_loss: 6.8912 - out_acc: 0.6143 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.70111\n",
            "epochs: 26\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 7.8600 - out_loss: 0.9360 - out_2_loss: 6.9240 - out_acc: 0.6571 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.70111\n",
            "epochs: 27\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 7.5773 - out_loss: 0.7306 - out_2_loss: 6.8466 - out_acc: 0.7000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 7.70111 to 7.57726, saving model to ./my_model.h5\n",
            "epochs: 28\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 7.4159 - out_loss: 0.7220 - out_2_loss: 6.6938 - out_acc: 0.7000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss improved from 7.57726 to 7.41586, saving model to ./my_model.h5\n",
            "epochs: 29\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 7.4166 - out_loss: 0.6910 - out_2_loss: 6.7256 - out_acc: 0.7286 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.41586\n",
            "epochs: 30\n",
            "1/1 [==============================] - 1s 642ms/step - loss: 7.4657 - out_loss: 0.6895 - out_2_loss: 6.7762 - out_acc: 0.8000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.41586\n",
            "epochs: 31\n",
            "1/1 [==============================] - 1s 643ms/step - loss: 7.5152 - out_loss: 0.6205 - out_2_loss: 6.8946 - out_acc: 0.7571 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.41586\n",
            "epochs: 32\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 7.4035 - out_loss: 0.5676 - out_2_loss: 6.8359 - out_acc: 0.8143 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 7.41586 to 7.40347, saving model to ./my_model.h5\n",
            "epochs: 33\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 7.4589 - out_loss: 0.5974 - out_2_loss: 6.8614 - out_acc: 0.8000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.40347\n",
            "epochs: 34\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 7.0352 - out_loss: 0.5412 - out_2_loss: 6.4940 - out_acc: 0.8000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 7.40347 to 7.03521, saving model to ./my_model.h5\n",
            "epochs: 35\n",
            "1/1 [==============================] - 1s 646ms/step - loss: 7.6064 - out_loss: 0.5323 - out_2_loss: 7.0740 - out_acc: 0.8571 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.03521\n",
            "epochs: 36\n",
            "1/1 [==============================] - 1s 792ms/step - loss: 7.1495 - out_loss: 0.4956 - out_2_loss: 6.6540 - out_acc: 0.8714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.03521\n",
            "epochs: 37\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 7.4148 - out_loss: 0.4704 - out_2_loss: 6.9444 - out_acc: 0.8286 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.03521\n",
            "epochs: 38\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 7.1860 - out_loss: 0.4476 - out_2_loss: 6.7384 - out_acc: 0.8714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.03521\n",
            "epochs: 39\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 7.1234 - out_loss: 0.4477 - out_2_loss: 6.6757 - out_acc: 0.8571 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.03521\n",
            "epochs: 40\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 7.0518 - out_loss: 0.4142 - out_2_loss: 6.6377 - out_acc: 0.8857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.03521\n",
            "epochs: 41\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 7.2257 - out_loss: 0.3818 - out_2_loss: 6.8439 - out_acc: 0.8857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.03521\n",
            "epochs: 42\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 7.0529 - out_loss: 0.3502 - out_2_loss: 6.7026 - out_acc: 0.9143 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 7.03521\n",
            "epochs: 43\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 6.9080 - out_loss: 0.3622 - out_2_loss: 6.5458 - out_acc: 0.8857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 7.03521 to 6.90800, saving model to ./my_model.h5\n",
            "epochs: 44\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 6.8029 - out_loss: 0.3662 - out_2_loss: 6.4367 - out_acc: 0.8429 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 6.90800 to 6.80292, saving model to ./my_model.h5\n",
            "epochs: 45\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 6.7853 - out_loss: 0.3315 - out_2_loss: 6.4538 - out_acc: 0.8714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 6.80292 to 6.78532, saving model to ./my_model.h5\n",
            "epochs: 46\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 6.7954 - out_loss: 0.2725 - out_2_loss: 6.5229 - out_acc: 0.9286 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.78532\n",
            "epochs: 47\n",
            "1/1 [==============================] - 1s 646ms/step - loss: 7.1609 - out_loss: 0.2591 - out_2_loss: 6.9018 - out_acc: 0.9143 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.78532\n",
            "epochs: 48\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 6.8053 - out_loss: 0.2682 - out_2_loss: 6.5371 - out_acc: 0.9000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.78532\n",
            "epochs: 49\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 7.0398 - out_loss: 0.2475 - out_2_loss: 6.7923 - out_acc: 0.9143 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.78532\n",
            "epochs: 50\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 7.0555 - out_loss: 0.3294 - out_2_loss: 6.7260 - out_acc: 0.9000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.78532\n",
            "epochs: 51\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 6.8983 - out_loss: 0.4387 - out_2_loss: 6.4596 - out_acc: 0.8714 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.78532\n",
            "epochs: 52\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 6.8179 - out_loss: 0.3852 - out_2_loss: 6.4327 - out_acc: 0.8857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.78532\n",
            "epochs: 53\n",
            "1/1 [==============================] - 1s 642ms/step - loss: 6.9040 - out_loss: 0.3839 - out_2_loss: 6.5201 - out_acc: 0.8571 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.78532\n",
            "epochs: 54\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 6.7523 - out_loss: 0.2397 - out_2_loss: 6.5126 - out_acc: 0.9286 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 6.78532 to 6.75226, saving model to ./my_model.h5\n",
            "epochs: 55\n",
            "1/1 [==============================] - 1s 728ms/step - loss: 6.7541 - out_loss: 0.2601 - out_2_loss: 6.4940 - out_acc: 0.9000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.75226\n",
            "epochs: 56\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 6.7340 - out_loss: 0.2465 - out_2_loss: 6.4874 - out_acc: 0.9429 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 6.75226 to 6.73396, saving model to ./my_model.h5\n",
            "epochs: 57\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 6.8061 - out_loss: 0.2792 - out_2_loss: 6.5269 - out_acc: 0.9000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.73396\n",
            "epochs: 58\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 6.6691 - out_loss: 0.2940 - out_2_loss: 6.3750 - out_acc: 0.9143 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 6.73396 to 6.66908, saving model to ./my_model.h5\n",
            "epochs: 59\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 6.6440 - out_loss: 0.2301 - out_2_loss: 6.4139 - out_acc: 0.9143 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 6.66908 to 6.64403, saving model to ./my_model.h5\n",
            "epochs: 60\n",
            "1/1 [==============================] - 1s 712ms/step - loss: 6.7809 - out_loss: 0.2750 - out_2_loss: 6.5059 - out_acc: 0.9000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.64403\n",
            "epochs: 61\n",
            "1/1 [==============================] - 1s 649ms/step - loss: 6.7826 - out_loss: 0.2380 - out_2_loss: 6.5446 - out_acc: 0.9286 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.64403\n",
            "epochs: 62\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 6.8887 - out_loss: 0.2525 - out_2_loss: 6.6362 - out_acc: 0.9000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.64403\n",
            "epochs: 63\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 6.6969 - out_loss: 0.2403 - out_2_loss: 6.4566 - out_acc: 0.9000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.64403\n",
            "epochs: 64\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 6.6230 - out_loss: 0.2345 - out_2_loss: 6.3885 - out_acc: 0.9143 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 6.64403 to 6.62303, saving model to ./my_model.h5\n",
            "epochs: 65\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 6.2471 - out_loss: 0.2247 - out_2_loss: 6.0224 - out_acc: 0.9143 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss improved from 6.62303 to 6.24713, saving model to ./my_model.h5\n",
            "epochs: 66\n",
            "1/1 [==============================] - 1s 647ms/step - loss: 6.6586 - out_loss: 0.2259 - out_2_loss: 6.4327 - out_acc: 0.9429 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 67\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 6.5897 - out_loss: 0.1702 - out_2_loss: 6.4195 - out_acc: 0.9429 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 68\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 6.3262 - out_loss: 0.1785 - out_2_loss: 6.1478 - out_acc: 0.9571 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 69\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 6.8720 - out_loss: 0.1862 - out_2_loss: 6.6858 - out_acc: 0.9429 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 70\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 6.4427 - out_loss: 0.1627 - out_2_loss: 6.2800 - out_acc: 0.9286 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 71\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 6.4724 - out_loss: 0.1703 - out_2_loss: 6.3022 - out_acc: 0.9143 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 72\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 6.7195 - out_loss: 0.1603 - out_2_loss: 6.5592 - out_acc: 0.9286 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 73\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 6.6763 - out_loss: 0.1403 - out_2_loss: 6.5360 - out_acc: 0.9286 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 74\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 6.2840 - out_loss: 0.1146 - out_2_loss: 6.1694 - out_acc: 0.9714 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 75\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 6.4478 - out_loss: 0.1470 - out_2_loss: 6.3008 - out_acc: 0.9571 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 76\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 6.7067 - out_loss: 0.1398 - out_2_loss: 6.5670 - out_acc: 0.9714 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 77\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 6.3424 - out_loss: 0.1172 - out_2_loss: 6.2252 - out_acc: 0.9714 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.24713\n",
            "epochs: 78\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 6.1897 - out_loss: 0.1351 - out_2_loss: 6.0547 - out_acc: 0.9429 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 6.24713 to 6.18974, saving model to ./my_model.h5\n",
            "epochs: 79\n",
            "1/1 [==============================] - 1s 626ms/step - loss: 6.5703 - out_loss: 0.1203 - out_2_loss: 6.4500 - out_acc: 0.9429 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18974\n",
            "epochs: 80\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 6.5034 - out_loss: 0.0990 - out_2_loss: 6.4044 - out_acc: 0.9714 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18974\n",
            "epochs: 81\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 6.3215 - out_loss: 0.1228 - out_2_loss: 6.1987 - out_acc: 0.9571 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18974\n",
            "epochs: 82\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 6.4655 - out_loss: 0.0821 - out_2_loss: 6.3834 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18974\n",
            "epochs: 83\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 6.5963 - out_loss: 0.1051 - out_2_loss: 6.4912 - out_acc: 0.9857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18974\n",
            "epochs: 84\n",
            "1/1 [==============================] - 1s 714ms/step - loss: 6.1826 - out_loss: 0.0673 - out_2_loss: 6.1153 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 6.18974 to 6.18256, saving model to ./my_model.h5\n",
            "epochs: 85\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 6.5229 - out_loss: 0.1094 - out_2_loss: 6.4135 - out_acc: 0.9429 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 86\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 6.5379 - out_loss: 0.0920 - out_2_loss: 6.4459 - out_acc: 0.9571 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 87\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 6.6283 - out_loss: 0.1493 - out_2_loss: 6.4790 - out_acc: 0.9286 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 88\n",
            "1/1 [==============================] - 1s 729ms/step - loss: 6.5261 - out_loss: 0.1291 - out_2_loss: 6.3970 - out_acc: 0.9429 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 89\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 6.4504 - out_loss: 0.0788 - out_2_loss: 6.3716 - out_acc: 0.9714 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 90\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 6.2048 - out_loss: 0.0784 - out_2_loss: 6.1264 - out_acc: 0.9714 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 91\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 6.4490 - out_loss: 0.2124 - out_2_loss: 6.2366 - out_acc: 0.9286 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 92\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 6.3090 - out_loss: 0.0989 - out_2_loss: 6.2102 - out_acc: 0.9714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 93\n",
            "1/1 [==============================] - 1s 813ms/step - loss: 6.2802 - out_loss: 0.0583 - out_2_loss: 6.2219 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 94\n",
            "1/1 [==============================] - 1s 747ms/step - loss: 6.3339 - out_loss: 0.1297 - out_2_loss: 6.2042 - out_acc: 0.9429 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 95\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 6.2917 - out_loss: 0.1085 - out_2_loss: 6.1832 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 96\n",
            "1/1 [==============================] - 1s 720ms/step - loss: 6.3890 - out_loss: 0.1076 - out_2_loss: 6.2814 - out_acc: 0.9714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 97\n",
            "1/1 [==============================] - 1s 742ms/step - loss: 6.2653 - out_loss: 0.0934 - out_2_loss: 6.1719 - out_acc: 0.9714 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 98\n",
            "1/1 [==============================] - 1s 739ms/step - loss: 6.4203 - out_loss: 0.0737 - out_2_loss: 6.3467 - out_acc: 0.9857 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 99\n",
            "1/1 [==============================] - 1s 718ms/step - loss: 6.3827 - out_loss: 0.0720 - out_2_loss: 6.3107 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 100\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 6.4961 - out_loss: 0.0773 - out_2_loss: 6.4188 - out_acc: 0.9714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 101\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 6.3013 - out_loss: 0.0809 - out_2_loss: 6.2204 - out_acc: 0.9714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.18256\n",
            "epochs: 102\n",
            "1/1 [==============================] - 1s 743ms/step - loss: 6.0761 - out_loss: 0.0638 - out_2_loss: 6.0124 - out_acc: 0.9857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 6.18256 to 6.07613, saving model to ./my_model.h5\n",
            "epochs: 103\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 6.5820 - out_loss: 0.0548 - out_2_loss: 6.5271 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.07613\n",
            "epochs: 104\n",
            "1/1 [==============================] - 1s 714ms/step - loss: 6.1382 - out_loss: 0.0570 - out_2_loss: 6.0812 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.07613\n",
            "epochs: 105\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 6.3086 - out_loss: 0.1130 - out_2_loss: 6.1956 - out_acc: 0.9714 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.07613\n",
            "epochs: 106\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 6.0453 - out_loss: 0.0514 - out_2_loss: 5.9939 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 6.07613 to 6.04530, saving model to ./my_model.h5\n",
            "epochs: 107\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 6.2705 - out_loss: 0.0512 - out_2_loss: 6.2193 - out_acc: 0.9857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.04530\n",
            "epochs: 108\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 6.2286 - out_loss: 0.0619 - out_2_loss: 6.1667 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.04530\n",
            "epochs: 109\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 6.3303 - out_loss: 0.0542 - out_2_loss: 6.2761 - out_acc: 0.9714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.04530\n",
            "epochs: 110\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 6.1187 - out_loss: 0.0704 - out_2_loss: 6.0483 - out_acc: 0.9857 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.04530\n",
            "epochs: 111\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 6.4131 - out_loss: 0.0447 - out_2_loss: 6.3685 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.04530\n",
            "epochs: 112\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 6.2411 - out_loss: 0.0691 - out_2_loss: 6.1720 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.04530\n",
            "epochs: 113\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 6.1652 - out_loss: 0.0636 - out_2_loss: 6.1016 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.04530\n",
            "epochs: 114\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 5.9427 - out_loss: 0.0590 - out_2_loss: 5.8837 - out_acc: 0.9857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 6.04530 to 5.94269, saving model to ./my_model.h5\n",
            "epochs: 115\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 6.0014 - out_loss: 0.0519 - out_2_loss: 5.9495 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.94269\n",
            "epochs: 116\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 5.8921 - out_loss: 0.0377 - out_2_loss: 5.8544 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 5.94269 to 5.89214, saving model to ./my_model.h5\n",
            "epochs: 117\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 6.2459 - out_loss: 0.2074 - out_2_loss: 6.0385 - out_acc: 0.9286 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.89214\n",
            "epochs: 118\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 6.2258 - out_loss: 0.0687 - out_2_loss: 6.1572 - out_acc: 0.9714 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.89214\n",
            "epochs: 119\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 5.8787 - out_loss: 0.0521 - out_2_loss: 5.8266 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 5.89214 to 5.87872, saving model to ./my_model.h5\n",
            "epochs: 120\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 6.0457 - out_loss: 0.0826 - out_2_loss: 5.9631 - out_acc: 0.9714 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.87872\n",
            "epochs: 121\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 6.0635 - out_loss: 0.0493 - out_2_loss: 6.0142 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.87872\n",
            "epochs: 122\n",
            "1/1 [==============================] - 1s 727ms/step - loss: 6.3497 - out_loss: 0.0397 - out_2_loss: 6.3100 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.87872\n",
            "epochs: 123\n",
            "1/1 [==============================] - 1s 743ms/step - loss: 6.2847 - out_loss: 0.0348 - out_2_loss: 6.2499 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.87872\n",
            "epochs: 124\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 6.0978 - out_loss: 0.0533 - out_2_loss: 6.0444 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.87872\n",
            "epochs: 125\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 6.4239 - out_loss: 0.0461 - out_2_loss: 6.3778 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.87872\n",
            "epochs: 126\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 6.4369 - out_loss: 0.0333 - out_2_loss: 6.4035 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.87872\n",
            "epochs: 127\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 6.0770 - out_loss: 0.0323 - out_2_loss: 6.0447 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.87872\n",
            "epochs: 128\n",
            "1/1 [==============================] - 1s 642ms/step - loss: 5.8586 - out_loss: 0.0495 - out_2_loss: 5.8091 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 5.87872 to 5.85857, saving model to ./my_model.h5\n",
            "epochs: 129\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 6.0513 - out_loss: 0.0535 - out_2_loss: 5.9977 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 130\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 6.2036 - out_loss: 0.0987 - out_2_loss: 6.1049 - out_acc: 0.9571 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 131\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 6.0617 - out_loss: 0.0364 - out_2_loss: 6.0253 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 132\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 6.2317 - out_loss: 0.0240 - out_2_loss: 6.2076 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 133\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 6.1945 - out_loss: 0.1288 - out_2_loss: 6.0657 - out_acc: 0.9571 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 134\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 6.1956 - out_loss: 0.0285 - out_2_loss: 6.1672 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 135\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 6.1832 - out_loss: 0.1276 - out_2_loss: 6.0556 - out_acc: 0.9429 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 136\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 6.3460 - out_loss: 0.3289 - out_2_loss: 6.0170 - out_acc: 0.8714 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 137\n",
            "1/1 [==============================] - 1s 728ms/step - loss: 6.3278 - out_loss: 0.1610 - out_2_loss: 6.1668 - out_acc: 0.9286 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 138\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 6.1376 - out_loss: 0.1208 - out_2_loss: 6.0168 - out_acc: 0.9429 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 139\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 6.1256 - out_loss: 0.0858 - out_2_loss: 6.0398 - out_acc: 0.9571 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 140\n",
            "1/1 [==============================] - 1s 646ms/step - loss: 6.0688 - out_loss: 0.0863 - out_2_loss: 5.9825 - out_acc: 0.9714 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 141\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 6.1018 - out_loss: 0.0595 - out_2_loss: 6.0423 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 142\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 6.0104 - out_loss: 0.0633 - out_2_loss: 5.9471 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 143\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 6.0320 - out_loss: 0.0669 - out_2_loss: 5.9651 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 144\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 5.9131 - out_loss: 0.0664 - out_2_loss: 5.8467 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 145\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 6.1430 - out_loss: 0.0538 - out_2_loss: 6.0892 - out_acc: 0.9857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 146\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 5.8960 - out_loss: 0.0564 - out_2_loss: 5.8396 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 147\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 6.0604 - out_loss: 0.0644 - out_2_loss: 5.9960 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.85857\n",
            "epochs: 148\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 5.8279 - out_loss: 0.0586 - out_2_loss: 5.7694 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 5.85857 to 5.82791, saving model to ./my_model.h5\n",
            "epochs: 149\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 6.1755 - out_loss: 0.0829 - out_2_loss: 6.0927 - out_acc: 0.9714 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.82791\n",
            "epochs: 150\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 5.8428 - out_loss: 0.1216 - out_2_loss: 5.7212 - out_acc: 0.9714 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.82791\n",
            "epochs: 151\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 6.1503 - out_loss: 0.0263 - out_2_loss: 6.1240 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.82791\n",
            "epochs: 152\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.7343 - out_loss: 0.0235 - out_2_loss: 5.7108 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 5.82791 to 5.73433, saving model to ./my_model.h5\n",
            "epochs: 153\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 5.7045 - out_loss: 0.0892 - out_2_loss: 5.6153 - out_acc: 0.9714 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 5.73433 to 5.70451, saving model to ./my_model.h5\n",
            "epochs: 154\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 5.8285 - out_loss: 0.0378 - out_2_loss: 5.7907 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 155\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 5.7971 - out_loss: 0.0297 - out_2_loss: 5.7674 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 156\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 5.9804 - out_loss: 0.0348 - out_2_loss: 5.9456 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 157\n",
            "1/1 [==============================] - 1s 703ms/step - loss: 5.7517 - out_loss: 0.0534 - out_2_loss: 5.6983 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 158\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 6.0957 - out_loss: 0.0329 - out_2_loss: 6.0628 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 159\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 5.9493 - out_loss: 0.0353 - out_2_loss: 5.9140 - out_acc: 0.9857 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 160\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 5.7547 - out_loss: 0.0275 - out_2_loss: 5.7271 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 161\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 6.0947 - out_loss: 0.0222 - out_2_loss: 6.0725 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 162\n",
            "1/1 [==============================] - 1s 646ms/step - loss: 5.9366 - out_loss: 0.0247 - out_2_loss: 5.9119 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 163\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 5.7623 - out_loss: 0.0329 - out_2_loss: 5.7293 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.70451\n",
            "epochs: 164\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 5.7012 - out_loss: 0.0211 - out_2_loss: 5.6801 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 5.70451 to 5.70119, saving model to ./my_model.h5\n",
            "epochs: 165\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 5.6521 - out_loss: 0.0177 - out_2_loss: 5.6344 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 5.70119 to 5.65206, saving model to ./my_model.h5\n",
            "epochs: 166\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.9404 - out_loss: 0.0218 - out_2_loss: 5.9185 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 167\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 6.0556 - out_loss: 0.0138 - out_2_loss: 6.0418 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 168\n",
            "1/1 [==============================] - 1s 652ms/step - loss: 5.9939 - out_loss: 0.0105 - out_2_loss: 5.9834 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 169\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 5.8572 - out_loss: 0.0124 - out_2_loss: 5.8448 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 170\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 5.9897 - out_loss: 0.0130 - out_2_loss: 5.9767 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 171\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 5.9951 - out_loss: 0.0136 - out_2_loss: 5.9815 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 172\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 5.7534 - out_loss: 0.0107 - out_2_loss: 5.7427 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 173\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 5.7666 - out_loss: 0.0099 - out_2_loss: 5.7567 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 174\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 5.9528 - out_loss: 0.0102 - out_2_loss: 5.9426 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 175\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 5.6872 - out_loss: 0.0071 - out_2_loss: 5.6801 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.65206\n",
            "epochs: 176\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 5.6237 - out_loss: 0.0119 - out_2_loss: 5.6118 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 5.65206 to 5.62372, saving model to ./my_model.h5\n",
            "epochs: 177\n",
            "1/1 [==============================] - 1s 645ms/step - loss: 5.7812 - out_loss: 0.0076 - out_2_loss: 5.7736 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.62372\n",
            "epochs: 178\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 5.7639 - out_loss: 0.0130 - out_2_loss: 5.7509 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.62372\n",
            "epochs: 179\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 5.6243 - out_loss: 0.0130 - out_2_loss: 5.6113 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.62372\n",
            "epochs: 180\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 5.9521 - out_loss: 0.0088 - out_2_loss: 5.9433 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.62372\n",
            "epochs: 181\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 5.9301 - out_loss: 0.0128 - out_2_loss: 5.9173 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.62372\n",
            "epochs: 182\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 5.7554 - out_loss: 0.0099 - out_2_loss: 5.7455 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.62372\n",
            "epochs: 183\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 5.9758 - out_loss: 0.0109 - out_2_loss: 5.9649 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.62372\n",
            "epochs: 184\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 5.9412 - out_loss: 0.0185 - out_2_loss: 5.9227 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.62372\n",
            "epochs: 185\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 5.7108 - out_loss: 0.0104 - out_2_loss: 5.7004 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.62372\n",
            "epochs: 186\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 5.6079 - out_loss: 0.0096 - out_2_loss: 5.5983 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 5.62372 to 5.60791, saving model to ./my_model.h5\n",
            "epochs: 187\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 5.8149 - out_loss: 0.0090 - out_2_loss: 5.8058 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.60791\n",
            "epochs: 188\n",
            "1/1 [==============================] - 1s 644ms/step - loss: 5.4072 - out_loss: 0.0091 - out_2_loss: 5.3981 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 5.60791 to 5.40718, saving model to ./my_model.h5\n",
            "epochs: 189\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 5.7647 - out_loss: 0.0082 - out_2_loss: 5.7564 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 190\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 5.7621 - out_loss: 0.0167 - out_2_loss: 5.7453 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 191\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 5.8192 - out_loss: 0.0211 - out_2_loss: 5.7981 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 192\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 5.9218 - out_loss: 0.0355 - out_2_loss: 5.8863 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 193\n",
            "1/1 [==============================] - 1s 645ms/step - loss: 5.6135 - out_loss: 0.0078 - out_2_loss: 5.6057 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 194\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 5.9067 - out_loss: 0.0323 - out_2_loss: 5.8744 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 195\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 5.6909 - out_loss: 0.0133 - out_2_loss: 5.6777 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 196\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 5.7375 - out_loss: 0.0081 - out_2_loss: 5.7295 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 197\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 5.6399 - out_loss: 0.0082 - out_2_loss: 5.6317 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 198\n",
            "1/1 [==============================] - 1s 650ms/step - loss: 5.5435 - out_loss: 0.0069 - out_2_loss: 5.5366 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 199\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 5.4430 - out_loss: 0.0055 - out_2_loss: 5.4375 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 200\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 5.4529 - out_loss: 0.0063 - out_2_loss: 5.4466 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 201\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 5.8124 - out_loss: 0.0103 - out_2_loss: 5.8021 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 202\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 5.7521 - out_loss: 0.0116 - out_2_loss: 5.7404 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 203\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 5.8181 - out_loss: 0.0099 - out_2_loss: 5.8082 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 204\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 5.5939 - out_loss: 0.0092 - out_2_loss: 5.5848 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 205\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 5.4552 - out_loss: 0.0192 - out_2_loss: 5.4361 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 206\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 5.8743 - out_loss: 0.0075 - out_2_loss: 5.8668 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 207\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 5.7238 - out_loss: 0.0094 - out_2_loss: 5.7144 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 208\n",
            "1/1 [==============================] - 1s 709ms/step - loss: 5.5790 - out_loss: 0.0211 - out_2_loss: 5.5578 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.40718\n",
            "epochs: 209\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 5.3985 - out_loss: 0.0109 - out_2_loss: 5.3876 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 5.40718 to 5.39849, saving model to ./my_model.h5\n",
            "epochs: 210\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 5.8038 - out_loss: 0.0133 - out_2_loss: 5.7905 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.39849\n",
            "epochs: 211\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 5.7247 - out_loss: 0.0132 - out_2_loss: 5.7115 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.39849\n",
            "epochs: 212\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 5.7527 - out_loss: 0.0125 - out_2_loss: 5.7402 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.39849\n",
            "epochs: 213\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 5.5161 - out_loss: 0.0151 - out_2_loss: 5.5010 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.39849\n",
            "epochs: 214\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 5.5620 - out_loss: 0.0128 - out_2_loss: 5.5492 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.39849\n",
            "epochs: 215\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 5.5148 - out_loss: 0.0103 - out_2_loss: 5.5045 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.39849\n",
            "epochs: 216\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 5.6646 - out_loss: 0.0078 - out_2_loss: 5.6569 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.39849\n",
            "epochs: 217\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 5.6263 - out_loss: 0.0107 - out_2_loss: 5.6155 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.39849\n",
            "epochs: 218\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 5.6055 - out_loss: 0.0165 - out_2_loss: 5.5890 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.39849\n",
            "epochs: 219\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 5.3644 - out_loss: 0.0154 - out_2_loss: 5.3490 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 5.39849 to 5.36442, saving model to ./my_model.h5\n",
            "epochs: 220\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 5.4389 - out_loss: 0.0107 - out_2_loss: 5.4281 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.36442\n",
            "epochs: 221\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 5.3610 - out_loss: 0.0079 - out_2_loss: 5.3531 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 5.36442 to 5.36101, saving model to ./my_model.h5\n",
            "epochs: 222\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 5.5547 - out_loss: 0.0058 - out_2_loss: 5.5489 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.36101\n",
            "epochs: 223\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 5.3236 - out_loss: 0.0066 - out_2_loss: 5.3170 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss improved from 5.36101 to 5.32358, saving model to ./my_model.h5\n",
            "epochs: 224\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 5.5552 - out_loss: 0.0061 - out_2_loss: 5.5492 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 225\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 5.7601 - out_loss: 0.0053 - out_2_loss: 5.7548 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 226\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 5.4647 - out_loss: 0.0057 - out_2_loss: 5.4589 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 227\n",
            "1/1 [==============================] - 1s 720ms/step - loss: 5.3717 - out_loss: 0.0101 - out_2_loss: 5.3617 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 228\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 5.3585 - out_loss: 0.0074 - out_2_loss: 5.3511 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 229\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 5.3973 - out_loss: 0.0076 - out_2_loss: 5.3898 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 230\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 5.6231 - out_loss: 0.0165 - out_2_loss: 5.6066 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 231\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 5.7874 - out_loss: 0.0226 - out_2_loss: 5.7648 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 232\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 5.5674 - out_loss: 0.0092 - out_2_loss: 5.5583 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 233\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 5.5821 - out_loss: 0.0120 - out_2_loss: 5.5701 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 234\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 5.4622 - out_loss: 0.0162 - out_2_loss: 5.4460 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 235\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 5.4619 - out_loss: 0.0130 - out_2_loss: 5.4489 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 236\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 5.6214 - out_loss: 0.0131 - out_2_loss: 5.6083 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 237\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 5.6108 - out_loss: 0.0120 - out_2_loss: 5.5988 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 238\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 5.4517 - out_loss: 0.0112 - out_2_loss: 5.4406 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 239\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 5.3325 - out_loss: 0.0122 - out_2_loss: 5.3203 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 240\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 5.4937 - out_loss: 0.0110 - out_2_loss: 5.4827 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 241\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.4118 - out_loss: 0.0078 - out_2_loss: 5.4040 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.32358\n",
            "epochs: 242\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 5.2696 - out_loss: 0.0052 - out_2_loss: 5.2644 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 5.32358 to 5.26957, saving model to ./my_model.h5\n",
            "epochs: 243\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.5625 - out_loss: 0.0088 - out_2_loss: 5.5537 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.26957\n",
            "epochs: 244\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 5.4316 - out_loss: 0.0061 - out_2_loss: 5.4254 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.26957\n",
            "epochs: 245\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 5.4397 - out_loss: 0.0070 - out_2_loss: 5.4327 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.26957\n",
            "epochs: 246\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 5.5384 - out_loss: 0.0064 - out_2_loss: 5.5320 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.26957\n",
            "epochs: 247\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 5.4202 - out_loss: 0.0067 - out_2_loss: 5.4134 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.26957\n",
            "epochs: 248\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 5.4430 - out_loss: 0.0173 - out_2_loss: 5.4257 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.26957\n",
            "epochs: 249\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 5.2218 - out_loss: 0.0063 - out_2_loss: 5.2155 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 5.26957 to 5.22180, saving model to ./my_model.h5\n",
            "epochs: 250\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 5.3681 - out_loss: 0.0061 - out_2_loss: 5.3620 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.22180\n",
            "epochs: 251\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 5.4168 - out_loss: 0.0061 - out_2_loss: 5.4107 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.22180\n",
            "epochs: 252\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 5.2364 - out_loss: 0.0056 - out_2_loss: 5.2308 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.22180\n",
            "epochs: 253\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 5.4764 - out_loss: 0.0054 - out_2_loss: 5.4710 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.22180\n",
            "epochs: 254\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 5.1080 - out_loss: 0.0095 - out_2_loss: 5.0985 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 5.22180 to 5.10800, saving model to ./my_model.h5\n",
            "epochs: 255\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 5.4161 - out_loss: 0.0054 - out_2_loss: 5.4107 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.10800\n",
            "epochs: 256\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 5.2802 - out_loss: 0.0089 - out_2_loss: 5.2712 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.10800\n",
            "epochs: 257\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 5.4112 - out_loss: 0.0110 - out_2_loss: 5.4001 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.10800\n",
            "epochs: 258\n",
            "1/1 [==============================] - 1s 650ms/step - loss: 5.2905 - out_loss: 0.0168 - out_2_loss: 5.2737 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.10800\n",
            "epochs: 259\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 5.4169 - out_loss: 0.0115 - out_2_loss: 5.4053 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.10800\n",
            "epochs: 260\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 5.4171 - out_loss: 0.0062 - out_2_loss: 5.4109 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.10800\n",
            "epochs: 261\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.3083 - out_loss: 0.0120 - out_2_loss: 5.2964 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.10800\n",
            "epochs: 262\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 5.2146 - out_loss: 0.0174 - out_2_loss: 5.1972 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.10800\n",
            "epochs: 263\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.0359 - out_loss: 0.0137 - out_2_loss: 5.0222 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 5.10800 to 5.03592, saving model to ./my_model.h5\n",
            "epochs: 264\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 5.5427 - out_loss: 0.0138 - out_2_loss: 5.5289 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 265\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 5.5429 - out_loss: 0.0069 - out_2_loss: 5.5360 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 266\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 5.2970 - out_loss: 0.0058 - out_2_loss: 5.2912 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 267\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 5.1172 - out_loss: 0.0061 - out_2_loss: 5.1112 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 268\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 5.1882 - out_loss: 0.0073 - out_2_loss: 5.1808 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 269\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 5.0725 - out_loss: 0.0073 - out_2_loss: 5.0653 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 270\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 5.2498 - out_loss: 0.0072 - out_2_loss: 5.2426 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 271\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 5.3188 - out_loss: 0.0099 - out_2_loss: 5.3089 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 272\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 5.1622 - out_loss: 0.0071 - out_2_loss: 5.1550 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 273\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 5.1719 - out_loss: 0.0100 - out_2_loss: 5.1619 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 274\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.5061 - out_loss: 0.0071 - out_2_loss: 5.4989 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 275\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 5.4288 - out_loss: 0.0069 - out_2_loss: 5.4220 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 276\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 5.1567 - out_loss: 0.0059 - out_2_loss: 5.1508 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 277\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 5.2263 - out_loss: 0.0053 - out_2_loss: 5.2210 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 278\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 5.4624 - out_loss: 0.0047 - out_2_loss: 5.4577 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 279\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 5.3699 - out_loss: 0.0053 - out_2_loss: 5.3646 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 280\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 5.4674 - out_loss: 0.0043 - out_2_loss: 5.4631 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 281\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 5.4694 - out_loss: 0.0032 - out_2_loss: 5.4662 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 282\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 5.1516 - out_loss: 0.0042 - out_2_loss: 5.1474 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 283\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 5.3964 - out_loss: 0.0045 - out_2_loss: 5.3919 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 284\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 5.3029 - out_loss: 0.0070 - out_2_loss: 5.2959 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 285\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.2467 - out_loss: 0.0089 - out_2_loss: 5.2378 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 286\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 5.0914 - out_loss: 0.0094 - out_2_loss: 5.0821 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 287\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 5.1970 - out_loss: 0.0047 - out_2_loss: 5.1923 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 288\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 5.4544 - out_loss: 0.0046 - out_2_loss: 5.4498 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 289\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 5.3986 - out_loss: 0.0036 - out_2_loss: 5.3949 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 290\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 5.1450 - out_loss: 0.0053 - out_2_loss: 5.1397 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 5.03592\n",
            "epochs: 291\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 4.9017 - out_loss: 0.0060 - out_2_loss: 4.8957 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss improved from 5.03592 to 4.90170, saving model to ./my_model.h5\n",
            "epochs: 292\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 5.4568 - out_loss: 0.0152 - out_2_loss: 5.4416 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 293\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 5.4285 - out_loss: 0.0065 - out_2_loss: 5.4221 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 294\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 5.2798 - out_loss: 0.0076 - out_2_loss: 5.2723 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 295\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 5.4652 - out_loss: 0.0037 - out_2_loss: 5.4615 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 296\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.3743 - out_loss: 0.0042 - out_2_loss: 5.3701 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 297\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 5.2823 - out_loss: 0.2093 - out_2_loss: 5.0729 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 298\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 5.1698 - out_loss: 0.0550 - out_2_loss: 5.1148 - out_acc: 0.9714 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 299\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 5.8814 - out_loss: 0.2680 - out_2_loss: 5.6134 - out_acc: 0.9429 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 300\n",
            "1/1 [==============================] - 1s 650ms/step - loss: 5.6357 - out_loss: 0.1781 - out_2_loss: 5.4576 - out_acc: 0.9714 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 301\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 5.6628 - out_loss: 0.1119 - out_2_loss: 5.5510 - out_acc: 0.9571 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 302\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 5.4920 - out_loss: 0.0336 - out_2_loss: 5.4584 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 303\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 5.3331 - out_loss: 0.0465 - out_2_loss: 5.2866 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 304\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 5.5870 - out_loss: 0.0527 - out_2_loss: 5.5344 - out_acc: 0.9857 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 305\n",
            "1/1 [==============================] - 1s 726ms/step - loss: 5.4137 - out_loss: 0.0580 - out_2_loss: 5.3557 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 306\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 5.3261 - out_loss: 0.0385 - out_2_loss: 5.2876 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 307\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.3074 - out_loss: 0.0488 - out_2_loss: 5.2586 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 308\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 5.4833 - out_loss: 0.0544 - out_2_loss: 5.4289 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 309\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 5.3180 - out_loss: 0.0503 - out_2_loss: 5.2677 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 310\n",
            "1/1 [==============================] - 1s 649ms/step - loss: 5.2367 - out_loss: 0.0403 - out_2_loss: 5.1964 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 311\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 5.3445 - out_loss: 0.0523 - out_2_loss: 5.2922 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 312\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 5.4966 - out_loss: 0.0631 - out_2_loss: 5.4335 - out_acc: 0.9714 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 313\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 5.5551 - out_loss: 0.0298 - out_2_loss: 5.5253 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 314\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 5.5147 - out_loss: 0.0687 - out_2_loss: 5.4459 - out_acc: 0.9857 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 315\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 5.2258 - out_loss: 0.0243 - out_2_loss: 5.2016 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 316\n",
            "1/1 [==============================] - 1s 645ms/step - loss: 5.3079 - out_loss: 0.0215 - out_2_loss: 5.2864 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 317\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 5.5725 - out_loss: 0.0511 - out_2_loss: 5.5214 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 318\n",
            "1/1 [==============================] - 1s 643ms/step - loss: 5.3856 - out_loss: 0.0453 - out_2_loss: 5.3403 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 319\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 5.5215 - out_loss: 0.1779 - out_2_loss: 5.3436 - out_acc: 0.9429 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 320\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 5.6020 - out_loss: 0.2938 - out_2_loss: 5.3082 - out_acc: 0.9143 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 321\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 5.6220 - out_loss: 0.2075 - out_2_loss: 5.4145 - out_acc: 0.9286 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 322\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 5.4126 - out_loss: 0.1455 - out_2_loss: 5.2671 - out_acc: 0.9714 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 323\n",
            "1/1 [==============================] - 1s 717ms/step - loss: 5.7558 - out_loss: 0.2198 - out_2_loss: 5.5360 - out_acc: 0.9571 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 324\n",
            "1/1 [==============================] - 1s 728ms/step - loss: 5.5750 - out_loss: 0.1662 - out_2_loss: 5.4087 - out_acc: 0.9571 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 325\n",
            "1/1 [==============================] - 1s 718ms/step - loss: 5.3081 - out_loss: 0.1132 - out_2_loss: 5.1949 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 326\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 5.3752 - out_loss: 0.0951 - out_2_loss: 5.2800 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 327\n",
            "1/1 [==============================] - 1s 708ms/step - loss: 5.4019 - out_loss: 0.0591 - out_2_loss: 5.3428 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 328\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 5.4912 - out_loss: 0.0380 - out_2_loss: 5.4533 - out_acc: 0.9857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 329\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 5.3494 - out_loss: 0.0323 - out_2_loss: 5.3171 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 330\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 5.4415 - out_loss: 0.0275 - out_2_loss: 5.4140 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 331\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 5.3554 - out_loss: 0.0249 - out_2_loss: 5.3305 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 332\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 5.3279 - out_loss: 0.0267 - out_2_loss: 5.3013 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 333\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 5.2204 - out_loss: 0.0158 - out_2_loss: 5.2046 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 334\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 5.2774 - out_loss: 0.0174 - out_2_loss: 5.2601 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 335\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 5.3711 - out_loss: 0.0139 - out_2_loss: 5.3572 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 336\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 5.1658 - out_loss: 0.0150 - out_2_loss: 5.1508 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 337\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 5.3172 - out_loss: 0.0138 - out_2_loss: 5.3035 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 338\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 5.2275 - out_loss: 0.0176 - out_2_loss: 5.2098 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 339\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 5.3511 - out_loss: 0.0175 - out_2_loss: 5.3337 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 340\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 4.9794 - out_loss: 0.0208 - out_2_loss: 4.9587 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 341\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 5.2236 - out_loss: 0.0277 - out_2_loss: 5.1959 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 342\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 5.1499 - out_loss: 0.0205 - out_2_loss: 5.1295 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 343\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 5.0862 - out_loss: 0.0141 - out_2_loss: 5.0721 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 344\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 5.0277 - out_loss: 0.0181 - out_2_loss: 5.0097 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 345\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 5.1978 - out_loss: 0.0298 - out_2_loss: 5.1680 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 346\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 5.3623 - out_loss: 0.0101 - out_2_loss: 5.3522 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 347\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 5.2607 - out_loss: 0.0296 - out_2_loss: 5.2311 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 348\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 5.2601 - out_loss: 0.0139 - out_2_loss: 5.2462 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 349\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 4.9941 - out_loss: 0.0194 - out_2_loss: 4.9747 - out_acc: 0.9857 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 350\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 5.1131 - out_loss: 0.0077 - out_2_loss: 5.1054 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 351\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 5.3955 - out_loss: 0.0095 - out_2_loss: 5.3861 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 352\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 5.0941 - out_loss: 0.0076 - out_2_loss: 5.0865 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 353\n",
            "1/1 [==============================] - 1s 722ms/step - loss: 5.2294 - out_loss: 0.0075 - out_2_loss: 5.2220 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 354\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 5.1810 - out_loss: 0.0056 - out_2_loss: 5.1754 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 355\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 5.2963 - out_loss: 0.0083 - out_2_loss: 5.2880 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 356\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 5.0727 - out_loss: 0.0052 - out_2_loss: 5.0675 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 357\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 5.3300 - out_loss: 0.0047 - out_2_loss: 5.3253 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 358\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 5.1822 - out_loss: 0.0044 - out_2_loss: 5.1777 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 359\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 5.1647 - out_loss: 0.0050 - out_2_loss: 5.1598 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 360\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 5.0912 - out_loss: 0.0066 - out_2_loss: 5.0846 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 361\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 5.1154 - out_loss: 0.0071 - out_2_loss: 5.1084 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 362\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 5.1087 - out_loss: 0.0045 - out_2_loss: 5.1042 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.90170\n",
            "epochs: 363\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.8881 - out_loss: 0.0057 - out_2_loss: 4.8824 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 4.90170 to 4.88810, saving model to ./my_model.h5\n",
            "epochs: 364\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 5.2062 - out_loss: 0.0077 - out_2_loss: 5.1985 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 365\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.8939 - out_loss: 0.0542 - out_2_loss: 4.8397 - out_acc: 0.9857 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 366\n",
            "1/1 [==============================] - 1s 644ms/step - loss: 5.1636 - out_loss: 0.0070 - out_2_loss: 5.1566 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 367\n",
            "1/1 [==============================] - 1s 735ms/step - loss: 5.1530 - out_loss: 0.0163 - out_2_loss: 5.1368 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 368\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 5.0400 - out_loss: 0.0068 - out_2_loss: 5.0333 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 369\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 5.1377 - out_loss: 0.0038 - out_2_loss: 5.1339 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 370\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 5.0744 - out_loss: 0.0087 - out_2_loss: 5.0657 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 371\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 5.0051 - out_loss: 0.0085 - out_2_loss: 4.9966 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 372\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.9671 - out_loss: 0.0133 - out_2_loss: 4.9538 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 373\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 5.0151 - out_loss: 0.0055 - out_2_loss: 5.0096 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 374\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 5.0328 - out_loss: 0.0035 - out_2_loss: 5.0293 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 375\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 5.2902 - out_loss: 0.0033 - out_2_loss: 5.2869 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 376\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 4.9744 - out_loss: 0.0050 - out_2_loss: 4.9694 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 377\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 5.0502 - out_loss: 0.0037 - out_2_loss: 5.0465 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 378\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 5.0323 - out_loss: 0.0045 - out_2_loss: 5.0278 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 379\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 5.1280 - out_loss: 0.0125 - out_2_loss: 5.1155 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 380\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 5.1578 - out_loss: 0.0054 - out_2_loss: 5.1524 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 381\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 5.0808 - out_loss: 0.0059 - out_2_loss: 5.0749 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 382\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 5.0569 - out_loss: 0.0054 - out_2_loss: 5.0515 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 383\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 5.1908 - out_loss: 0.0064 - out_2_loss: 5.1844 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 384\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 5.1168 - out_loss: 0.0040 - out_2_loss: 5.1127 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 385\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 5.0688 - out_loss: 0.0041 - out_2_loss: 5.0647 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.88810\n",
            "epochs: 386\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.8583 - out_loss: 0.0062 - out_2_loss: 4.8521 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 4.88810 to 4.85835, saving model to ./my_model.h5\n",
            "epochs: 387\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.8460 - out_loss: 0.0048 - out_2_loss: 4.8412 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 4.85835 to 4.84599, saving model to ./my_model.h5\n",
            "epochs: 388\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.8744 - out_loss: 0.0047 - out_2_loss: 4.8697 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.84599\n",
            "epochs: 389\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 4.8599 - out_loss: 0.0052 - out_2_loss: 4.8546 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.84599\n",
            "epochs: 390\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 4.9517 - out_loss: 0.0045 - out_2_loss: 4.9472 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.84599\n",
            "epochs: 391\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 5.0895 - out_loss: 0.0060 - out_2_loss: 5.0834 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.84599\n",
            "epochs: 392\n",
            "1/1 [==============================] - 1s 721ms/step - loss: 4.9646 - out_loss: 0.0073 - out_2_loss: 4.9573 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.84599\n",
            "epochs: 393\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.9675 - out_loss: 0.0099 - out_2_loss: 4.9576 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.84599\n",
            "epochs: 394\n",
            "1/1 [==============================] - 1s 720ms/step - loss: 4.9355 - out_loss: 0.0086 - out_2_loss: 4.9268 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.84599\n",
            "epochs: 395\n",
            "1/1 [==============================] - 1s 760ms/step - loss: 4.8765 - out_loss: 0.0089 - out_2_loss: 4.8676 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.84599\n",
            "epochs: 396\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 5.0489 - out_loss: 0.0039 - out_2_loss: 5.0450 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.84599\n",
            "epochs: 397\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.6778 - out_loss: 0.0051 - out_2_loss: 4.6727 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss improved from 4.84599 to 4.67779, saving model to ./my_model.h5\n",
            "epochs: 398\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.7939 - out_loss: 0.0052 - out_2_loss: 4.7887 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.67779\n",
            "epochs: 399\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.7777 - out_loss: 0.0041 - out_2_loss: 4.7736 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.67779\n",
            "epochs: 400\n",
            "1/1 [==============================] - 1s 708ms/step - loss: 4.8903 - out_loss: 0.0052 - out_2_loss: 4.8851 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.67779\n",
            "epochs: 401\n",
            "1/1 [==============================] - 1s 710ms/step - loss: 4.6427 - out_loss: 0.0064 - out_2_loss: 4.6363 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss improved from 4.67779 to 4.64267, saving model to ./my_model.h5\n",
            "epochs: 402\n",
            "1/1 [==============================] - 1s 743ms/step - loss: 4.8094 - out_loss: 0.0041 - out_2_loss: 4.8053 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.64267\n",
            "epochs: 403\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.9991 - out_loss: 0.0042 - out_2_loss: 4.9948 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.64267\n",
            "epochs: 404\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.7867 - out_loss: 0.0044 - out_2_loss: 4.7822 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.64267\n",
            "epochs: 405\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 5.0491 - out_loss: 0.0039 - out_2_loss: 5.0452 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.64267\n",
            "epochs: 406\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 4.9683 - out_loss: 0.0039 - out_2_loss: 4.9645 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.64267\n",
            "epochs: 407\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 4.8901 - out_loss: 0.0040 - out_2_loss: 4.8861 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.64267\n",
            "epochs: 408\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 4.9630 - out_loss: 0.0055 - out_2_loss: 4.9575 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.64267\n",
            "epochs: 409\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.8157 - out_loss: 0.0026 - out_2_loss: 4.8131 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.64267\n",
            "epochs: 410\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 4.4648 - out_loss: 0.0045 - out_2_loss: 4.4603 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss improved from 4.64267 to 4.46482, saving model to ./my_model.h5\n",
            "epochs: 411\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.8998 - out_loss: 0.0037 - out_2_loss: 4.8961 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 412\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.8774 - out_loss: 0.0026 - out_2_loss: 4.8747 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 413\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.8232 - out_loss: 0.0032 - out_2_loss: 4.8199 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 414\n",
            "1/1 [==============================] - 1s 714ms/step - loss: 5.0281 - out_loss: 0.0038 - out_2_loss: 5.0243 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 415\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 4.7662 - out_loss: 0.0029 - out_2_loss: 4.7632 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 416\n",
            "1/1 [==============================] - 1s 797ms/step - loss: 4.9135 - out_loss: 0.0031 - out_2_loss: 4.9104 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 417\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 4.8406 - out_loss: 0.0113 - out_2_loss: 4.8294 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 418\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 4.6043 - out_loss: 0.0050 - out_2_loss: 4.5993 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 419\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.6983 - out_loss: 0.0039 - out_2_loss: 4.6945 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 420\n",
            "1/1 [==============================] - 1s 710ms/step - loss: 4.7127 - out_loss: 0.0051 - out_2_loss: 4.7076 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 421\n",
            "1/1 [==============================] - 1s 708ms/step - loss: 4.6700 - out_loss: 0.0025 - out_2_loss: 4.6675 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 422\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 4.6099 - out_loss: 0.0031 - out_2_loss: 4.6067 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 423\n",
            "1/1 [==============================] - 1s 731ms/step - loss: 4.6472 - out_loss: 0.0020 - out_2_loss: 4.6452 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 424\n",
            "1/1 [==============================] - 1s 738ms/step - loss: 4.8591 - out_loss: 0.0030 - out_2_loss: 4.8562 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 425\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 4.7748 - out_loss: 0.0028 - out_2_loss: 4.7720 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 426\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 5.0130 - out_loss: 0.0058 - out_2_loss: 5.0072 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 427\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.7631 - out_loss: 0.0037 - out_2_loss: 4.7593 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 428\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 4.7561 - out_loss: 0.0051 - out_2_loss: 4.7510 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 429\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.6980 - out_loss: 0.0030 - out_2_loss: 4.6950 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 430\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.7166 - out_loss: 0.0066 - out_2_loss: 4.7100 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 431\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 4.9917 - out_loss: 0.0038 - out_2_loss: 4.9879 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.46482\n",
            "epochs: 432\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 4.4585 - out_loss: 0.0029 - out_2_loss: 4.4556 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 4.46482 to 4.45850, saving model to ./my_model.h5\n",
            "epochs: 433\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 4.7096 - out_loss: 0.0051 - out_2_loss: 4.7045 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.45850\n",
            "epochs: 434\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.7548 - out_loss: 0.0047 - out_2_loss: 4.7501 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.45850\n",
            "epochs: 435\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.6244 - out_loss: 0.0038 - out_2_loss: 4.6206 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.45850\n",
            "epochs: 436\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.6895 - out_loss: 0.0060 - out_2_loss: 4.6835 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.45850\n",
            "epochs: 437\n",
            "1/1 [==============================] - 1s 721ms/step - loss: 4.6129 - out_loss: 0.0022 - out_2_loss: 4.6107 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.45850\n",
            "epochs: 438\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.8847 - out_loss: 0.0074 - out_2_loss: 4.8773 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.45850\n",
            "epochs: 439\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 4.8261 - out_loss: 0.0081 - out_2_loss: 4.8181 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.45850\n",
            "epochs: 440\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.8571 - out_loss: 0.0016 - out_2_loss: 4.8555 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.45850\n",
            "epochs: 441\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.6691 - out_loss: 0.0031 - out_2_loss: 4.6661 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.45850\n",
            "epochs: 442\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 4.4236 - out_loss: 0.0018 - out_2_loss: 4.4217 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss improved from 4.45850 to 4.42356, saving model to ./my_model.h5\n",
            "epochs: 443\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 4.9609 - out_loss: 0.0023 - out_2_loss: 4.9587 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 444\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 4.4900 - out_loss: 0.0020 - out_2_loss: 4.4880 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 445\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 4.7151 - out_loss: 0.0017 - out_2_loss: 4.7135 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 446\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 4.5269 - out_loss: 0.0027 - out_2_loss: 4.5241 - out_acc: 1.0000 - out_2_acc: 0.1286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 447\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.7647 - out_loss: 0.0023 - out_2_loss: 4.7623 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 448\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 4.4879 - out_loss: 0.0016 - out_2_loss: 4.4863 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 449\n",
            "1/1 [==============================] - 1s 728ms/step - loss: 5.0039 - out_loss: 0.0021 - out_2_loss: 5.0018 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 450\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.5571 - out_loss: 0.0038 - out_2_loss: 4.5533 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 451\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 4.8528 - out_loss: 0.0025 - out_2_loss: 4.8503 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 452\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 4.6398 - out_loss: 0.0029 - out_2_loss: 4.6369 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 453\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.7995 - out_loss: 0.0037 - out_2_loss: 4.7957 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 454\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.8500 - out_loss: 0.0047 - out_2_loss: 4.8454 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 455\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 4.6121 - out_loss: 0.0036 - out_2_loss: 4.6085 - out_acc: 1.0000 - out_2_acc: 0.1286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 456\n",
            "1/1 [==============================] - 1s 720ms/step - loss: 4.6106 - out_loss: 0.0036 - out_2_loss: 4.6069 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 457\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.5942 - out_loss: 0.0018 - out_2_loss: 4.5924 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 458\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 4.7763 - out_loss: 0.0055 - out_2_loss: 4.7708 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 459\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.5534 - out_loss: 0.0137 - out_2_loss: 4.5397 - out_acc: 0.9857 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 460\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.8582 - out_loss: 0.0079 - out_2_loss: 4.8502 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 461\n",
            "1/1 [==============================] - 1s 633ms/step - loss: 4.7955 - out_loss: 0.0027 - out_2_loss: 4.7927 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 462\n",
            "1/1 [==============================] - 1s 634ms/step - loss: 4.6324 - out_loss: 0.0024 - out_2_loss: 4.6300 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 463\n",
            "1/1 [==============================] - 1s 638ms/step - loss: 4.7016 - out_loss: 0.0020 - out_2_loss: 4.6997 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 464\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 4.6621 - out_loss: 0.0042 - out_2_loss: 4.6578 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 465\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.5527 - out_loss: 0.0022 - out_2_loss: 4.5505 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 466\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.8011 - out_loss: 0.0032 - out_2_loss: 4.7979 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 467\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 4.4398 - out_loss: 0.0060 - out_2_loss: 4.4338 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 468\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.6871 - out_loss: 0.0027 - out_2_loss: 4.6844 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 469\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 4.8148 - out_loss: 0.0018 - out_2_loss: 4.8130 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 470\n",
            "1/1 [==============================] - 1s 710ms/step - loss: 4.8738 - out_loss: 0.0017 - out_2_loss: 4.8721 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 471\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.6204 - out_loss: 0.0055 - out_2_loss: 4.6149 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 472\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.6295 - out_loss: 0.0033 - out_2_loss: 4.6262 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 473\n",
            "1/1 [==============================] - 1s 650ms/step - loss: 4.7581 - out_loss: 0.0045 - out_2_loss: 4.7536 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 474\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 5.0775 - out_loss: 0.0028 - out_2_loss: 5.0748 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 475\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 4.6736 - out_loss: 0.0033 - out_2_loss: 4.6703 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 476\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.6034 - out_loss: 0.0030 - out_2_loss: 4.6003 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 477\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.7762 - out_loss: 0.0066 - out_2_loss: 4.7696 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 478\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.9241 - out_loss: 0.0043 - out_2_loss: 4.9198 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 479\n",
            "1/1 [==============================] - 1s 703ms/step - loss: 4.7679 - out_loss: 0.0047 - out_2_loss: 4.7632 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 480\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 4.8861 - out_loss: 0.0043 - out_2_loss: 4.8818 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 481\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.6052 - out_loss: 0.0039 - out_2_loss: 4.6013 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 482\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.8548 - out_loss: 0.0221 - out_2_loss: 4.8327 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 483\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.6543 - out_loss: 0.0110 - out_2_loss: 4.6433 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 484\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.7244 - out_loss: 0.0042 - out_2_loss: 4.7202 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 485\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.5773 - out_loss: 0.0058 - out_2_loss: 4.5715 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 486\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.5434 - out_loss: 0.0035 - out_2_loss: 4.5399 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 487\n",
            "1/1 [==============================] - 1s 721ms/step - loss: 4.6199 - out_loss: 0.0021 - out_2_loss: 4.6178 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.42356\n",
            "epochs: 488\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 4.2926 - out_loss: 0.0037 - out_2_loss: 4.2889 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss improved from 4.42356 to 4.29256, saving model to ./my_model.h5\n",
            "epochs: 489\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.5978 - out_loss: 0.0029 - out_2_loss: 4.5949 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 490\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.7363 - out_loss: 0.0022 - out_2_loss: 4.7341 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 491\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.6764 - out_loss: 0.0018 - out_2_loss: 4.6746 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 492\n",
            "1/1 [==============================] - 1s 710ms/step - loss: 4.6737 - out_loss: 0.0021 - out_2_loss: 4.6716 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 493\n",
            "1/1 [==============================] - 1s 718ms/step - loss: 4.6009 - out_loss: 0.0023 - out_2_loss: 4.5986 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 494\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.6068 - out_loss: 0.0022 - out_2_loss: 4.6046 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 495\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 4.7325 - out_loss: 0.0032 - out_2_loss: 4.7293 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 496\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 4.4826 - out_loss: 0.0031 - out_2_loss: 4.4794 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 497\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 4.4911 - out_loss: 0.0150 - out_2_loss: 4.4761 - out_acc: 0.9857 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 498\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.4274 - out_loss: 0.0044 - out_2_loss: 4.4230 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 499\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 4.4752 - out_loss: 0.0025 - out_2_loss: 4.4727 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 500\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.4942 - out_loss: 0.0020 - out_2_loss: 4.4922 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 501\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.5507 - out_loss: 0.0026 - out_2_loss: 4.5481 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 502\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.6918 - out_loss: 0.0078 - out_2_loss: 4.6840 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 503\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 4.4547 - out_loss: 0.0073 - out_2_loss: 4.4474 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 504\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.7987 - out_loss: 0.0016 - out_2_loss: 4.7970 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 505\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 4.5260 - out_loss: 0.0018 - out_2_loss: 4.5242 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 506\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.7036 - out_loss: 0.0015 - out_2_loss: 4.7020 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 507\n",
            "1/1 [==============================] - 1s 719ms/step - loss: 4.6468 - out_loss: 0.0017 - out_2_loss: 4.6450 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 508\n",
            "1/1 [==============================] - 1s 712ms/step - loss: 4.6096 - out_loss: 0.0031 - out_2_loss: 4.6065 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 509\n",
            "1/1 [==============================] - 1s 721ms/step - loss: 4.6784 - out_loss: 0.0028 - out_2_loss: 4.6756 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 510\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.4875 - out_loss: 0.0065 - out_2_loss: 4.4809 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 511\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.7584 - out_loss: 0.0312 - out_2_loss: 4.7272 - out_acc: 0.9857 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 512\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 4.3641 - out_loss: 0.0069 - out_2_loss: 4.3572 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 513\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.6677 - out_loss: 0.0031 - out_2_loss: 4.6647 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 514\n",
            "1/1 [==============================] - 1s 732ms/step - loss: 4.7376 - out_loss: 0.0048 - out_2_loss: 4.7328 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 515\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.3581 - out_loss: 0.0030 - out_2_loss: 4.3551 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 516\n",
            "1/1 [==============================] - 1s 727ms/step - loss: 4.4704 - out_loss: 0.0029 - out_2_loss: 4.4675 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 517\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.4617 - out_loss: 0.0021 - out_2_loss: 4.4596 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 518\n",
            "1/1 [==============================] - 1s 719ms/step - loss: 4.5963 - out_loss: 0.0034 - out_2_loss: 4.5929 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 519\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 4.6429 - out_loss: 0.0015 - out_2_loss: 4.6413 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 520\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.3292 - out_loss: 0.0025 - out_2_loss: 4.3266 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 521\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.5222 - out_loss: 0.0010 - out_2_loss: 4.5211 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 522\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.5338 - out_loss: 0.0031 - out_2_loss: 4.5307 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 523\n",
            "1/1 [==============================] - 1s 709ms/step - loss: 4.6406 - out_loss: 0.0020 - out_2_loss: 4.6386 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 524\n",
            "1/1 [==============================] - 1s 717ms/step - loss: 4.5789 - out_loss: 0.0078 - out_2_loss: 4.5711 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 525\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.4836 - out_loss: 0.0053 - out_2_loss: 4.4784 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 526\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.5292 - out_loss: 0.0033 - out_2_loss: 4.5259 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 527\n",
            "1/1 [==============================] - 1s 646ms/step - loss: 4.6662 - out_loss: 0.0028 - out_2_loss: 4.6634 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 528\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.6362 - out_loss: 0.0023 - out_2_loss: 4.6338 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 529\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 4.6345 - out_loss: 0.0035 - out_2_loss: 4.6310 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 530\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.5812 - out_loss: 0.0036 - out_2_loss: 4.5776 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 531\n",
            "1/1 [==============================] - 1s 718ms/step - loss: 4.6281 - out_loss: 0.0042 - out_2_loss: 4.6239 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 532\n",
            "1/1 [==============================] - 1s 718ms/step - loss: 4.4225 - out_loss: 0.0037 - out_2_loss: 4.4188 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 533\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.5923 - out_loss: 0.0044 - out_2_loss: 4.5879 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 534\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 4.6980 - out_loss: 0.0028 - out_2_loss: 4.6953 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 535\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.4873 - out_loss: 0.0021 - out_2_loss: 4.4852 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 536\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.4198 - out_loss: 0.0030 - out_2_loss: 4.4169 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 537\n",
            "1/1 [==============================] - 1s 712ms/step - loss: 4.5596 - out_loss: 0.0018 - out_2_loss: 4.5579 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 538\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 4.3750 - out_loss: 0.0017 - out_2_loss: 4.3733 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 539\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.5469 - out_loss: 0.0020 - out_2_loss: 4.5449 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 540\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.5132 - out_loss: 0.0021 - out_2_loss: 4.5111 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 541\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.6364 - out_loss: 0.0021 - out_2_loss: 4.6343 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 542\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.3872 - out_loss: 0.0011 - out_2_loss: 4.3861 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 543\n",
            "1/1 [==============================] - 1s 734ms/step - loss: 4.4608 - out_loss: 0.0016 - out_2_loss: 4.4593 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 544\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 4.5245 - out_loss: 0.0029 - out_2_loss: 4.5217 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 545\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.5652 - out_loss: 0.0067 - out_2_loss: 4.5585 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 546\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.4351 - out_loss: 0.0016 - out_2_loss: 4.4335 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 547\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 4.5386 - out_loss: 0.0029 - out_2_loss: 4.5357 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 548\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.4857 - out_loss: 0.0038 - out_2_loss: 4.4819 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 549\n",
            "1/1 [==============================] - 1s 703ms/step - loss: 4.3746 - out_loss: 0.0028 - out_2_loss: 4.3718 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 550\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.6341 - out_loss: 0.0019 - out_2_loss: 4.6321 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 551\n",
            "1/1 [==============================] - 1s 722ms/step - loss: 4.6341 - out_loss: 0.0035 - out_2_loss: 4.6306 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 552\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.6847 - out_loss: 0.0030 - out_2_loss: 4.6816 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.29256\n",
            "epochs: 553\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.1963 - out_loss: 0.0045 - out_2_loss: 4.1918 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 4.29256 to 4.19632, saving model to ./my_model.h5\n",
            "epochs: 554\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.5716 - out_loss: 0.0038 - out_2_loss: 4.5679 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 555\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 4.5668 - out_loss: 0.0038 - out_2_loss: 4.5631 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 556\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.7818 - out_loss: 0.0052 - out_2_loss: 4.7766 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 557\n",
            "1/1 [==============================] - 1s 722ms/step - loss: 4.4807 - out_loss: 0.0027 - out_2_loss: 4.4780 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 558\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.4204 - out_loss: 0.0044 - out_2_loss: 4.4159 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 559\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.4675 - out_loss: 0.0024 - out_2_loss: 4.4651 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 560\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 4.4342 - out_loss: 0.0047 - out_2_loss: 4.4295 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 561\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.3753 - out_loss: 0.0029 - out_2_loss: 4.3725 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 562\n",
            "1/1 [==============================] - 1s 709ms/step - loss: 4.4677 - out_loss: 0.0039 - out_2_loss: 4.4638 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 563\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.5826 - out_loss: 0.0039 - out_2_loss: 4.5786 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19632\n",
            "epochs: 564\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 4.1928 - out_loss: 0.0021 - out_2_loss: 4.1907 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss improved from 4.19632 to 4.19281, saving model to ./my_model.h5\n",
            "epochs: 565\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.4900 - out_loss: 0.0022 - out_2_loss: 4.4878 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 566\n",
            "1/1 [==============================] - 1s 710ms/step - loss: 4.4634 - out_loss: 0.0022 - out_2_loss: 4.4612 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 567\n",
            "1/1 [==============================] - 1s 714ms/step - loss: 4.3357 - out_loss: 0.0080 - out_2_loss: 4.3276 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 568\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.4338 - out_loss: 0.0051 - out_2_loss: 4.4287 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 569\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 4.3145 - out_loss: 0.0146 - out_2_loss: 4.2999 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 570\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.6566 - out_loss: 0.0220 - out_2_loss: 4.6346 - out_acc: 0.9857 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 571\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.5526 - out_loss: 0.0014 - out_2_loss: 4.5512 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 572\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.4971 - out_loss: 0.0032 - out_2_loss: 4.4939 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 573\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.7528 - out_loss: 0.0049 - out_2_loss: 4.7479 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 574\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.3004 - out_loss: 0.0019 - out_2_loss: 4.2985 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 575\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.4987 - out_loss: 0.0015 - out_2_loss: 4.4973 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 576\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.3810 - out_loss: 0.0020 - out_2_loss: 4.3791 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 577\n",
            "1/1 [==============================] - 1s 725ms/step - loss: 4.3080 - out_loss: 0.0031 - out_2_loss: 4.3049 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 578\n",
            "1/1 [==============================] - 1s 749ms/step - loss: 4.5885 - out_loss: 0.0029 - out_2_loss: 4.5856 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 579\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.4380 - out_loss: 0.0015 - out_2_loss: 4.4365 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 580\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.4474 - out_loss: 0.0030 - out_2_loss: 4.4444 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 581\n",
            "1/1 [==============================] - 1s 708ms/step - loss: 4.3883 - out_loss: 0.0015 - out_2_loss: 4.3867 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 582\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 4.2690 - out_loss: 0.0017 - out_2_loss: 4.2673 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 583\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.3757 - out_loss: 0.0030 - out_2_loss: 4.3726 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 584\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 4.5900 - out_loss: 0.0021 - out_2_loss: 4.5879 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 585\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.3832 - out_loss: 0.0024 - out_2_loss: 4.3808 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 586\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 4.5019 - out_loss: 0.0026 - out_2_loss: 4.4993 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 587\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.4356 - out_loss: 0.0034 - out_2_loss: 4.4322 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 588\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.3347 - out_loss: 0.0019 - out_2_loss: 4.3328 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 589\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.6186 - out_loss: 0.0043 - out_2_loss: 4.6143 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 590\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.4268 - out_loss: 0.0034 - out_2_loss: 4.4234 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 591\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 4.4709 - out_loss: 0.0024 - out_2_loss: 4.4685 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 592\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.2466 - out_loss: 0.0024 - out_2_loss: 4.2443 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 593\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.4866 - out_loss: 0.0028 - out_2_loss: 4.4837 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 594\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.6225 - out_loss: 0.0014 - out_2_loss: 4.6211 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 595\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.4335 - out_loss: 0.0020 - out_2_loss: 4.4316 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 596\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.4206 - out_loss: 0.0020 - out_2_loss: 4.4186 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 597\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.4533 - out_loss: 0.0030 - out_2_loss: 4.4503 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 598\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 4.3723 - out_loss: 0.0147 - out_2_loss: 4.3575 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 599\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.4624 - out_loss: 0.0031 - out_2_loss: 4.4593 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 600\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 4.5197 - out_loss: 0.0016 - out_2_loss: 4.5182 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 601\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.2527 - out_loss: 0.0023 - out_2_loss: 4.2504 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 602\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 4.5068 - out_loss: 0.0016 - out_2_loss: 4.5052 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 603\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.3929 - out_loss: 0.0029 - out_2_loss: 4.3900 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 604\n",
            "1/1 [==============================] - 1s 728ms/step - loss: 4.4752 - out_loss: 0.0012 - out_2_loss: 4.4741 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.19281\n",
            "epochs: 605\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.1776 - out_loss: 0.0022 - out_2_loss: 4.1754 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss improved from 4.19281 to 4.17762, saving model to ./my_model.h5\n",
            "epochs: 606\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 4.3908 - out_loss: 0.0023 - out_2_loss: 4.3884 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.17762\n",
            "epochs: 607\n",
            "1/1 [==============================] - 1s 708ms/step - loss: 4.3603 - out_loss: 0.0026 - out_2_loss: 4.3577 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.17762\n",
            "epochs: 608\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.2753 - out_loss: 0.0021 - out_2_loss: 4.2731 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.17762\n",
            "epochs: 609\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.2595 - out_loss: 0.0012 - out_2_loss: 4.2584 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.17762\n",
            "epochs: 610\n",
            "1/1 [==============================] - 1s 748ms/step - loss: 4.3431 - out_loss: 0.0022 - out_2_loss: 4.3408 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.17762\n",
            "epochs: 611\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 4.2938 - out_loss: 0.0010 - out_2_loss: 4.2927 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.17762\n",
            "epochs: 612\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 4.3799 - out_loss: 0.0021 - out_2_loss: 4.3779 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.17762\n",
            "epochs: 613\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.2659 - out_loss: 0.0040 - out_2_loss: 4.2618 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.17762\n",
            "epochs: 614\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.1215 - out_loss: 0.0026 - out_2_loss: 4.1189 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss improved from 4.17762 to 4.12148, saving model to ./my_model.h5\n",
            "epochs: 615\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 4.4143 - out_loss: 0.0024 - out_2_loss: 4.4119 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 616\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.5215 - out_loss: 0.0031 - out_2_loss: 4.5184 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 617\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.6193 - out_loss: 0.0025 - out_2_loss: 4.6168 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 618\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.3507 - out_loss: 0.0022 - out_2_loss: 4.3485 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 619\n",
            "1/1 [==============================] - 1s 714ms/step - loss: 4.3411 - out_loss: 0.0036 - out_2_loss: 4.3376 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 620\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.3916 - out_loss: 0.0027 - out_2_loss: 4.3889 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 621\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 4.6920 - out_loss: 0.0052 - out_2_loss: 4.6869 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 622\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 4.4843 - out_loss: 0.0022 - out_2_loss: 4.4822 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 623\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.4037 - out_loss: 0.0020 - out_2_loss: 4.4017 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 624\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.1457 - out_loss: 0.0013 - out_2_loss: 4.1444 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 625\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.3295 - out_loss: 0.0012 - out_2_loss: 4.3283 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 626\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 4.4210 - out_loss: 0.0014 - out_2_loss: 4.4196 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 627\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 4.3381 - out_loss: 0.0016 - out_2_loss: 4.3366 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 628\n",
            "1/1 [==============================] - 1s 645ms/step - loss: 4.3767 - out_loss: 0.0115 - out_2_loss: 4.3652 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 629\n",
            "1/1 [==============================] - 1s 717ms/step - loss: 4.2681 - out_loss: 0.0022 - out_2_loss: 4.2659 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 630\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 4.4126 - out_loss: 0.0012 - out_2_loss: 4.4114 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 631\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 4.3272 - out_loss: 0.0015 - out_2_loss: 4.3257 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 632\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.5575 - out_loss: 9.9510e-04 - out_2_loss: 4.5565 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 633\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.2737 - out_loss: 0.0017 - out_2_loss: 4.2720 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 634\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.6311 - out_loss: 0.0031 - out_2_loss: 4.6280 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 635\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.3999 - out_loss: 0.0048 - out_2_loss: 4.3951 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 636\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.5598 - out_loss: 0.0020 - out_2_loss: 4.5578 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 637\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 4.6106 - out_loss: 0.0041 - out_2_loss: 4.6065 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 638\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.4376 - out_loss: 0.0023 - out_2_loss: 4.4352 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 639\n",
            "1/1 [==============================] - 1s 741ms/step - loss: 4.5185 - out_loss: 0.0027 - out_2_loss: 4.5158 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 640\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.2321 - out_loss: 0.0028 - out_2_loss: 4.2293 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 641\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 4.5165 - out_loss: 0.0015 - out_2_loss: 4.5150 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 642\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.2180 - out_loss: 0.0018 - out_2_loss: 4.2163 - out_acc: 1.0000 - out_2_acc: 0.1286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 643\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.4349 - out_loss: 0.0016 - out_2_loss: 4.4333 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 644\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 4.4424 - out_loss: 0.0019 - out_2_loss: 4.4405 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 645\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 4.1571 - out_loss: 0.0014 - out_2_loss: 4.1557 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 646\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 4.4428 - out_loss: 0.0016 - out_2_loss: 4.4412 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.12148\n",
            "epochs: 647\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 4.0998 - out_loss: 0.0026 - out_2_loss: 4.0972 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss improved from 4.12148 to 4.09978, saving model to ./my_model.h5\n",
            "epochs: 648\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 4.1777 - out_loss: 0.0013 - out_2_loss: 4.1765 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 649\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 4.3339 - out_loss: 0.0048 - out_2_loss: 4.3291 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 650\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.4770 - out_loss: 0.0019 - out_2_loss: 4.4751 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 651\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 4.3245 - out_loss: 0.0020 - out_2_loss: 4.3225 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 652\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.4094 - out_loss: 0.0042 - out_2_loss: 4.4052 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 653\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.3161 - out_loss: 0.0010 - out_2_loss: 4.3151 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 654\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.2822 - out_loss: 0.0017 - out_2_loss: 4.2804 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 655\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.1819 - out_loss: 0.0023 - out_2_loss: 4.1796 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 656\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 4.1038 - out_loss: 0.0012 - out_2_loss: 4.1025 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 657\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 4.5610 - out_loss: 0.0012 - out_2_loss: 4.5598 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 658\n",
            "1/1 [==============================] - 1s 652ms/step - loss: 4.4298 - out_loss: 0.0018 - out_2_loss: 4.4280 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 659\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.4272 - out_loss: 0.0019 - out_2_loss: 4.4252 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 660\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 4.2786 - out_loss: 0.0016 - out_2_loss: 4.2769 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 661\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 4.5176 - out_loss: 0.0028 - out_2_loss: 4.5148 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 662\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 4.2836 - out_loss: 0.0013 - out_2_loss: 4.2823 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 663\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 4.3947 - out_loss: 0.0020 - out_2_loss: 4.3927 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 664\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 4.1891 - out_loss: 0.0015 - out_2_loss: 4.1876 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 665\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 4.3462 - out_loss: 0.0016 - out_2_loss: 4.3446 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 666\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 4.3652 - out_loss: 0.0029 - out_2_loss: 4.3623 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 667\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.2714 - out_loss: 0.0056 - out_2_loss: 4.2658 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 668\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.4566 - out_loss: 0.0118 - out_2_loss: 4.4448 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 669\n",
            "1/1 [==============================] - 1s 723ms/step - loss: 4.6903 - out_loss: 0.0017 - out_2_loss: 4.6887 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 670\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 4.2688 - out_loss: 0.0185 - out_2_loss: 4.2502 - out_acc: 0.9857 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 671\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.2745 - out_loss: 0.0181 - out_2_loss: 4.2564 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 672\n",
            "1/1 [==============================] - 1s 719ms/step - loss: 4.5278 - out_loss: 0.0227 - out_2_loss: 4.5050 - out_acc: 0.9857 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 673\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 4.2854 - out_loss: 0.0015 - out_2_loss: 4.2839 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 674\n",
            "1/1 [==============================] - 1s 703ms/step - loss: 4.4078 - out_loss: 0.0022 - out_2_loss: 4.4056 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 675\n",
            "1/1 [==============================] - 1s 724ms/step - loss: 4.4206 - out_loss: 0.0047 - out_2_loss: 4.4159 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 676\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.4451 - out_loss: 0.0085 - out_2_loss: 4.4367 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 677\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 4.5918 - out_loss: 0.0052 - out_2_loss: 4.5866 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 678\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.4226 - out_loss: 0.0087 - out_2_loss: 4.4139 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 679\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 4.4383 - out_loss: 0.0061 - out_2_loss: 4.4322 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 680\n",
            "1/1 [==============================] - 1s 727ms/step - loss: 4.4128 - out_loss: 0.0428 - out_2_loss: 4.3700 - out_acc: 0.9857 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 681\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.4614 - out_loss: 0.0103 - out_2_loss: 4.4511 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 682\n",
            "1/1 [==============================] - 1s 729ms/step - loss: 4.2239 - out_loss: 0.0026 - out_2_loss: 4.2213 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 683\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 4.4645 - out_loss: 0.0034 - out_2_loss: 4.4612 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 684\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.3229 - out_loss: 0.0038 - out_2_loss: 4.3191 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 685\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.3173 - out_loss: 0.0026 - out_2_loss: 4.3147 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 686\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 4.3858 - out_loss: 0.0018 - out_2_loss: 4.3840 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 687\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.3145 - out_loss: 0.0027 - out_2_loss: 4.3117 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 688\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 4.3691 - out_loss: 0.0022 - out_2_loss: 4.3669 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 689\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 4.4143 - out_loss: 0.0047 - out_2_loss: 4.4096 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 690\n",
            "1/1 [==============================] - 1s 714ms/step - loss: 4.5441 - out_loss: 0.0048 - out_2_loss: 4.5393 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 691\n",
            "1/1 [==============================] - 1s 728ms/step - loss: 4.2598 - out_loss: 0.0066 - out_2_loss: 4.2532 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 692\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 4.3876 - out_loss: 0.0022 - out_2_loss: 4.3854 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 693\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.2976 - out_loss: 0.0036 - out_2_loss: 4.2939 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 694\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 4.3475 - out_loss: 0.0042 - out_2_loss: 4.3433 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 695\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.4706 - out_loss: 0.0014 - out_2_loss: 4.4692 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 696\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 4.2296 - out_loss: 0.0017 - out_2_loss: 4.2279 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.09978\n",
            "epochs: 697\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 4.0141 - out_loss: 0.0023 - out_2_loss: 4.0118 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 4.09978 to 4.01406, saving model to ./my_model.h5\n",
            "epochs: 698\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.3387 - out_loss: 0.0020 - out_2_loss: 4.3368 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 699\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.2702 - out_loss: 0.0024 - out_2_loss: 4.2678 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 700\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 4.0936 - out_loss: 0.0014 - out_2_loss: 4.0922 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 701\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 4.3677 - out_loss: 0.0018 - out_2_loss: 4.3658 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 702\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 4.3673 - out_loss: 0.0027 - out_2_loss: 4.3646 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 703\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 4.1228 - out_loss: 0.0015 - out_2_loss: 4.1214 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 704\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.4276 - out_loss: 0.0039 - out_2_loss: 4.4237 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 705\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.1828 - out_loss: 0.0046 - out_2_loss: 4.1782 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 706\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 4.2526 - out_loss: 0.0021 - out_2_loss: 4.2504 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 707\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 4.2354 - out_loss: 9.9775e-04 - out_2_loss: 4.2344 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 708\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 4.1899 - out_loss: 0.0016 - out_2_loss: 4.1883 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 709\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 4.2871 - out_loss: 0.0011 - out_2_loss: 4.2859 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 710\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 4.2912 - out_loss: 0.0059 - out_2_loss: 4.2853 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 711\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.3365 - out_loss: 0.0012 - out_2_loss: 4.3353 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 712\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.3121 - out_loss: 8.2082e-04 - out_2_loss: 4.3113 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 713\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.0861 - out_loss: 0.0018 - out_2_loss: 4.0843 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 714\n",
            "1/1 [==============================] - 1s 638ms/step - loss: 4.2064 - out_loss: 0.0011 - out_2_loss: 4.2053 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 715\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 4.1439 - out_loss: 0.0017 - out_2_loss: 4.1422 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 716\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 4.2723 - out_loss: 0.0015 - out_2_loss: 4.2708 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 717\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.2154 - out_loss: 0.0041 - out_2_loss: 4.2113 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 718\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 4.3340 - out_loss: 0.0018 - out_2_loss: 4.3322 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 719\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 4.2608 - out_loss: 0.0030 - out_2_loss: 4.2578 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 720\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 4.3636 - out_loss: 0.0022 - out_2_loss: 4.3614 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 721\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.1502 - out_loss: 0.0050 - out_2_loss: 4.1452 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 722\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.3257 - out_loss: 0.0124 - out_2_loss: 4.3133 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 723\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.3899 - out_loss: 0.0054 - out_2_loss: 4.3845 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 724\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.2465 - out_loss: 0.0060 - out_2_loss: 4.2405 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 725\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 4.3693 - out_loss: 0.0023 - out_2_loss: 4.3670 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.01406\n",
            "epochs: 726\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 4.0091 - out_loss: 0.0022 - out_2_loss: 4.0069 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss improved from 4.01406 to 4.00910, saving model to ./my_model.h5\n",
            "epochs: 727\n",
            "1/1 [==============================] - 1s 717ms/step - loss: 4.1717 - out_loss: 0.0023 - out_2_loss: 4.1694 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 728\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 4.1902 - out_loss: 0.0031 - out_2_loss: 4.1871 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 729\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 4.1835 - out_loss: 0.0015 - out_2_loss: 4.1820 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 730\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.3034 - out_loss: 0.0023 - out_2_loss: 4.3011 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 731\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 4.2403 - out_loss: 0.0015 - out_2_loss: 4.2388 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 732\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 4.2951 - out_loss: 0.0015 - out_2_loss: 4.2936 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 733\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.2156 - out_loss: 9.9041e-04 - out_2_loss: 4.2146 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 734\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 4.3006 - out_loss: 0.0014 - out_2_loss: 4.2992 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 735\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.3243 - out_loss: 7.7325e-04 - out_2_loss: 4.3235 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 736\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.2671 - out_loss: 0.0017 - out_2_loss: 4.2654 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 737\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.3491 - out_loss: 0.0016 - out_2_loss: 4.3475 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 738\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.2900 - out_loss: 7.3751e-04 - out_2_loss: 4.2892 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 739\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.2659 - out_loss: 0.0011 - out_2_loss: 4.2648 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 740\n",
            "1/1 [==============================] - 1s 709ms/step - loss: 4.0693 - out_loss: 0.0011 - out_2_loss: 4.0682 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 741\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.0296 - out_loss: 8.7329e-04 - out_2_loss: 4.0287 - out_acc: 1.0000 - out_2_acc: 0.1286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 742\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.1560 - out_loss: 0.0016 - out_2_loss: 4.1545 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 4.00910\n",
            "epochs: 743\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.0004 - out_loss: 6.9518e-04 - out_2_loss: 3.9997 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss improved from 4.00910 to 4.00043, saving model to ./my_model.h5\n",
            "epochs: 744\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 3.9529 - out_loss: 0.0012 - out_2_loss: 3.9518 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss improved from 4.00043 to 3.95293, saving model to ./my_model.h5\n",
            "epochs: 745\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.3469 - out_loss: 0.0013 - out_2_loss: 4.3455 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 746\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 3.9864 - out_loss: 0.0012 - out_2_loss: 3.9852 - out_acc: 1.0000 - out_2_acc: 0.1286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 747\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.3278 - out_loss: 0.0016 - out_2_loss: 4.3262 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 748\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.2234 - out_loss: 0.0011 - out_2_loss: 4.2222 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 749\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.3357 - out_loss: 0.0016 - out_2_loss: 4.3341 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 750\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 4.0084 - out_loss: 0.0011 - out_2_loss: 4.0072 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 751\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.2120 - out_loss: 0.0013 - out_2_loss: 4.2107 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 752\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.3408 - out_loss: 0.0018 - out_2_loss: 4.3390 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 753\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 4.3149 - out_loss: 0.0010 - out_2_loss: 4.3139 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 754\n",
            "1/1 [==============================] - 1s 703ms/step - loss: 4.1598 - out_loss: 0.0016 - out_2_loss: 4.1582 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 755\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.0786 - out_loss: 0.0012 - out_2_loss: 4.0774 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 756\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.2275 - out_loss: 0.0019 - out_2_loss: 4.2256 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 757\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 4.3326 - out_loss: 0.0019 - out_2_loss: 4.3307 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 758\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 4.1994 - out_loss: 0.0032 - out_2_loss: 4.1963 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 759\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 4.1450 - out_loss: 0.0025 - out_2_loss: 4.1425 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 760\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 4.2095 - out_loss: 0.0014 - out_2_loss: 4.2081 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 761\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 4.1581 - out_loss: 0.0027 - out_2_loss: 4.1555 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 762\n",
            "1/1 [==============================] - 1s 719ms/step - loss: 4.1466 - out_loss: 0.0023 - out_2_loss: 4.1443 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 763\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.3966 - out_loss: 0.0014 - out_2_loss: 4.3952 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 764\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.1961 - out_loss: 0.0014 - out_2_loss: 4.1947 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 765\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.2521 - out_loss: 0.0018 - out_2_loss: 4.2503 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 766\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 4.0152 - out_loss: 0.0020 - out_2_loss: 4.0132 - out_acc: 1.0000 - out_2_acc: 0.1286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 767\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 4.0729 - out_loss: 0.0018 - out_2_loss: 4.0711 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 768\n",
            "1/1 [==============================] - 1s 709ms/step - loss: 4.1254 - out_loss: 0.0011 - out_2_loss: 4.1242 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 769\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 4.0442 - out_loss: 0.0013 - out_2_loss: 4.0429 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.95293\n",
            "epochs: 770\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 3.8914 - out_loss: 8.7607e-04 - out_2_loss: 3.8905 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss improved from 3.95293 to 3.89135, saving model to ./my_model.h5\n",
            "epochs: 771\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 4.1122 - out_loss: 0.0010 - out_2_loss: 4.1112 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 772\n",
            "1/1 [==============================] - 1s 645ms/step - loss: 4.2668 - out_loss: 0.0010 - out_2_loss: 4.2658 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 773\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.0728 - out_loss: 0.0018 - out_2_loss: 4.0710 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 774\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.1832 - out_loss: 0.0031 - out_2_loss: 4.1801 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 775\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.2435 - out_loss: 0.0014 - out_2_loss: 4.2421 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 776\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 4.0240 - out_loss: 0.0032 - out_2_loss: 4.0208 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 777\n",
            "1/1 [==============================] - 1s 723ms/step - loss: 4.4753 - out_loss: 0.0015 - out_2_loss: 4.4738 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 778\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 4.2770 - out_loss: 0.0014 - out_2_loss: 4.2756 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 779\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.2548 - out_loss: 0.0015 - out_2_loss: 4.2533 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 780\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.4195 - out_loss: 0.0010 - out_2_loss: 4.4184 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 781\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.4762 - out_loss: 0.0019 - out_2_loss: 4.4742 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 782\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.3437 - out_loss: 0.0011 - out_2_loss: 4.3426 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 783\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 4.0464 - out_loss: 0.0023 - out_2_loss: 4.0441 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 784\n",
            "1/1 [==============================] - 1s 723ms/step - loss: 4.2451 - out_loss: 9.0662e-04 - out_2_loss: 4.2442 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 785\n",
            "1/1 [==============================] - 1s 720ms/step - loss: 4.2558 - out_loss: 0.0014 - out_2_loss: 4.2545 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 786\n",
            "1/1 [==============================] - 1s 749ms/step - loss: 4.1436 - out_loss: 0.0016 - out_2_loss: 4.1420 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 787\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 4.0445 - out_loss: 0.0012 - out_2_loss: 4.0433 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 788\n",
            "1/1 [==============================] - 1s 722ms/step - loss: 4.2700 - out_loss: 0.0011 - out_2_loss: 4.2689 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 789\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.1475 - out_loss: 0.0014 - out_2_loss: 4.1461 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 790\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.2080 - out_loss: 0.0013 - out_2_loss: 4.2067 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 791\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.0851 - out_loss: 0.0021 - out_2_loss: 4.0830 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 792\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 4.1636 - out_loss: 0.0021 - out_2_loss: 4.1615 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 793\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.1154 - out_loss: 0.0032 - out_2_loss: 4.1122 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 794\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.2889 - out_loss: 0.0012 - out_2_loss: 4.2877 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 795\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.2488 - out_loss: 9.8062e-04 - out_2_loss: 4.2478 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 796\n",
            "1/1 [==============================] - 1s 721ms/step - loss: 4.1507 - out_loss: 0.0038 - out_2_loss: 4.1469 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 797\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.1212 - out_loss: 0.0015 - out_2_loss: 4.1196 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 798\n",
            "1/1 [==============================] - 1s 719ms/step - loss: 4.0819 - out_loss: 0.0039 - out_2_loss: 4.0780 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 799\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.1962 - out_loss: 0.0011 - out_2_loss: 4.1950 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 800\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 4.3730 - out_loss: 0.0011 - out_2_loss: 4.3719 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 801\n",
            "1/1 [==============================] - 1s 636ms/step - loss: 4.1427 - out_loss: 0.0013 - out_2_loss: 4.1414 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 802\n",
            "1/1 [==============================] - 1s 644ms/step - loss: 4.2708 - out_loss: 0.0012 - out_2_loss: 4.2696 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 803\n",
            "1/1 [==============================] - 1s 644ms/step - loss: 4.3213 - out_loss: 0.0017 - out_2_loss: 4.3196 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 804\n",
            "1/1 [==============================] - 1s 717ms/step - loss: 4.1105 - out_loss: 0.0012 - out_2_loss: 4.1092 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 805\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.2994 - out_loss: 7.9854e-04 - out_2_loss: 4.2986 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 806\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.1048 - out_loss: 0.0013 - out_2_loss: 4.1035 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 807\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 4.2833 - out_loss: 8.9709e-04 - out_2_loss: 4.2824 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 808\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.2116 - out_loss: 0.0013 - out_2_loss: 4.2103 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 809\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 4.1573 - out_loss: 0.0014 - out_2_loss: 4.1559 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 810\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.2985 - out_loss: 7.3513e-04 - out_2_loss: 4.2977 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 811\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 3.9762 - out_loss: 0.0011 - out_2_loss: 3.9751 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 812\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.3207 - out_loss: 0.0015 - out_2_loss: 4.3192 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 813\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.2237 - out_loss: 0.0012 - out_2_loss: 4.2224 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 814\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.0025 - out_loss: 0.0013 - out_2_loss: 4.0012 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 815\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 4.2706 - out_loss: 0.0018 - out_2_loss: 4.2688 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 816\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.2176 - out_loss: 6.9802e-04 - out_2_loss: 4.2169 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 817\n",
            "1/1 [==============================] - 1s 725ms/step - loss: 4.1725 - out_loss: 8.9763e-04 - out_2_loss: 4.1716 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 818\n",
            "1/1 [==============================] - 1s 740ms/step - loss: 4.2936 - out_loss: 7.9381e-04 - out_2_loss: 4.2929 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 819\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 4.0349 - out_loss: 0.0012 - out_2_loss: 4.0337 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 820\n",
            "1/1 [==============================] - 1s 650ms/step - loss: 4.0620 - out_loss: 0.0012 - out_2_loss: 4.0607 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 821\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.1823 - out_loss: 8.7119e-04 - out_2_loss: 4.1814 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 822\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 4.0811 - out_loss: 0.0012 - out_2_loss: 4.0798 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 823\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 4.1387 - out_loss: 7.6157e-04 - out_2_loss: 4.1380 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 824\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.2509 - out_loss: 0.0012 - out_2_loss: 4.2497 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 825\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.0528 - out_loss: 0.0016 - out_2_loss: 4.0512 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 826\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 4.0993 - out_loss: 0.0024 - out_2_loss: 4.0969 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 827\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 4.1150 - out_loss: 8.2122e-04 - out_2_loss: 4.1141 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 828\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.1235 - out_loss: 0.0010 - out_2_loss: 4.1225 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 829\n",
            "1/1 [==============================] - 1s 716ms/step - loss: 3.9337 - out_loss: 0.0010 - out_2_loss: 3.9327 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 830\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.2006 - out_loss: 0.0011 - out_2_loss: 4.1995 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 831\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 4.2414 - out_loss: 0.0014 - out_2_loss: 4.2399 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 832\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 4.1132 - out_loss: 0.0010 - out_2_loss: 4.1121 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 833\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.2599 - out_loss: 0.0027 - out_2_loss: 4.2573 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 834\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.0037 - out_loss: 0.0018 - out_2_loss: 4.0019 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 835\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 3.9386 - out_loss: 0.0016 - out_2_loss: 3.9370 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 836\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.1607 - out_loss: 0.0018 - out_2_loss: 4.1588 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 837\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 4.2879 - out_loss: 0.0011 - out_2_loss: 4.2867 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 838\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 4.0779 - out_loss: 0.0012 - out_2_loss: 4.0767 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 839\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 4.1885 - out_loss: 0.0018 - out_2_loss: 4.1867 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 840\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.2311 - out_loss: 0.0019 - out_2_loss: 4.2292 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 841\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.1103 - out_loss: 0.0018 - out_2_loss: 4.1084 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 842\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.2621 - out_loss: 0.0020 - out_2_loss: 4.2601 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 843\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 3.9843 - out_loss: 0.0017 - out_2_loss: 3.9826 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 844\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 4.0082 - out_loss: 0.0012 - out_2_loss: 4.0070 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 845\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.1859 - out_loss: 0.0015 - out_2_loss: 4.1844 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 846\n",
            "1/1 [==============================] - 1s 720ms/step - loss: 4.0428 - out_loss: 0.0024 - out_2_loss: 4.0404 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 847\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 4.0113 - out_loss: 0.0012 - out_2_loss: 4.0101 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 848\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 3.9679 - out_loss: 0.0026 - out_2_loss: 3.9654 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 849\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.2930 - out_loss: 0.0014 - out_2_loss: 4.2916 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 850\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 4.2233 - out_loss: 0.0018 - out_2_loss: 4.2215 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 851\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.2140 - out_loss: 0.0039 - out_2_loss: 4.2101 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 852\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 4.1587 - out_loss: 0.0013 - out_2_loss: 4.1573 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 853\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 4.2344 - out_loss: 0.0039 - out_2_loss: 4.2305 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 854\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 4.0455 - out_loss: 0.0014 - out_2_loss: 4.0440 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 855\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 4.3041 - out_loss: 0.0011 - out_2_loss: 4.3031 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 856\n",
            "1/1 [==============================] - 1s 739ms/step - loss: 4.2589 - out_loss: 0.0017 - out_2_loss: 4.2572 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 857\n",
            "1/1 [==============================] - 1s 703ms/step - loss: 4.0722 - out_loss: 0.0019 - out_2_loss: 4.0703 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 858\n",
            "1/1 [==============================] - 1s 742ms/step - loss: 4.1130 - out_loss: 0.0016 - out_2_loss: 4.1115 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 859\n",
            "1/1 [==============================] - 1s 719ms/step - loss: 4.0609 - out_loss: 0.0021 - out_2_loss: 4.0588 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 860\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.1499 - out_loss: 9.1226e-04 - out_2_loss: 4.1490 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 861\n",
            "1/1 [==============================] - 1s 723ms/step - loss: 4.0976 - out_loss: 0.0018 - out_2_loss: 4.0958 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 862\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 4.0997 - out_loss: 0.0016 - out_2_loss: 4.0980 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 863\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 3.9319 - out_loss: 0.0010 - out_2_loss: 3.9309 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 864\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 4.1961 - out_loss: 0.0011 - out_2_loss: 4.1950 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 865\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 3.9229 - out_loss: 0.0011 - out_2_loss: 3.9218 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 866\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.3035 - out_loss: 0.0011 - out_2_loss: 4.3024 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 867\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 4.1715 - out_loss: 0.0014 - out_2_loss: 4.1701 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 868\n",
            "1/1 [==============================] - 1s 697ms/step - loss: 4.1774 - out_loss: 0.0038 - out_2_loss: 4.1736 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 869\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 4.1991 - out_loss: 0.0071 - out_2_loss: 4.1920 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 870\n",
            "1/1 [==============================] - 1s 724ms/step - loss: 4.1038 - out_loss: 0.0013 - out_2_loss: 4.1025 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 871\n",
            "1/1 [==============================] - 1s 744ms/step - loss: 4.1209 - out_loss: 0.0015 - out_2_loss: 4.1194 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 872\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.1042 - out_loss: 0.0015 - out_2_loss: 4.1027 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 873\n",
            "1/1 [==============================] - 1s 703ms/step - loss: 4.2733 - out_loss: 0.0014 - out_2_loss: 4.2720 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 874\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 4.1351 - out_loss: 0.0013 - out_2_loss: 4.1337 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 875\n",
            "1/1 [==============================] - 1s 732ms/step - loss: 4.0351 - out_loss: 0.0013 - out_2_loss: 4.0339 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 876\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.2511 - out_loss: 0.0011 - out_2_loss: 4.2500 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 877\n",
            "1/1 [==============================] - 1s 677ms/step - loss: 4.2838 - out_loss: 8.4590e-04 - out_2_loss: 4.2829 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 878\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 4.2199 - out_loss: 0.0013 - out_2_loss: 4.2187 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 879\n",
            "1/1 [==============================] - 1s 701ms/step - loss: 4.1903 - out_loss: 0.0014 - out_2_loss: 4.1889 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 880\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 3.9407 - out_loss: 0.0019 - out_2_loss: 3.9389 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 881\n",
            "1/1 [==============================] - 1s 684ms/step - loss: 4.0906 - out_loss: 8.0849e-04 - out_2_loss: 4.0898 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 882\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 4.1357 - out_loss: 5.2636e-04 - out_2_loss: 4.1351 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 883\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 4.1681 - out_loss: 0.0015 - out_2_loss: 4.1666 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 884\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 4.2525 - out_loss: 0.0014 - out_2_loss: 4.2510 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 885\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 4.2287 - out_loss: 0.0019 - out_2_loss: 4.2268 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 886\n",
            "1/1 [==============================] - 1s 745ms/step - loss: 4.0324 - out_loss: 0.0010 - out_2_loss: 4.0314 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 887\n",
            "1/1 [==============================] - 1s 722ms/step - loss: 4.1153 - out_loss: 0.0020 - out_2_loss: 4.1133 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 888\n",
            "1/1 [==============================] - 1s 719ms/step - loss: 4.1429 - out_loss: 0.0017 - out_2_loss: 4.1412 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 889\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 4.1636 - out_loss: 0.0019 - out_2_loss: 4.1618 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 890\n",
            "1/1 [==============================] - 1s 709ms/step - loss: 3.9882 - out_loss: 0.0020 - out_2_loss: 3.9862 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 891\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 4.0874 - out_loss: 0.0015 - out_2_loss: 4.0859 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 892\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 3.9594 - out_loss: 0.0014 - out_2_loss: 3.9580 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 893\n",
            "1/1 [==============================] - 1s 708ms/step - loss: 4.0577 - out_loss: 0.0025 - out_2_loss: 4.0551 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 894\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.0841 - out_loss: 0.0013 - out_2_loss: 4.0828 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 895\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 4.0947 - out_loss: 8.3812e-04 - out_2_loss: 4.0938 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 896\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.3814 - out_loss: 7.2311e-04 - out_2_loss: 4.3807 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 897\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 4.0971 - out_loss: 7.6094e-04 - out_2_loss: 4.0964 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 898\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 3.9880 - out_loss: 0.0023 - out_2_loss: 3.9856 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 899\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 4.1429 - out_loss: 0.0010 - out_2_loss: 4.1418 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 900\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.0194 - out_loss: 0.0015 - out_2_loss: 4.0179 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 901\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.0234 - out_loss: 0.0011 - out_2_loss: 4.0223 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 902\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.1301 - out_loss: 0.0012 - out_2_loss: 4.1289 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 903\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 4.1370 - out_loss: 6.9532e-04 - out_2_loss: 4.1363 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 904\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 4.0107 - out_loss: 0.0013 - out_2_loss: 4.0094 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 905\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.2551 - out_loss: 0.0015 - out_2_loss: 4.2536 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 906\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.1718 - out_loss: 0.0079 - out_2_loss: 4.1640 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 907\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 4.1360 - out_loss: 0.0015 - out_2_loss: 4.1345 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 908\n",
            "1/1 [==============================] - 1s 738ms/step - loss: 4.2511 - out_loss: 9.5258e-04 - out_2_loss: 4.2501 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 909\n",
            "1/1 [==============================] - 1s 681ms/step - loss: 4.1143 - out_loss: 0.0011 - out_2_loss: 4.1133 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 910\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 4.0436 - out_loss: 0.0016 - out_2_loss: 4.0420 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.89135\n",
            "epochs: 911\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 3.8195 - out_loss: 0.0048 - out_2_loss: 3.8147 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss improved from 3.89135 to 3.81948, saving model to ./my_model.h5\n",
            "epochs: 912\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.0426 - out_loss: 0.0030 - out_2_loss: 4.0396 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 913\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 4.0585 - out_loss: 0.0025 - out_2_loss: 4.0560 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 914\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 4.1724 - out_loss: 0.0040 - out_2_loss: 4.1685 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 915\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 4.1341 - out_loss: 0.0025 - out_2_loss: 4.1316 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 916\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 4.0372 - out_loss: 0.0055 - out_2_loss: 4.0317 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 917\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.1140 - out_loss: 0.0031 - out_2_loss: 4.1108 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 918\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 3.9038 - out_loss: 0.0015 - out_2_loss: 3.9023 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 919\n",
            "1/1 [==============================] - 1s 685ms/step - loss: 4.1734 - out_loss: 0.0016 - out_2_loss: 4.1718 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 920\n",
            "1/1 [==============================] - 1s 675ms/step - loss: 4.0118 - out_loss: 8.7736e-04 - out_2_loss: 4.0110 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 921\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 4.1921 - out_loss: 0.0013 - out_2_loss: 4.1908 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 922\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 4.1695 - out_loss: 0.0028 - out_2_loss: 4.1667 - out_acc: 1.0000 - out_2_acc: 0.1286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 923\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 4.0461 - out_loss: 0.0011 - out_2_loss: 4.0449 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 924\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.2395 - out_loss: 0.0012 - out_2_loss: 4.2383 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 925\n",
            "1/1 [==============================] - 1s 718ms/step - loss: 3.9864 - out_loss: 8.9812e-04 - out_2_loss: 3.9855 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 926\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 3.9593 - out_loss: 0.0013 - out_2_loss: 3.9580 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 927\n",
            "1/1 [==============================] - 1s 704ms/step - loss: 4.0931 - out_loss: 0.0012 - out_2_loss: 4.0919 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 928\n",
            "1/1 [==============================] - 1s 725ms/step - loss: 4.1988 - out_loss: 0.0013 - out_2_loss: 4.1975 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 929\n",
            "1/1 [==============================] - 1s 728ms/step - loss: 3.9808 - out_loss: 0.0014 - out_2_loss: 3.9794 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 930\n",
            "1/1 [==============================] - 1s 725ms/step - loss: 4.1011 - out_loss: 6.6159e-04 - out_2_loss: 4.1005 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 931\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.2333 - out_loss: 5.9735e-04 - out_2_loss: 4.2327 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 932\n",
            "1/1 [==============================] - 1s 680ms/step - loss: 4.0660 - out_loss: 6.6985e-04 - out_2_loss: 4.0653 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 933\n",
            "1/1 [==============================] - 1s 690ms/step - loss: 4.1365 - out_loss: 0.0016 - out_2_loss: 4.1349 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 934\n",
            "1/1 [==============================] - 1s 695ms/step - loss: 4.0828 - out_loss: 0.0012 - out_2_loss: 4.0816 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 935\n",
            "1/1 [==============================] - 1s 712ms/step - loss: 4.0692 - out_loss: 0.0011 - out_2_loss: 4.0681 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 936\n",
            "1/1 [==============================] - 1s 717ms/step - loss: 4.0575 - out_loss: 0.0015 - out_2_loss: 4.0560 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 937\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.1781 - out_loss: 0.0018 - out_2_loss: 4.1763 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 938\n",
            "1/1 [==============================] - 1s 692ms/step - loss: 4.0163 - out_loss: 0.0012 - out_2_loss: 4.0150 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.81948\n",
            "epochs: 939\n",
            "1/1 [==============================] - 1s 688ms/step - loss: 3.8039 - out_loss: 0.0019 - out_2_loss: 3.8019 - out_acc: 1.0000 - out_2_acc: 0.1571\n",
            "\n",
            "Epoch 00001: loss improved from 3.81948 to 3.80389, saving model to ./my_model.h5\n",
            "epochs: 940\n",
            "1/1 [==============================] - 1s 711ms/step - loss: 3.9031 - out_loss: 9.5763e-04 - out_2_loss: 3.9021 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 941\n",
            "1/1 [==============================] - 1s 712ms/step - loss: 4.2081 - out_loss: 0.0011 - out_2_loss: 4.2070 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 942\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 4.0850 - out_loss: 9.9103e-04 - out_2_loss: 4.0841 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 943\n",
            "1/1 [==============================] - 1s 715ms/step - loss: 3.8722 - out_loss: 0.0013 - out_2_loss: 3.8709 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 944\n",
            "1/1 [==============================] - 1s 721ms/step - loss: 4.1377 - out_loss: 0.0018 - out_2_loss: 4.1359 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 945\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.0512 - out_loss: 0.0016 - out_2_loss: 4.0496 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 946\n",
            "1/1 [==============================] - 1s 719ms/step - loss: 3.9563 - out_loss: 0.0010 - out_2_loss: 3.9553 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 947\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 3.8557 - out_loss: 9.5955e-04 - out_2_loss: 3.8547 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 948\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 4.0283 - out_loss: 0.0010 - out_2_loss: 4.0273 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 949\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 3.9071 - out_loss: 0.0026 - out_2_loss: 3.9045 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 950\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 4.1814 - out_loss: 0.0029 - out_2_loss: 4.1785 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 951\n",
            "1/1 [==============================] - 1s 696ms/step - loss: 4.1148 - out_loss: 9.6752e-04 - out_2_loss: 4.1138 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 952\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 4.1572 - out_loss: 6.6880e-04 - out_2_loss: 4.1565 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 953\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 4.1786 - out_loss: 0.0022 - out_2_loss: 4.1763 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 954\n",
            "1/1 [==============================] - 1s 736ms/step - loss: 4.1669 - out_loss: 0.0026 - out_2_loss: 4.1643 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 955\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 4.0274 - out_loss: 0.0019 - out_2_loss: 4.0255 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 956\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 4.1829 - out_loss: 0.0013 - out_2_loss: 4.1816 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 957\n",
            "1/1 [==============================] - 1s 726ms/step - loss: 3.9529 - out_loss: 8.4541e-04 - out_2_loss: 3.9520 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 958\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 4.0663 - out_loss: 0.0018 - out_2_loss: 4.0644 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 959\n",
            "1/1 [==============================] - 1s 712ms/step - loss: 4.1329 - out_loss: 9.5501e-04 - out_2_loss: 4.1319 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 960\n",
            "1/1 [==============================] - 1s 682ms/step - loss: 4.0473 - out_loss: 0.0015 - out_2_loss: 4.0458 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 961\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 4.2550 - out_loss: 0.0011 - out_2_loss: 4.2539 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 962\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.1601 - out_loss: 0.0011 - out_2_loss: 4.1590 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 963\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 4.2486 - out_loss: 0.0012 - out_2_loss: 4.2474 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 964\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 4.1186 - out_loss: 0.0020 - out_2_loss: 4.1167 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 965\n",
            "1/1 [==============================] - 1s 720ms/step - loss: 4.1166 - out_loss: 0.0014 - out_2_loss: 4.1152 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 966\n",
            "1/1 [==============================] - 1s 723ms/step - loss: 3.9229 - out_loss: 0.0012 - out_2_loss: 3.9216 - out_acc: 1.0000 - out_2_acc: 0.1143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 967\n",
            "1/1 [==============================] - 1s 698ms/step - loss: 3.9084 - out_loss: 9.4184e-04 - out_2_loss: 3.9075 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 968\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 4.1237 - out_loss: 0.0011 - out_2_loss: 4.1226 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 969\n",
            "1/1 [==============================] - 1s 724ms/step - loss: 3.9876 - out_loss: 0.0025 - out_2_loss: 3.9850 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 970\n",
            "1/1 [==============================] - 1s 744ms/step - loss: 4.1572 - out_loss: 7.3732e-04 - out_2_loss: 4.1565 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 971\n",
            "1/1 [==============================] - 1s 733ms/step - loss: 4.0248 - out_loss: 9.8318e-04 - out_2_loss: 4.0238 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 972\n",
            "1/1 [==============================] - 1s 718ms/step - loss: 4.0080 - out_loss: 5.0728e-04 - out_2_loss: 4.0075 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 973\n",
            "1/1 [==============================] - 1s 678ms/step - loss: 4.1758 - out_loss: 8.1793e-04 - out_2_loss: 4.1749 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 974\n",
            "1/1 [==============================] - 1s 741ms/step - loss: 3.9315 - out_loss: 9.2568e-04 - out_2_loss: 3.9306 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 975\n",
            "1/1 [==============================] - 1s 702ms/step - loss: 3.9278 - out_loss: 9.6848e-04 - out_2_loss: 3.9268 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 976\n",
            "1/1 [==============================] - 1s 700ms/step - loss: 4.0987 - out_loss: 9.5552e-04 - out_2_loss: 4.0978 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 977\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.0683 - out_loss: 0.0011 - out_2_loss: 4.0672 - out_acc: 1.0000 - out_2_acc: 0.0143\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 978\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 3.9347 - out_loss: 9.5867e-04 - out_2_loss: 3.9338 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 979\n",
            "1/1 [==============================] - 1s 734ms/step - loss: 3.9861 - out_loss: 8.5874e-04 - out_2_loss: 3.9853 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 980\n",
            "1/1 [==============================] - 1s 712ms/step - loss: 4.0356 - out_loss: 6.7300e-04 - out_2_loss: 4.0349 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 981\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 4.2179 - out_loss: 7.4375e-04 - out_2_loss: 4.2172 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 982\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 4.1817 - out_loss: 0.0013 - out_2_loss: 4.1805 - out_acc: 1.0000 - out_2_acc: 0.0000e+00\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 983\n",
            "1/1 [==============================] - 1s 687ms/step - loss: 4.0886 - out_loss: 7.2389e-04 - out_2_loss: 4.0879 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 984\n",
            "1/1 [==============================] - 1s 699ms/step - loss: 4.1071 - out_loss: 7.0350e-04 - out_2_loss: 4.1064 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 985\n",
            "1/1 [==============================] - 1s 691ms/step - loss: 3.9499 - out_loss: 5.8562e-04 - out_2_loss: 3.9493 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 986\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 3.9135 - out_loss: 0.0010 - out_2_loss: 3.9125 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 987\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 4.0742 - out_loss: 0.0011 - out_2_loss: 4.0731 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 988\n",
            "1/1 [==============================] - 1s 686ms/step - loss: 4.1078 - out_loss: 0.0010 - out_2_loss: 4.1068 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 989\n",
            "1/1 [==============================] - 1s 693ms/step - loss: 4.0664 - out_loss: 0.0011 - out_2_loss: 4.0654 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 990\n",
            "1/1 [==============================] - 1s 705ms/step - loss: 3.9492 - out_loss: 0.0014 - out_2_loss: 3.9477 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 991\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 4.0292 - out_loss: 0.0014 - out_2_loss: 4.0278 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 992\n",
            "1/1 [==============================] - 1s 683ms/step - loss: 4.0260 - out_loss: 8.9586e-04 - out_2_loss: 4.0251 - out_acc: 1.0000 - out_2_acc: 0.0571\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 993\n",
            "1/1 [==============================] - 1s 712ms/step - loss: 4.1329 - out_loss: 7.1254e-04 - out_2_loss: 4.1322 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 994\n",
            "1/1 [==============================] - 1s 689ms/step - loss: 3.8183 - out_loss: 0.0015 - out_2_loss: 3.8168 - out_acc: 1.0000 - out_2_acc: 0.1000\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 995\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 3.8844 - out_loss: 0.0011 - out_2_loss: 3.8833 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 996\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 3.9080 - out_loss: 8.2296e-04 - out_2_loss: 3.9072 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 997\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 4.0903 - out_loss: 0.0015 - out_2_loss: 4.0889 - out_acc: 1.0000 - out_2_acc: 0.0429\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 998\n",
            "1/1 [==============================] - 1s 694ms/step - loss: 3.9211 - out_loss: 0.0014 - out_2_loss: 3.9196 - out_acc: 1.0000 - out_2_acc: 0.0714\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 999\n",
            "1/1 [==============================] - 1s 671ms/step - loss: 4.0964 - out_loss: 8.0383e-04 - out_2_loss: 4.0955 - out_acc: 1.0000 - out_2_acc: 0.0286\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n",
            "epochs: 1000\n",
            "1/1 [==============================] - 1s 713ms/step - loss: 3.9697 - out_loss: 0.0020 - out_2_loss: 3.9677 - out_acc: 1.0000 - out_2_acc: 0.0857\n",
            "\n",
            "Epoch 00001: loss did not improve from 3.80389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C65zg3OFhnk2"
      },
      "source": [
        "LSTM_AE.load_weights(path_weight)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxdKcG-RILTS"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "# input_test=input_w2v\n",
        "layer_name = 'bidirectionalx'\n",
        "# layer_name = 'layer_0'\n",
        "get_feature_model = Model(inputs=LSTM_AE.input, outputs=LSTM_AE.get_layer(layer_name).output)\n",
        "cls = get_feature_model.predict([input_w2v])\n",
        "\n",
        "sscaler = preprocessing.StandardScaler()\n",
        "sscaler_z = sscaler.fit(cls)\n",
        "\n",
        "z_transformed = sscaler_z.transform(cls)\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS1_l36pukca",
        "outputId": "5a025222-39cc-49ab-c3bf-a92087873984"
      },
      "source": [
        "get_feature_model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 239)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "layer_0 (Embedding)             (None, 239, 16)      22320       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1, 239, 16)   0           layer_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_2x (GlobalAveragePooling2 (None, 16)           0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_3x (Dense)                (None, 8)            136         layer_2x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_4x (Dense)                (None, 16)           144         layer_3x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_6x (Conv2D)               (None, 1, 239, 1)    17          lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 1, 16)        0           layer_4x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_7x (Multiply)             (None, 1, 239, 16)   0           layer_0[0][0]                    \n",
            "                                                                 layer_6x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_5x (Multiply)             (None, 239, 16)      0           layer_0[0][0]                    \n",
            "                                                                 reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_8x (Lambda)               (None, 239, 16)      0           layer_7x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 239, 16)      0           layer_5x[0][0]                   \n",
            "                                                                 layer_8x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectionalx (Bidirectional)  (None, 200)          93600       add[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 116,217\n",
            "Trainable params: 93,897\n",
            "Non-trainable params: 22,320\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "0by1dt5Kb1lX",
        "outputId": "a9ff82d3-bd85-4c77-e25b-f83ee48e20c9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA  # 主成分分析器\n",
        "\n",
        "# 主成分分析の実行\n",
        "pca = PCA()\n",
        "pca.fit(cls)\n",
        "# データを主成分空間に写像 = 次元圧縮\n",
        "feature = pca.transform(cls)\n",
        "# print(labels[line_inf])\n",
        "# 第一主成分と第二主成分でプロットする\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(cls[line_inf, 0], cls[line_inf, 1])\n",
        "plt.show()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1duETkEc6KQq",
        "outputId": "982b02e3-8899-417b-b074-ff137eeec174"
      },
      "source": [
        "# 文章検索とその距離が近い文章の出力\n",
        "\n",
        "input_sentence = input(\"検索文を以下に入力:\")\n",
        "print(input_sentence)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "検索文を以下に入力:夏の甲子園、新型コロナウイルスの影響で松商学園が不戦勝。\n",
            "夏の甲子園、新型コロナウイルスの影響で松商学園が不戦勝。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnGk0TXZ6KeG"
      },
      "source": [
        "def make_search_w2v(input_sentence, timesteps, dictionary):\n",
        "  search_w2v = []\n",
        "  row_mecab = []\n",
        "  pos = []\n",
        "  # 各行(row)を形態素で分割\n",
        "  # m = MeCab.Tagger('-Owakati')\n",
        "  m = MeCab.Tagger('-chasen')\n",
        "  # if not row == row:\n",
        "  #   print('error')\n",
        "  #   row = ' '\n",
        "  node = m.parseToNode(input_sentence)\n",
        "  while node:  # 文章の最後まで繰り返す\\\n",
        "    row_mecab.append(node.surface)  # 単語情報を抜き出す\n",
        "    pos.append(node.feature.split(',')[0])  # 品詞情報を抜き出す\n",
        "\n",
        "    node = node.next  # 次の単語\n",
        "\n",
        "\n",
        "  print(row_mecab)\n",
        "  # print(pos)\n",
        "  vec_sentence = []\n",
        "  count = 0\n",
        "\n",
        "  for word in row_mecab:\n",
        "    # print('word:', word)\n",
        "    # print(pos_inf)\n",
        "    #辞書に含まれていない単語を逐次追加\n",
        "    if word not in dictionary:\n",
        "      search_w2v.append(0)\n",
        "    else:\n",
        "      search_w2v.append(dictionary.index(word))\n",
        "\n",
        "    count += 1\n",
        "  #データ全体の単語INDEXを取得\n",
        "  # vec_data.append(vec_sentence)\n",
        "  search_w2v = np.array(search_w2v)\n",
        "  print(search_w2v)\n",
        "  search_w2v = np.pad(search_w2v, [timesteps-count, 0])\n",
        "  # search_w2v = search_w2v.reshape(-1, timesteps)\n",
        "\n",
        "  return search_w2v"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evPQYWP7_dez",
        "outputId": "02eb41c7-c095-450d-b6fc-36d55d0631a1"
      },
      "source": [
        "search_w2v = make_search_w2v(input_sentence, timesteps, dictionary)\n",
        "\n",
        "# 検索文に近い文章順に出力\n",
        "search_w2v = search_w2v.reshape(-1, timesteps)\n",
        "search = get_feature_model.predict([search_w2v])\n",
        "\n",
        "dist = []\n",
        "for i in range(len(data)):\n",
        "  dist.append(np.linalg.norm(cls[i]-search))\n",
        "\n",
        "print(dist)\n",
        "sort = np.argsort(dist)\n",
        "print(input_sentence)\n",
        "print(data[sort[0:20]])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '夏', 'の', '甲子', '園', '、', '新型', 'コロナ', 'ウイルス', 'の', '影響', 'で', '松商', '学園', 'が', '不', '戦勝', '。', '']\n",
            "[   1    0    7    0    0   47   69   70   71    7  655   58    0    0\n",
            "   11  863 1010   17    1]\n",
            "[11.60619, 7.289329, 11.897192, 13.520095, 13.484687, 12.595279, 13.25815, 11.147855, 11.5774765, 9.622116, 12.366198, 13.507279, 10.829314, 11.591113, 12.895043, 10.048434, 9.813623, 11.796124, 12.735783, 13.367205, 13.560903, 12.280574, 13.492857, 14.923057, 14.029908, 15.447337, 14.274039, 13.48882, 13.0400095, 14.813008, 14.6335535, 12.912138, 13.907794, 14.532872, 15.387716, 10.625391, 14.317352, 14.099684, 13.888202, 16.419918, 16.786024, 15.992762, 17.11485, 15.709111, 17.110598, 14.308354, 13.353859, 13.904952, 16.501541, 16.076332, 17.431095, 15.710764, 17.256937, 15.596201, 16.608114, 15.656668, 17.336094, 14.651081, 17.591814, 13.145619, 13.85801, 15.49428, 15.15259, 16.579243, 14.622484, 15.465564, 16.124838, 14.430124, 14.2659235, 15.557859]\n",
            "夏の甲子園、新型コロナウイルスの影響で松商学園が不戦勝。\n",
            "1     阪神は新型コロナウイルスの感染拡大でシーズン開幕が延期される中、野球以外でファンに向けて何が...\n",
            "9                            ブルペンデーで、ヤクルトの若手投手陣がアピールした。\n",
            "16                       21日は守護神から先発に転向した楽天松井の登板などに注目だ。\n",
            "15                    プロ野球は本来の開幕日だった3月20日から練習試合が始まっている。\n",
            "35    新型コロナウイルスの感染拡大の影響で2月下旬から公式戦中断中のJリーグは19日に臨時実行委員...\n",
            "12                  巨人新外国人のサンチェス投手が、21日の練習試合DeNA戦に先発する。\n",
            "7     今年初の実戦打撃では5回1死一塁でバスターを決め、内野安打。実りある試合で弾みをつけた右腕は...\n",
            "8     巨人阿部慎之助2軍監督が20日、41歳の誕生日を迎えた。ジャイアンツ球場で行われた2軍練習前...\n",
            "13    約2週間前から杉内2軍投手コーチらのアドバイスを受け、ブルペンで10球ごとに新球へチェンジす...\n",
            "0     インターネットサイト「YouTube」の球団公式チャンネルが話題になっている。17日にはキャ...\n",
            "17    阪神福留孝介外野手（42）が本来の開幕予定日だった20日、ヤクルトとの練習試合（神宮）でハッ...\n",
            "2     企画発案から撮影に編集…。球団にとっても大きな負担となるが、矢野監督は「ファンがこういうのを...\n",
            "21    浜口は神奈川大から16年ドラフト1位で入団。1年目に10勝を挙げチームの日本シリーズ進出に貢...\n",
            "10    先発の2年目清水は2回を無失点。3番手のドラフト4位大西は2回を完璧に抑えた。9回は、ソフト...\n",
            "5     1回は直球と得意球のカットボールで押すも、2回から組み立てを変えてカーブやフォークボールを多...\n",
            "18    2回に左中間へ激走二塁打を放つと、その後も中前、右前と全方位に打ち分けた。新型コロナウイルス...\n",
            "14    日本流を落とし込みながら、投球では「これまでと同じアプローチでやっていくよ」と昨季韓国で17...\n",
            "31       J1湘南ベルマーレは、今季のJ1とJ2で降格なしとの特例が適用されることを「刺激」に変えた。\n",
            "28    北海道コンサドーレ札幌野々村芳和社長（47）が19日、新型コロナウイルスの影響でクラブが被る...\n",
            "59    米女子プロゴルフ協会（LPGA）は20日、新型コロナウイルスの感染拡大によりロッテ選手権（ハ...\n",
            "Name: 記事, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLWcEHiInyZd",
        "outputId": "4ec41001-a68d-4c5b-b23f-f95935e12c4e"
      },
      "source": [
        "print(data[sort[0:20]])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1     阪神は新型コロナウイルスの感染拡大でシーズン開幕が延期される中、野球以外でファンに向けて何が...\n",
            "16                       21日は守護神から先発に転向した楽天松井の登板などに注目だ。\n",
            "9                            ブルペンデーで、ヤクルトの若手投手陣がアピールした。\n",
            "15                    プロ野球は本来の開幕日だった3月20日から練習試合が始まっている。\n",
            "13    約2週間前から杉内2軍投手コーチらのアドバイスを受け、ブルペンで10球ごとに新球へチェンジす...\n",
            "2     企画発案から撮影に編集…。球団にとっても大きな負担となるが、矢野監督は「ファンがこういうのを...\n",
            "7     今年初の実戦打撃では5回1死一塁でバスターを決め、内野安打。実りある試合で弾みをつけた右腕は...\n",
            "12                  巨人新外国人のサンチェス投手が、21日の練習試合DeNA戦に先発する。\n",
            "0     インターネットサイト「YouTube」の球団公式チャンネルが話題になっている。17日にはキャ...\n",
            "17    阪神福留孝介外野手（42）が本来の開幕予定日だった20日、ヤクルトとの練習試合（神宮）でハッ...\n",
            "10    先発の2年目清水は2回を無失点。3番手のドラフト4位大西は2回を完璧に抑えた。9回は、ソフト...\n",
            "8     巨人阿部慎之助2軍監督が20日、41歳の誕生日を迎えた。ジャイアンツ球場で行われた2軍練習前...\n",
            "5     1回は直球と得意球のカットボールで押すも、2回から組み立てを変えてカーブやフォークボールを多...\n",
            "46    蜂窩（ほうか）織炎による発熱で途中休場し、11日目から再出場した西前頭15枚目千代丸（28＝...\n",
            "21    浜口は神奈川大から16年ドラフト1位で入団。1年目に10勝を挙げチームの日本シリーズ進出に貢...\n",
            "45    場所前には行きつけの整骨院に通い、曲がっていた背骨を矯正した。そのかいあってか、今場所も力強...\n",
            "18    2回に左中間へ激走二塁打を放つと、その後も中前、右前と全方位に打ち分けた。新型コロナウイルス...\n",
            "47    一時は40度まで熱が上がった体調について「もう戻った。体は元気です」と話した。この日も右足に...\n",
            "20    知人に描いてもらった2人の似顔絵を手にした左腕は「すごく明るくて前向きなところは、僕にとって...\n",
            "60    また4月2～5日からの延期が決まっていたメジャー第1戦のANAインスピレーション（カリフォル...\n",
            "Name: 記事, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOXla0TuoxLK",
        "outputId": "966f4fb7-38a7-4694-afd8-d54fa20c0b5c"
      },
      "source": [
        "print(dist)\n",
        "print(sort)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11.60619, 7.289329, 11.897192, 13.520095, 13.484687, 12.595279, 13.25815, 11.147855, 11.5774765, 9.622116, 12.366198, 13.507279, 10.829314, 11.591113, 12.895043, 10.048434, 9.813623, 11.796124, 12.735783, 13.367205, 13.560903, 12.280574, 13.492857, 14.923057, 14.029908, 15.447337, 14.274039, 13.48882, 13.0400095, 14.813008, 14.6335535, 12.912138, 13.907794, 14.532872, 15.387716, 10.625391, 14.317352, 14.099684, 13.888202, 16.419918, 16.786024, 15.992762, 17.11485, 15.709111, 17.110598, 14.308354, 13.353859, 13.904952, 16.501541, 16.076332, 17.431095, 15.710764, 17.256937, 15.596201, 16.608114, 15.656668, 17.336094, 14.651081, 17.591814, 13.145619, 13.85801, 15.49428, 15.15259, 16.579243, 14.622484, 15.465564, 16.124838, 14.430124, 14.2659235, 15.557859]\n",
            "[ 1  9 16 15 35 12  7  8 13  0 17  2 21 10  5 18 14 31 28 59  6 46 19  4\n",
            " 27 22 11  3 20 60 38 47 32 24 37 68 26 45 36 67 33 64 30 57 29 23 62 34\n",
            " 25 65 61 69 53 55 43 51 41 49 66 39 48 63 54 40 44 42 52 56 50 58]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQASbJgMr3m1"
      },
      "source": [
        "**検索文入力後、文章特徴量の距離が近い文章を近い順に表示してくれるプログラム**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz-sBxCQqVBi",
        "outputId": "2345a3e9-d46c-46bc-ad37-2ade55ad911d"
      },
      "source": [
        "input_sentence_ex = input(\"検索文を以下に入力:\")\n",
        "\n",
        "search_w2v_ex = make_search_w2v(input_sentence_ex, timesteps, dictionary)\n",
        "\n",
        "# 検索文に近い文章順に出力\n",
        "search_w2v_ex = search_w2v_ex.reshape(-1, timesteps)\n",
        "search_ex = get_feature_model.predict([search_w2v_ex])\n",
        "\n",
        "dist_ex = []\n",
        "for i in range(len(data)):\n",
        "  dist_ex.append(np.linalg.norm(cls[i]-search_ex))\n",
        "\n",
        "print(dist_ex)\n",
        "sort_ex = np.argsort(dist_ex)\n",
        "print(sort_ex)\n",
        "print(input_sentence_ex)\n",
        "print(data[sort_ex[0:20]])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "検索文を以下に入力:三浦知良の所属する横浜FCは現在J1最下位。シーズンも後半戦を迎えており、J2降格阻止へ正念場を迎えている。\n",
            "['', '三浦', '知良', 'の', '所属', 'する', '横浜', 'FC', 'は', '現在', 'J', '1', '最', '下位', '。', 'シーズン', 'も', '後半', '戦', 'を', '迎え', 'て', 'おり', '、', 'J', '2', '降格', '阻止', 'へ', '正念', '場', 'を', '迎え', 'て', 'いる', '。', '']\n",
            "[   1    0    0    7    0  207  467    0   20  617  585  210  431  786\n",
            "   17   74  116 1285  137   29  320   15  574   47  585   61  657    0\n",
            "  373    0   99   29  320   15   16   17    1]\n",
            "[11.750049, 13.236371, 12.357518, 13.091983, 13.476556, 12.756116, 13.488909, 10.979095, 14.1539755, 14.696422, 14.314573, 13.10139, 16.091068, 13.98025, 13.026462, 15.536998, 12.457187, 15.156784, 12.599987, 14.456303, 12.716157, 12.688601, 12.957963, 14.9107065, 12.497544, 14.183282, 13.594772, 11.827199, 13.513799, 10.715146, 12.702923, 10.511167, 13.334847, 14.162459, 13.675364, 11.879459, 11.981836, 14.572578, 10.816486, 15.344657, 14.369186, 14.969699, 15.558805, 14.637801, 12.963426, 14.798522, 15.055143, 15.265045, 14.797959, 12.845704, 13.728472, 15.211376, 14.790019, 16.490591, 12.730213, 12.974686, 13.937829, 14.432979, 14.541815, 15.21548, 15.879251, 15.972475, 14.476455, 15.549756, 15.269568, 13.976837, 14.694147, 15.242478, 14.919954, 13.666175]\n",
            "[31 29 38  7  0 27 35 36  2 16 24 18 21 30 20 54  5 49 22 44 55 14  3 11\n",
            "  1 32  4  6 28 26 69 34 50 56 65 13  8 33 25 10 40 57 19 62 58 37 43 66\n",
            "  9 52 48 45 23 68 41 46 17 51 59 67 47 64 39 15 63 42 60 61 12 53]\n",
            "三浦知良の所属する横浜FCは現在J1最下位。シーズンも後半戦を迎えており、J2降格阻止へ正念場を迎えている。\n",
            "31       J1湘南ベルマーレは、今季のJ1とJ2で降格なしとの特例が適用されることを「刺激」に変えた。\n",
            "29    Jリーグ公式戦は中断中。現時点で札幌のホームゲームは、札幌ドームで週末に予定されていた2試合...\n",
            "38    公式戦として成立する消化試合数は、全日程の75％以上案が選択肢に挙がっている。賞金の分配法や...\n",
            "7     今年初の実戦打撃では5回1死一塁でバスターを決め、内野安打。実りある試合で弾みをつけた右腕は...\n",
            "0     インターネットサイト「YouTube」の球団公式チャンネルが話題になっている。17日にはキャ...\n",
            "27    鳥栖は昨季、5億円以上の赤字を出し、今季も資金難に苦しんでいる。仮に「リーグ戦安定開催融資制...\n",
            "35    新型コロナウイルスの感染拡大の影響で2月下旬から公式戦中断中のJリーグは19日に臨時実行委員...\n",
            "36    J2、J3ともライセンス上問題のない上位2チームが自動昇格。今季18チームのJ1は来季20チ...\n",
            "2     企画発案から撮影に編集…。球団にとっても大きな負担となるが、矢野監督は「ファンがこういうのを...\n",
            "16                       21日は守護神から先発に転向した楽天松井の登板などに注目だ。\n",
            "24     Jリーグの村井満チェアマン（60）が19日、経営難に陥ったサガン鳥栖と協議中であることを認めた。\n",
            "18    2回に左中間へ激走二塁打を放つと、その後も中前、右前と全方位に打ち分けた。新型コロナウイルス...\n",
            "21    浜口は神奈川大から16年ドラフト1位で入団。1年目に10勝を挙げチームの日本シリーズ進出に貢...\n",
            "30    この日、J1の降格チームなしが決まった。だが今季の戦い方に変わりはない。「現場の目標はACL...\n",
            "20    知人に描いてもらった2人の似顔絵を手にした左腕は「すごく明るくて前向きなところは、僕にとって...\n",
            "54    何度も顔を合わせた相手との対戦を「幕内で何度も対戦があるから、ちょっと何か気負いすぎたかな、...\n",
            "5     1回は直球と得意球のカットボールで押すも、2回から組み立てを変えてカーブやフォークボールを多...\n",
            "49    1番でも多くの好取組を－。日本相撲協会の審判部は14日目の幕内取組を、13日目の全取組終了後...\n",
            "22    日本サッカー協会（JFA）は20日、田嶋幸三会長（62）が14日に新型コロナウイルスを発症し...\n",
            "44    大関昇進に向けて、試練の3番勝負の最初でつまずいた。14日目は不戦勝を除いて過去1勝1敗の鶴...\n",
            "Name: 記事, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEA1sTDdkWfz"
      },
      "source": [
        "# 距離関数Dの作成（ユークリッド距離関数）\n",
        "def euclidean_distance(vects):\n",
        "  # print(vects)\n",
        "  x, y = vects\n",
        "  # print('x:', x)\n",
        "  # print('y:', y)\n",
        "  sum_square = tf.math.reduce_sum(tf.math.square(x-y), axis=1, keepdims=True)\n",
        "  return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
        "\n",
        "# def eucl_dist_output_shape(shapes):\n",
        "#   shape1, shape2 = shapes\n",
        "#   return (shape1[0], 1)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsWykJtzlMG_"
      },
      "source": [
        "# Contrastive Loss関数の作成\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "  margin = 1  # margin:ハイパーパラメータ\n",
        "  square_pred = tf.math.square(y_pred)\n",
        "  margin_square = tf.math.square(tf.math.maximum(margin - y_pred, 0))\n",
        "  return tf.math.reduce_mean(y_true * square_pred + (1-y_true) * margin_square)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxPx7RG_mvi2",
        "outputId": "34022a9a-5297-46cf-bbce-348b32e0f9dc"
      },
      "source": [
        "# 再学習用モデルの検索文章側定義\n",
        "search_sentence = Model(inputs=LSTM_AE.input, outputs=LSTM_AE.get_layer(layer_name).output)\n",
        "search_feature = search_sentence.output\n",
        "\n",
        "# 再学習用モデルの検索結果側定義\n",
        "# inputs_result = Input(shape=(timesteps, ))\n",
        "# result_sentence = Model(inputs=inputs, outputs=LSTM_AE.get_layer(layer_name).output)\n",
        "inputs_result = Input(shape=(cls.shape[1], ))\n",
        "merge_layer = Lambda(euclidean_distance)([search_feature, inputs_result])\n",
        "normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
        "distance = Dense(1, activation=\"sigmoid\")(normal_layer)\n",
        "\n",
        "print(search_sentence.input)\n",
        "\n",
        "reLSTM = Model(inputs=[search_sentence.input, inputs_result], outputs=distance)\n",
        "reLSTM.summary()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 239), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 239)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "layer_0 (Embedding)             (None, 239, 16)      22320       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1, 239, 16)   0           layer_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_2x (GlobalAveragePooling2 (None, 16)           0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_3x (Dense)                (None, 8)            136         layer_2x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_4x (Dense)                (None, 16)           144         layer_3x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_6x (Conv2D)               (None, 1, 239, 1)    17          lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 1, 16)        0           layer_4x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_7x (Multiply)             (None, 1, 239, 16)   0           layer_0[0][0]                    \n",
            "                                                                 layer_6x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_5x (Multiply)             (None, 239, 16)      0           layer_0[0][0]                    \n",
            "                                                                 reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_8x (Lambda)               (None, 239, 16)      0           layer_7x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 239, 16)      0           layer_5x[0][0]                   \n",
            "                                                                 layer_8x[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectionalx (Bidirectional)  (None, 200)          93600       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 1)            0           bidirectionalx[0][0]             \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 1)            4           lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            2           batch_normalization[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 116,223\n",
            "Trainable params: 93,901\n",
            "Non-trainable params: 22,322\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8e8HM6dxyOe",
        "outputId": "d27eb6b7-059f-4e69-9410-04ffe6969470"
      },
      "source": [
        "%ls /drive/MyDrive/"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \u001b[0m\u001b[01;34mトーク\u001b[0m/\n",
            " 20210621_Chapter4_廣澤_配布用.ipynb\n",
            " 2021_共通ゼミ_廣澤_配布用.pptx\n",
            " clustering.csv\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            " \u001b[01;34mColab_Notebooks\u001b[0m/\n",
            " Example_1.csv\n",
            " Example_1.gsheet\n",
            " \u001b[01;34mpytorch-openpose\u001b[0m/\n",
            " pytorch_openpose.ipynb\n",
            " ReLSTM用データセット.csv\n",
            "'○参考用語集(ver3).gdoc'\n",
            " 実践プログラミングゼミ４班中間発表.gslides\n",
            " 本_候補.docx\n",
            " 研究時間管理表.gsheet\n",
            " 領収書.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpnrWjr3fEXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1120721a-0313-4e96-c105-95ffd6ee1eaa"
      },
      "source": [
        "result_tmp = pd.read_csv('/drive/MyDrive/ReLSTM用データセット.csv', encoding='cp932')\n",
        "print(result_tmp)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 検索文章  ...  クラスラベル\n",
            "0          オリンピックの野球競技において、稲葉監督率いる侍ジャパンは悲願の金メダルを獲得した。  ...       0\n",
            "1          オリンピックの野球競技において、稲葉監督率いる侍ジャパンは悲願の金メダルを獲得した。  ...       1\n",
            "2          オリンピックの野球競技において、稲葉監督率いる侍ジャパンは悲願の金メダルを獲得した。  ...       0\n",
            "3          オリンピックの野球競技において、稲葉監督率いる侍ジャパンは悲願の金メダルを獲得した。  ...       1\n",
            "4          オリンピックの野球競技において、稲葉監督率いる侍ジャパンは悲願の金メダルを獲得した。  ...       0\n",
            "5                              暴行事件で謹慎中の中田、巨人へトレード移籍。  ...       0\n",
            "6                              暴行事件で謹慎中の中田、巨人へトレード移籍。  ...       1\n",
            "7                              暴行事件で謹慎中の中田、巨人へトレード移籍。  ...       1\n",
            "8                        夏の甲子園、新型コロナウイルスの影響で松商学園が不戦勝。  ...       0\n",
            "9                        夏の甲子園、新型コロナウイルスの影響で松商学園が不戦勝。  ...       0\n",
            "10                       夏の甲子園、新型コロナウイルスの影響で松商学園が不戦勝。  ...       1\n",
            "11                       夏の甲子園、新型コロナウイルスの影響で松商学園が不戦勝。  ...       1\n",
            "12  引退をかけて本場所に臨んだ横綱白鵬は千秋楽の大関照ノ富士との全勝同士の一番を制し、全勝優勝を...  ...       1\n",
            "13  引退をかけて本場所に臨んだ横綱白鵬は千秋楽の大関照ノ富士との全勝同士の一番を制し、全勝優勝を...  ...       1\n",
            "14  引退をかけて本場所に臨んだ横綱白鵬は千秋楽の大関照ノ富士との全勝同士の一番を制し、全勝優勝を...  ...       0\n",
            "15  引退をかけて本場所に臨んだ横綱白鵬は千秋楽の大関照ノ富士との全勝同士の一番を制し、全勝優勝を...  ...       0\n",
            "16  三浦知良の所属する横浜FCは現在J1最下位。シーズンも後半戦を迎えており、J2降格阻止へ正念...  ...       0\n",
            "17  三浦知良の所属する横浜FCは現在J1最下位。シーズンも後半戦を迎えており、J2降格阻止へ正念...  ...       0\n",
            "18  三浦知良の所属する横浜FCは現在J1最下位。シーズンも後半戦を迎えており、J2降格阻止へ正念...  ...       1\n",
            "19  三浦知良の所属する横浜FCは現在J1最下位。シーズンも後半戦を迎えており、J2降格阻止へ正念...  ...       1\n",
            "20  三浦知良の所属する横浜FCは現在J1最下位。シーズンも後半戦を迎えており、J2降格阻止へ正念...  ...       0\n",
            "\n",
            "[21 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKztpACN0IBo",
        "outputId": "0f22b5ab-b33e-4f4c-eadb-b22ddaf9021c"
      },
      "source": [
        "print(result_tmp.shape[0])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD2MD5U9ItGJ",
        "outputId": "2abafcb4-0592-46b5-daf0-852cfc957e24"
      },
      "source": [
        "# 同じクラス(似ている文章同士)は1、似ていない文章のペアは0とする。\n",
        "\n",
        "# result_database = []\n",
        "# result_database.append([input_sentence, sort[0], data[sort[0]], 0])\n",
        "# result_database.append([input_sentence, sort[1], data[sort[1]], 1])\n",
        "# result_database.append([input_sentence, sort[2], data[sort[2]], 0])\n",
        "# result_database.append([input_sentence, sort[3], data[sort[3]], 1])\n",
        "# print(result_database)\n",
        "# result_database = np.array(result_database)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['オリンピックの野球競技において、稲葉監督率いる侍ジャパンは悲願の金メダルを獲得した。', 9, 'ブルペンデーで、ヤクルトの若手投手陣がアピールした。', 0], ['オリンピックの野球競技において、稲葉監督率いる侍ジャパンは悲願の金メダルを獲得した。', 16, '21日は守護神から先発に転向した楽天松井の登板などに注目だ。', 1], ['オリンピックの野球競技において、稲葉監督率いる侍ジャパンは悲願の金メダルを獲得した。', 7, '今年初の実戦打撃では5回1死一塁でバスターを決め、内野安打。実りある試合で弾みをつけた右腕は「あとは微調整や、やりたいことに集中できる」と先を見据えた。今後は1度登板機会を空け、次回は4月第1週の週末を予定する。そこから定められた開幕日に合わせていく。', 0], ['オリンピックの野球競技において、稲葉監督率いる侍ジャパンは悲願の金メダルを獲得した。', 15, 'プロ野球は本来の開幕日だった3月20日から練習試合が始まっている。', 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7kfaR4U2KH9",
        "outputId": "000aaa0b-f24e-4be2-e2d9-219b19e3c300"
      },
      "source": [
        "# cls = get_feature_model.predict([input_w2v])\n",
        "# print(cls.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEhoCJXIJ7S8",
        "outputId": "e7548541-4190-48e9-8410-d4ac69963d88"
      },
      "source": [
        "\n",
        "search_input_sentence = result_tmp['検索文章']\n",
        "search_result_sentence = result_tmp['結果文章index']\n",
        "tr_y = result_tmp['クラスラベル']\n",
        "line_search_inf = np.arange(result_tmp.shape[0])\n",
        "print(line_search_inf)\n",
        "print(search_result_sentence)\n",
        "tr_y = np.array(tr_y, dtype='float32')\n",
        "print(tr_y)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
            "0     64\n",
            "1      4\n",
            "2     68\n",
            "3     63\n",
            "4     67\n",
            "5     45\n",
            "6     12\n",
            "7     14\n",
            "8      9\n",
            "9     16\n",
            "10    35\n",
            "11    28\n",
            "12    41\n",
            "13    43\n",
            "14    16\n",
            "15     0\n",
            "16     7\n",
            "17     0\n",
            "18    36\n",
            "19    30\n",
            "20    41\n",
            "Name: 結果文章index, dtype: int64\n",
            "[0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNUeXHmIQEkf",
        "outputId": "8c2595e5-4579-41be-9354-00fd9aab0973"
      },
      "source": [
        "print(int(search_result_sentence[0]))\n",
        "print(input_w2v[42])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   1 908 335  11 957 958 959 960 961   7 962 963  13  47\n",
            " 892 893 894  20 964 965  29 966  15 967  15  89  39  17   4 435 111  90\n",
            " 968  11 969  87 208  15 134  39  17 933 137 116  61  66 281  87 208  15\n",
            " 134  39   7  58  47 970   7 904  29 971 972 973 108  39  17  58 116 971\n",
            " 974 145  39   7  20 975 970  11 134  39   6  17 895 938   7 939   7 815\n",
            " 942  92 391  13  82  15  47 417  11 167 946  13 116 976 158 977  52 525\n",
            "  29 978  39  17   1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g_tkYnrP8Bn",
        "outputId": "375e0ace-7d5e-4cf0-fb2e-8a7067f8dcb3"
      },
      "source": [
        "print(input_w2v[int(search_result_sentence[0])])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    1 1295 1277 1263    7\n",
            " 1284  309  153  137   47 1333  550 1334   56  557 1290 1335   19   47\n",
            " 1336  550 1337   59    7 1259   11  446   13   14   39  143   29  370\n",
            "   47  466    7 1338  872   58  395  286  776   29 1339   39 1340 1341\n",
            "   56  363  931 1342   59   11   47 1245    7  554   29  670   39   17\n",
            "    1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BgAU6TzOe5G",
        "outputId": "85c44aa1-7071-4395-9596-ea74f6e20efc"
      },
      "source": [
        "search_result_sentence_w2v = []\n",
        "\n",
        "for i in range(result_tmp.shape[0]):\n",
        "  search_result_sentence_w2v.append(input_w2v[int(search_result_sentence[i])])\n",
        "\n",
        "search_result_sentence_w2v = np.array(search_result_sentence_w2v)\n",
        "print(search_result_sentence_w2v.shape)\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21, 239)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38RcoXFFOc_i",
        "outputId": "6c8791e7-b316-481c-a0a1-4a9a45777b9f"
      },
      "source": [
        "search_result_feature = get_feature_model.predict([search_result_sentence_w2v])\n",
        "print(search_result_feature.shape)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "htQR-1mg1OX5",
        "outputId": "400ef2d5-4ef5-410d-d27e-02ebc0d452d7"
      },
      "source": [
        "search_input_sentence[5]"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'暴行事件で謹慎中の中田、巨人へトレード移籍。'"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNNAb2YLMhTO",
        "outputId": "15395055-2e9a-4ff2-a42f-466099f8dd9e"
      },
      "source": [
        "search_input_sentence_w2v = []\n",
        "for i in range(result_tmp.shape[0]):\n",
        "  search_input_sentence_w2v.append(make_search_w2v(search_input_sentence[i], timesteps, dictionary))\n",
        "\n",
        "# print(search_input_sentence_w2v)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'オリンピック', 'の', '野球', '競技', 'に', 'おい', 'て', '、', '稲葉', '監督', '率いる', '侍', 'ジャパン', 'は', '悲願', 'の', '金', 'メダル', 'を', '獲得', 'し', 'た', '。', '']\n",
            "[   1 1303    7   79    0   13    0   15   47    0   94    0    0    0\n",
            "   20    0    7    0    0   29    0   38   39   17    1]\n",
            "['', 'オリンピック', 'の', '野球', '競技', 'に', 'おい', 'て', '、', '稲葉', '監督', '率いる', '侍', 'ジャパン', 'は', '悲願', 'の', '金', 'メダル', 'を', '獲得', 'し', 'た', '。', '']\n",
            "[   1 1303    7   79    0   13    0   15   47    0   94    0    0    0\n",
            "   20    0    7    0    0   29    0   38   39   17    1]\n",
            "['', 'オリンピック', 'の', '野球', '競技', 'に', 'おい', 'て', '、', '稲葉', '監督', '率いる', '侍', 'ジャパン', 'は', '悲願', 'の', '金', 'メダル', 'を', '獲得', 'し', 'た', '。', '']\n",
            "[   1 1303    7   79    0   13    0   15   47    0   94    0    0    0\n",
            "   20    0    7    0    0   29    0   38   39   17    1]\n",
            "['', 'オリンピック', 'の', '野球', '競技', 'に', 'おい', 'て', '、', '稲葉', '監督', '率いる', '侍', 'ジャパン', 'は', '悲願', 'の', '金', 'メダル', 'を', '獲得', 'し', 'た', '。', '']\n",
            "[   1 1303    7   79    0   13    0   15   47    0   94    0    0    0\n",
            "   20    0    7    0    0   29    0   38   39   17    1]\n",
            "['', 'オリンピック', 'の', '野球', '競技', 'に', 'おい', 'て', '、', '稲葉', '監督', '率いる', '侍', 'ジャパン', 'は', '悲願', 'の', '金', 'メダル', 'を', '獲得', 'し', 'た', '。', '']\n",
            "[   1 1303    7   79    0   13    0   15   47    0   94    0    0    0\n",
            "   20    0    7    0    0   29    0   38   39   17    1]\n",
            "['', '暴行', '事件', 'で', '謹慎', '中', 'の', '中田', '、', '巨人', 'へ', 'トレード', '移籍', '。', '']\n",
            "[  1   0   0  58   0  24   7   0  47 314 373   0   0  17   1]\n",
            "['', '暴行', '事件', 'で', '謹慎', '中', 'の', '中田', '、', '巨人', 'へ', 'トレード', '移籍', '。', '']\n",
            "[  1   0   0  58   0  24   7   0  47 314 373   0   0  17   1]\n",
            "['', '暴行', '事件', 'で', '謹慎', '中', 'の', '中田', '、', '巨人', 'へ', 'トレード', '移籍', '。', '']\n",
            "[  1   0   0  58   0  24   7   0  47 314 373   0   0  17   1]\n",
            "['', '夏', 'の', '甲子', '園', '、', '新型', 'コロナ', 'ウイルス', 'の', '影響', 'で', '松商', '学園', 'が', '不', '戦勝', '。', '']\n",
            "[   1    0    7    0    0   47   69   70   71    7  655   58    0    0\n",
            "   11  863 1010   17    1]\n",
            "['', '夏', 'の', '甲子', '園', '、', '新型', 'コロナ', 'ウイルス', 'の', '影響', 'で', '松商', '学園', 'が', '不', '戦勝', '。', '']\n",
            "[   1    0    7    0    0   47   69   70   71    7  655   58    0    0\n",
            "   11  863 1010   17    1]\n",
            "['', '夏', 'の', '甲子', '園', '、', '新型', 'コロナ', 'ウイルス', 'の', '影響', 'で', '松商', '学園', 'が', '不', '戦勝', '。', '']\n",
            "[   1    0    7    0    0   47   69   70   71    7  655   58    0    0\n",
            "   11  863 1010   17    1]\n",
            "['', '夏', 'の', '甲子', '園', '、', '新型', 'コロナ', 'ウイルス', 'の', '影響', 'で', '松商', '学園', 'が', '不', '戦勝', '。', '']\n",
            "[   1    0    7    0    0   47   69   70   71    7  655   58    0    0\n",
            "   11  863 1010   17    1]\n",
            "['', '引退', 'を', 'かけ', 'て', '本', '場所', 'に', '臨ん', 'だ', '横綱', '白鵬', 'は', '千秋', '楽', 'の', '大関', '照', 'ノ', '富士', 'と', 'の', '全勝', '同士', 'の', '一', '番', 'を', '制し', '、', '全勝', '優勝', 'を', '果たし', 'た', '。', '']\n",
            "[   1 1190   29 1181   15  192  942   13 1221  276  933  891   20 1117\n",
            " 1118    7  895    0    0    0   87    7    0    0    7  770 1007   29\n",
            "    0   47    0  776   29    0   39   17    1]\n",
            "['', '引退', 'を', 'かけ', 'て', '本', '場所', 'に', '臨ん', 'だ', '横綱', '白鵬', 'は', '千秋', '楽', 'の', '大関', '照', 'ノ', '富士', 'と', 'の', '全勝', '同士', 'の', '一', '番', 'を', '制し', '、', '全勝', '優勝', 'を', '果たし', 'た', '。', '']\n",
            "[   1 1190   29 1181   15  192  942   13 1221  276  933  891   20 1117\n",
            " 1118    7  895    0    0    0   87    7    0    0    7  770 1007   29\n",
            "    0   47    0  776   29    0   39   17    1]\n",
            "['', '引退', 'を', 'かけ', 'て', '本', '場所', 'に', '臨ん', 'だ', '横綱', '白鵬', 'は', '千秋', '楽', 'の', '大関', '照', 'ノ', '富士', 'と', 'の', '全勝', '同士', 'の', '一', '番', 'を', '制し', '、', '全勝', '優勝', 'を', '果たし', 'た', '。', '']\n",
            "[   1 1190   29 1181   15  192  942   13 1221  276  933  891   20 1117\n",
            " 1118    7  895    0    0    0   87    7    0    0    7  770 1007   29\n",
            "    0   47    0  776   29    0   39   17    1]\n",
            "['', '引退', 'を', 'かけ', 'て', '本', '場所', 'に', '臨ん', 'だ', '横綱', '白鵬', 'は', '千秋', '楽', 'の', '大関', '照', 'ノ', '富士', 'と', 'の', '全勝', '同士', 'の', '一', '番', 'を', '制し', '、', '全勝', '優勝', 'を', '果たし', 'た', '。', '']\n",
            "[   1 1190   29 1181   15  192  942   13 1221  276  933  891   20 1117\n",
            " 1118    7  895    0    0    0   87    7    0    0    7  770 1007   29\n",
            "    0   47    0  776   29    0   39   17    1]\n",
            "['', '三浦', '知良', 'の', '所属', 'する', '横浜', 'FC', 'は', '現在', 'J', '1', '最', '下位', '。', 'シーズン', 'も', '後半', '戦', 'を', '迎え', 'て', 'おり', '、', 'J', '2', '降格', '阻止', 'へ', '正念', '場', 'を', '迎え', 'て', 'いる', '。', '']\n",
            "[   1    0    0    7    0  207  467    0   20  617  585  210  431  786\n",
            "   17   74  116 1285  137   29  320   15  574   47  585   61  657    0\n",
            "  373    0   99   29  320   15   16   17    1]\n",
            "['', '三浦', '知良', 'の', '所属', 'する', '横浜', 'FC', 'は', '現在', 'J', '1', '最', '下位', '。', 'シーズン', 'も', '後半', '戦', 'を', '迎え', 'て', 'おり', '、', 'J', '2', '降格', '阻止', 'へ', '正念', '場', 'を', '迎え', 'て', 'いる', '。', '']\n",
            "[   1    0    0    7    0  207  467    0   20  617  585  210  431  786\n",
            "   17   74  116 1285  137   29  320   15  574   47  585   61  657    0\n",
            "  373    0   99   29  320   15   16   17    1]\n",
            "['', '三浦', '知良', 'の', '所属', 'する', '横浜', 'FC', 'は', '現在', 'J', '1', '最', '下位', '。', 'シーズン', 'も', '後半', '戦', 'を', '迎え', 'て', 'おり', '、', 'J', '2', '降格', '阻止', 'へ', '正念', '場', 'を', '迎え', 'て', 'いる', '。', '']\n",
            "[   1    0    0    7    0  207  467    0   20  617  585  210  431  786\n",
            "   17   74  116 1285  137   29  320   15  574   47  585   61  657    0\n",
            "  373    0   99   29  320   15   16   17    1]\n",
            "['', '三浦', '知良', 'の', '所属', 'する', '横浜', 'FC', 'は', '現在', 'J', '1', '最', '下位', '。', 'シーズン', 'も', '後半', '戦', 'を', '迎え', 'て', 'おり', '、', 'J', '2', '降格', '阻止', 'へ', '正念', '場', 'を', '迎え', 'て', 'いる', '。', '']\n",
            "[   1    0    0    7    0  207  467    0   20  617  585  210  431  786\n",
            "   17   74  116 1285  137   29  320   15  574   47  585   61  657    0\n",
            "  373    0   99   29  320   15   16   17    1]\n",
            "['', '三浦', '知良', 'の', '所属', 'する', '横浜', 'FC', 'は', '現在', 'J', '1', '最', '下位', '。', 'シーズン', 'も', '後半', '戦', 'を', '迎え', 'て', 'おり', '、', 'J', '2', '降格', '阻止', 'へ', '正念', '場', 'を', '迎え', 'て', 'いる', '。', '']\n",
            "[   1    0    0    7    0  207  467    0   20  617  585  210  431  786\n",
            "   17   74  116 1285  137   29  320   15  574   47  585   61  657    0\n",
            "  373    0   99   29  320   15   16   17    1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "770EW923NnRM",
        "outputId": "70f83f7c-6eb2-495d-82d8-01db1d413d47"
      },
      "source": [
        "search_input_sentence_w2v = np.array(search_input_sentence_w2v)\n",
        "print(search_input_sentence_w2v.shape)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21, 239)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qapztis5OL5L",
        "outputId": "32be48d5-bbfe-4345-83b2-4c55b62c7817"
      },
      "source": [
        "path_weight2 = './my_remodel.h5'\n",
        "\n",
        "reLSTM.compile(loss=contrastive_loss, optimizer=RMSprop())  # constrative lossを用いたモデルではoptimizerはAdamではなくRMSpropを用いるケースが多かった(根拠なしなので変更の余地あり)\n",
        "reLSTM.fit([search_input_sentence_w2v, search_result_feature], \n",
        "           tr_y, \n",
        "           epochs= 300, \n",
        "           batch_size=batchsize_lstm, \n",
        "           shuffle=True, )\n",
        "\n",
        "reLSTM.save_weights(path_weight2)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.2804\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.1445\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.1011\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0650\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0492\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0416\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 0s 407ms/step - loss: 0.0424\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 0s 389ms/step - loss: 0.0398\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.0317\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 0.0284\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0271\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0262\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 0s 394ms/step - loss: 0.0264\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0247\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0237\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 0s 390ms/step - loss: 0.0230\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0227\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0225\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0225\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0228\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0238\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 0.0239\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0234\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0231\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0220\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0218\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0215\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0214\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.0213\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0213\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0211\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.0211\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0210\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 0s 430ms/step - loss: 0.0211\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0211\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0211\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 0.0210\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0207\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0205\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 0s 406ms/step - loss: 0.0202\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 0s 389ms/step - loss: 0.0201\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.0199\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0199\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0199\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 0s 390ms/step - loss: 0.0200\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0198\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0198\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.0194\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0192\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0190\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0189\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0190\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0193\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 0.0196\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 0s 395ms/step - loss: 0.0195\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 0s 394ms/step - loss: 0.0189\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.0186\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0187\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0188\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0190\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 0.0185\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 0.0181\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0179\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0178\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0177\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 0s 407ms/step - loss: 0.0176\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0176\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0176\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0178\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0179\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0177\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 0.0174\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 0s 407ms/step - loss: 0.0172\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0171\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0172\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0172\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0171\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0167\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0166\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.0166\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0167\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 0.0170\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0166\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0163\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.0161\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 0.0160\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0159\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0159\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0159\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0159\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 0.0158\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0157\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 0.0155\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 0s 439ms/step - loss: 0.0154\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0153\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.0153\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0153\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0155\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0154\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0152\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0151\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0151\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0151\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 0.0150\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 0.0148\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0147\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 0s 391ms/step - loss: 0.0145\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.0144\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.0143\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.0142\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0141\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0141\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0141\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0141\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0141\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0140\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 0.0139\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 0.0137\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 0.0136\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.0135\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.0135\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 0s 388ms/step - loss: 0.0135\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0134\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 0s 433ms/step - loss: 0.0134\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0134\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0133\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 0.0132\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 0.0130\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 0s 446ms/step - loss: 0.0129\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0129\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0129\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0128\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0128\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 0s 392ms/step - loss: 0.0128\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0126\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0125\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0124\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0124\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0124\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 0.0124\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 0s 407ms/step - loss: 0.0124\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 0.0124\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 0.0122\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0121\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0120\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 0.0119\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0118\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0117\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 0.0117\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0116\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0116\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 0s 441ms/step - loss: 0.0116\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0116\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 0.0115\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0114\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0113\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0112\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 0s 391ms/step - loss: 0.0111\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 0.0111\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0110\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0110\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 0.0109\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 0s 433ms/step - loss: 0.0109\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0108\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0109\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0108\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0107\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0106\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.0106\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0105\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 0.0105\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0104\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0103\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0103\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 0.0102\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 0s 425ms/step - loss: 0.0102\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0101\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0100\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 0s 442ms/step - loss: 0.0100\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 0s 406ms/step - loss: 0.0099\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0099\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0099\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 0.0098\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 0.0098\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0097\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0096\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0096\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 0.0096\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.0095\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0095\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0094\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.0094\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0093\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0092\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0092\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0091\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0091\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0091\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0090\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 0s 407ms/step - loss: 0.0090\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.0089\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 0.0088\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 0.0088\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0087\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0087\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0086\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 0.0086\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0085\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 0.0085\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 0s 406ms/step - loss: 0.0085\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 0.0085\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 0s 398ms/step - loss: 0.0085\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 0.0084\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0083\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.0082\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0082\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 0s 407ms/step - loss: 0.0081\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 0.0081\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0080\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0080\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 0.0079\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 0s 397ms/step - loss: 0.0079\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.0079\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 0.0079\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0079\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 0s 429ms/step - loss: 0.0078\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 0s 394ms/step - loss: 0.0077\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 0.0077\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0076\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0076\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 0.0076\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 0s 425ms/step - loss: 0.0075\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0075\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0074\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.0074\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.0073\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 0.0073\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0073\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0072\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.0072\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0071\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0071\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0070\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0070\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0070\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 0s 406ms/step - loss: 0.0069\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0069\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0069\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 0s 400ms/step - loss: 0.0068\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0068\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0068\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0067\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 0.0067\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0066\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 0.0066\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.0066\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0065\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 0s 390ms/step - loss: 0.0065\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0065\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 0.0065\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 0s 390ms/step - loss: 0.0064\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0064\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0063\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 0.0063\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 0.0062\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0062\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0061\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0061\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0061\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 0s 406ms/step - loss: 0.0060\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0060\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 0s 437ms/step - loss: 0.0060\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 0.0060\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.0059\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 0.0059\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 0.0059\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0058\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 0s 426ms/step - loss: 0.0058\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 0.0058\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 0s 390ms/step - loss: 0.0057\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 0s 393ms/step - loss: 0.0057\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 0s 406ms/step - loss: 0.0057\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0056\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0056\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 0s 391ms/step - loss: 0.0056\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0055\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.0055\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 0s 403ms/step - loss: 0.0055\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0054\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0054\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.0054\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0053\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.0053\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.0053\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 0s 399ms/step - loss: 0.0052\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.0052\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0052\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 0.0052\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.0051\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 0.0051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-ba6138034014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m            shuffle=True, )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mreLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_weight2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'path_weight2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmmF7dZ_-m6y"
      },
      "source": [
        "reLSTM.load_weights(path_weight2)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "n0QFvS0io1H6",
        "outputId": "363b64cd-c707-4624-9524-2519d55dcfd8"
      },
      "source": [
        "\n",
        "# 主成分分析の実行\n",
        "pca = PCA()\n",
        "pca.fit(search)\n",
        "# データを主成分空間に写像 = 次元圧縮\n",
        "feature = pca.transform(search)\n",
        "# print(labels[line_inf])\n",
        "# 第一主成分と第二主成分でプロットする\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(search[0, 0], search[0, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_pca.py:454: RuntimeWarning: invalid value encountered in true_divide\n",
            "  explained_variance_ = (S ** 2) / (n_samples - 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "F6Iw9MdlopJG",
        "outputId": "f3dfb0a0-9fa5-4772-d12c-18acac88c5f1"
      },
      "source": [
        "\n",
        "# 主成分分析の実行\n",
        "pca = PCA()\n",
        "pca.fit(search2)\n",
        "# データを主成分空間に写像 = 次元圧縮\n",
        "feature = pca.transform(search2)\n",
        "# print(labels[line_inf])\n",
        "# 第一主成分と第二主成分でプロットする\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(search2[0, 0], search2[0, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_pca.py:454: RuntimeWarning: invalid value encountered in true_divide\n",
            "  explained_variance_ = (S ** 2) / (n_samples - 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "ixQEj9beUuFP",
        "outputId": "b2d7de52-e824-4242-b9ac-56d0d457dba7"
      },
      "source": [
        "cls2 = get_feature_model.predict([input_w2v])\n",
        "\n",
        "# 主成分分析の実行\n",
        "pca = PCA()\n",
        "pca.fit(cls2)\n",
        "# データを主成分空間に写像 = 次元圧縮\n",
        "feature = pca.transform(cls2)\n",
        "# print(labels[line_inf])\n",
        "# 第一主成分と第二主成分でプロットする\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(cls2[line_inf, 0], cls2[line_inf, 1])\n",
        "plt.show()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnW-l1483jbX",
        "outputId": "7649ce69-362f-4808-e8e2-8c5f46cd6149"
      },
      "source": [
        "search_w2v2 = make_search_w2v(input_sentence, timesteps, dictionary)\n",
        "\n",
        "# 検索文に近い文章順に出力\n",
        "search_w2v2 = search_w2v2.reshape(-1, timesteps)\n",
        "search2 = get_feature_model.predict([search_w2v2])\n",
        "\n",
        "dist2 = []\n",
        "for i in range(len(data)):\n",
        "  dist2.append(np.linalg.norm(cls2[i]-search2))\n",
        "\n",
        "print(dist2)\n",
        "sort2 = np.argsort(dist2)\n",
        "print(sort2)\n",
        "print(input_sentence)\n",
        "print(data[sort2[0:20]])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '夏', 'の', '甲子', '園', '、', '新型', 'コロナ', 'ウイルス', 'の', '影響', 'で', '松商', '学園', 'が', '不', '戦勝', '。', '']\n",
            "[   1    0    7    0    0   47   69   70   71    7  655   58    0    0\n",
            "   11  863 1010   17    1]\n",
            "[11.937191, 9.275287, 12.063947, 12.467684, 13.158714, 12.258073, 12.950623, 12.579599, 8.32902, 8.574096, 10.443934, 13.26057, 6.358309, 5.93308, 13.927605, 5.923301, 6.649548, 9.772929, 13.627724, 13.051834, 13.283286, 12.286848, 11.659482, 13.458756, 10.792719, 13.645281, 12.750891, 11.240231, 13.972383, 11.372501, 12.705611, 10.667471, 12.185158, 15.33211, 13.784128, 5.7280374, 13.464687, 13.69492, 12.026132, 15.729927, 13.80745, 14.594473, 12.578789, 14.925783, 12.42527, 11.660424, 13.742016, 11.915233, 14.315142, 12.365778, 13.355773, 14.527607, 14.408342, 13.75589, 13.348898, 11.802179, 12.80746, 11.666447, 13.209088, 11.164155, 11.698257, 13.695509, 11.938202, 14.021385, 13.524063, 14.518519, 12.975325, 13.174511, 12.94833, 9.438634]\n",
            "[35 15 13 12 16  8  9  1 69 17 10 31 24 59 27 29 22 45 57 60 55 47  0 62\n",
            " 38  2 32  5 21 49 44  3 42  7 30 26 56 68  6 66 19  4 67 58 11 20 54 50\n",
            " 23 36 64 18 25 37 61 46 53 34 40 14 28 63 48 52 65 51 41 43 33 39]\n",
            "夏の甲子園、新型コロナウイルスの影響で松商学園が不戦勝。\n",
            "35    新型コロナウイルスの感染拡大の影響で2月下旬から公式戦中断中のJリーグは19日に臨時実行委員...\n",
            "15                    プロ野球は本来の開幕日だった3月20日から練習試合が始まっている。\n",
            "13    約2週間前から杉内2軍投手コーチらのアドバイスを受け、ブルペンで10球ごとに新球へチェンジす...\n",
            "12                  巨人新外国人のサンチェス投手が、21日の練習試合DeNA戦に先発する。\n",
            "16                       21日は守護神から先発に転向した楽天松井の登板などに注目だ。\n",
            "8     巨人阿部慎之助2軍監督が20日、41歳の誕生日を迎えた。ジャイアンツ球場で行われた2軍練習前...\n",
            "9                            ブルペンデーで、ヤクルトの若手投手陣がアピールした。\n",
            "1     阪神は新型コロナウイルスの感染拡大でシーズン開幕が延期される中、野球以外でファンに向けて何が...\n",
            "69    その好発進も幻と化した。第1日終了後に第2ラウンド以降は無観客で行うと発表していたPGAは、...\n",
            "17    阪神福留孝介外野手（42）が本来の開幕予定日だった20日、ヤクルトとの練習試合（神宮）でハッ...\n",
            "10    先発の2年目清水は2回を無失点。3番手のドラフト4位大西は2回を完璧に抑えた。9回は、ソフト...\n",
            "31       J1湘南ベルマーレは、今季のJ1とJ2で降格なしとの特例が適用されることを「刺激」に変えた。\n",
            "24     Jリーグの村井満チェアマン（60）が19日、経営難に陥ったサガン鳥栖と協議中であることを認めた。\n",
            "59    米女子プロゴルフ協会（LPGA）は20日、新型コロナウイルスの感染拡大によりロッテ選手権（ハ...\n",
            "27    鳥栖は昨季、5億円以上の赤字を出し、今季も資金難に苦しんでいる。仮に「リーグ戦安定開催融資制...\n",
            "29    Jリーグ公式戦は中断中。現時点で札幌のホームゲームは、札幌ドームで週末に予定されていた2試合...\n",
            "22    日本サッカー協会（JFA）は20日、田嶋幸三会長（62）が14日に新型コロナウイルスを発症し...\n",
            "45    場所前には行きつけの整骨院に通い、曲がっていた背骨を矯正した。そのかいあってか、今場所も力強...\n",
            "57    新型コロナウイルスの感染拡大のため多くのスポーツイベントが中止となるなか、米男子ゴルフのタイ...\n",
            "60    また4月2～5日からの延期が決まっていたメジャー第1戦のANAインスピレーション（カリフォル...\n",
            "Name: 記事, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv_7AHlC-UvT",
        "outputId": "1796c43b-8991-4a2d-a923-2758e2644c5f"
      },
      "source": [
        "print(input_sentence)\n",
        "print('1回目学習後')\n",
        "print(data[sort[0:20]])\n",
        "print('距離再計算学習後')\n",
        "print(data[sort2[0:20]])\n",
        "print(sort2)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "夏の甲子園、新型コロナウイルスの影響で松商学園が不戦勝。\n",
            "1回目学習後\n",
            "1     阪神は新型コロナウイルスの感染拡大でシーズン開幕が延期される中、野球以外でファンに向けて何が...\n",
            "9                            ブルペンデーで、ヤクルトの若手投手陣がアピールした。\n",
            "16                       21日は守護神から先発に転向した楽天松井の登板などに注目だ。\n",
            "15                    プロ野球は本来の開幕日だった3月20日から練習試合が始まっている。\n",
            "35    新型コロナウイルスの感染拡大の影響で2月下旬から公式戦中断中のJリーグは19日に臨時実行委員...\n",
            "12                  巨人新外国人のサンチェス投手が、21日の練習試合DeNA戦に先発する。\n",
            "7     今年初の実戦打撃では5回1死一塁でバスターを決め、内野安打。実りある試合で弾みをつけた右腕は...\n",
            "8     巨人阿部慎之助2軍監督が20日、41歳の誕生日を迎えた。ジャイアンツ球場で行われた2軍練習前...\n",
            "13    約2週間前から杉内2軍投手コーチらのアドバイスを受け、ブルペンで10球ごとに新球へチェンジす...\n",
            "0     インターネットサイト「YouTube」の球団公式チャンネルが話題になっている。17日にはキャ...\n",
            "17    阪神福留孝介外野手（42）が本来の開幕予定日だった20日、ヤクルトとの練習試合（神宮）でハッ...\n",
            "2     企画発案から撮影に編集…。球団にとっても大きな負担となるが、矢野監督は「ファンがこういうのを...\n",
            "21    浜口は神奈川大から16年ドラフト1位で入団。1年目に10勝を挙げチームの日本シリーズ進出に貢...\n",
            "10    先発の2年目清水は2回を無失点。3番手のドラフト4位大西は2回を完璧に抑えた。9回は、ソフト...\n",
            "5     1回は直球と得意球のカットボールで押すも、2回から組み立てを変えてカーブやフォークボールを多...\n",
            "18    2回に左中間へ激走二塁打を放つと、その後も中前、右前と全方位に打ち分けた。新型コロナウイルス...\n",
            "14    日本流を落とし込みながら、投球では「これまでと同じアプローチでやっていくよ」と昨季韓国で17...\n",
            "31       J1湘南ベルマーレは、今季のJ1とJ2で降格なしとの特例が適用されることを「刺激」に変えた。\n",
            "28    北海道コンサドーレ札幌野々村芳和社長（47）が19日、新型コロナウイルスの影響でクラブが被る...\n",
            "59    米女子プロゴルフ協会（LPGA）は20日、新型コロナウイルスの感染拡大によりロッテ選手権（ハ...\n",
            "Name: 記事, dtype: object\n",
            "距離再計算学習後\n",
            "35    新型コロナウイルスの感染拡大の影響で2月下旬から公式戦中断中のJリーグは19日に臨時実行委員...\n",
            "15                    プロ野球は本来の開幕日だった3月20日から練習試合が始まっている。\n",
            "13    約2週間前から杉内2軍投手コーチらのアドバイスを受け、ブルペンで10球ごとに新球へチェンジす...\n",
            "12                  巨人新外国人のサンチェス投手が、21日の練習試合DeNA戦に先発する。\n",
            "16                       21日は守護神から先発に転向した楽天松井の登板などに注目だ。\n",
            "8     巨人阿部慎之助2軍監督が20日、41歳の誕生日を迎えた。ジャイアンツ球場で行われた2軍練習前...\n",
            "9                            ブルペンデーで、ヤクルトの若手投手陣がアピールした。\n",
            "1     阪神は新型コロナウイルスの感染拡大でシーズン開幕が延期される中、野球以外でファンに向けて何が...\n",
            "69    その好発進も幻と化した。第1日終了後に第2ラウンド以降は無観客で行うと発表していたPGAは、...\n",
            "17    阪神福留孝介外野手（42）が本来の開幕予定日だった20日、ヤクルトとの練習試合（神宮）でハッ...\n",
            "10    先発の2年目清水は2回を無失点。3番手のドラフト4位大西は2回を完璧に抑えた。9回は、ソフト...\n",
            "31       J1湘南ベルマーレは、今季のJ1とJ2で降格なしとの特例が適用されることを「刺激」に変えた。\n",
            "24     Jリーグの村井満チェアマン（60）が19日、経営難に陥ったサガン鳥栖と協議中であることを認めた。\n",
            "59    米女子プロゴルフ協会（LPGA）は20日、新型コロナウイルスの感染拡大によりロッテ選手権（ハ...\n",
            "27    鳥栖は昨季、5億円以上の赤字を出し、今季も資金難に苦しんでいる。仮に「リーグ戦安定開催融資制...\n",
            "29    Jリーグ公式戦は中断中。現時点で札幌のホームゲームは、札幌ドームで週末に予定されていた2試合...\n",
            "22    日本サッカー協会（JFA）は20日、田嶋幸三会長（62）が14日に新型コロナウイルスを発症し...\n",
            "45    場所前には行きつけの整骨院に通い、曲がっていた背骨を矯正した。そのかいあってか、今場所も力強...\n",
            "57    新型コロナウイルスの感染拡大のため多くのスポーツイベントが中止となるなか、米男子ゴルフのタイ...\n",
            "60    また4月2～5日からの延期が決まっていたメジャー第1戦のANAインスピレーション（カリフォル...\n",
            "Name: 記事, dtype: object\n",
            "[35 15 13 12 16  8  9  1 69 17 10 31 24 59 27 29 22 45 57 60 55 47  0 62\n",
            " 38  2 32  5 21 49 44  3 42  7 30 26 56 68  6 66 19  4 67 58 11 20 54 50\n",
            " 23 36 64 18 25 37 61 46 53 34 40 14 28 63 48 52 65 51 41 43 33 39]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49fPrvud3Qhl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yAbgiCoygD-",
        "outputId": "17353468-cf8a-4910-afce-92710202b9f7"
      },
      "source": [
        "dist = np.array(dist)\n",
        "dist2 = np.array(dist2)\n",
        "\n",
        "diff=dist-dist2\n",
        "diff_sort = np.argsort(diff)\n",
        "print(diff)\n",
        "print(diff_sort)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.33100128 -1.9859576  -0.16675472  1.0524111   0.32597256  0.3372059\n",
            "  0.30752754 -1.4317446   3.248457    1.0480204   1.9222631   0.24670982\n",
            "  4.4710054   5.658033   -1.0325613   4.125133    3.1640754   2.0231953\n",
            " -0.8919411   0.31537056  0.2776165  -0.00627422  1.833375    1.4643002\n",
            "  3.2371893   1.8020563   1.5231485   2.2485895  -0.93237305  3.440507\n",
            "  1.9279423   2.244667    1.7226362  -0.7992382   1.6035881   4.8973536\n",
            "  0.85266495  0.40476418  1.8620701   0.689991    2.9785738   1.3982887\n",
            "  4.5360603   0.78332806  4.6853275   2.6479301  -0.3881569   1.9897194\n",
            "  2.1863995   3.7105541   4.075322    1.183157    2.8485947   1.840311\n",
            "  3.2592163   3.8544884   4.528634    2.9846344   4.3827257   1.9814644\n",
            "  2.1597528   1.7987709   3.214388    2.5578575   1.0984211   0.9470444\n",
            "  3.1495132   1.2556133   1.3175936   6.1192255 ]\n",
            "[ 1  7 14 28 18 33 46  0  2 21 11 20  6 19  4  5 37 39 43 36 65  9  3 64\n",
            " 51 67 68 41 23 26 34 32 61 25 22 53 38 10 30 59 47 17 60 48 31 27 63 45\n",
            " 52 40 57 66 16 62 24  8 54 29 49 55 50 15 58 12 56 42 44 35 13 69]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "A8vsDwWYhIEV",
        "outputId": "8784a76b-fc5c-47e5-c64c-72adec7d030a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "max_cluster_num = 10\n",
        "\n",
        "# エルボー法で最適なクラスタ数を探索する\n",
        "sse = np.zeros((max_cluster_num, ))\n",
        "se = np.zeros((max_cluster_num, ))\n",
        "inertia = np.zeros((max_cluster_num, ))\n",
        "\n",
        "for i in range(max_cluster_num):\n",
        "  cluster_num = i+1\n",
        "  kmeans = KMeans(n_clusters = cluster_num)\n",
        "  pred = kmeans.fit_predict(cls)\n",
        "  inertia[i] = kmeans.inertia_\n",
        "\n",
        "  transforms = kmeans.transform(cls)\n",
        "  distances = np.zeros((cls.shape[0]))\n",
        "\n",
        "  for index in range(len(transforms)):\n",
        "    distances[index] = transforms[index, pred[index]]\n",
        "\n",
        "  se[i] = np.sum(distances)\n",
        "  sse[i] = np.sum(distances**2)\n",
        "\n",
        "a = np.arange(10)\n",
        "plt.plot(a, inertia)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FjbgMvcktw7",
        "outputId": "fc1643b5-fd28-4cb3-fd66-b5a361523aa8"
      },
      "source": [
        "c_num = 3\n",
        "pred_3 = KMeans(n_clusters = c_num).fit(cls)\n",
        "print(pred_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
            "       n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
            "       random_state=None, tol=0.0001, verbose=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq-tWc4fjT-4",
        "outputId": "da9d33b2-16fd-4aee-d0cf-4bfacd78967d"
      },
      "source": [
        "print(sse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1277.96008296  961.45446324  685.65841581  470.58130386  445.51222423\n",
            "  421.40599282  393.3658018   380.00818891  357.7934451   349.44024254]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "5tYcT76sSpp8",
        "outputId": "03090f4e-f548-4d7a-b74c-e4c8f967c5f9"
      },
      "source": [
        "from sklearn.decomposition import PCA  # 主成分分析器\n",
        "\n",
        "labels = pred_3.labels_\n",
        "\n",
        "labels = np.array(labels)\n",
        "colors = [\"r\" ,\"g\", \"b\"]\n",
        "colors = np.array(colors)\n",
        "print(labels)\n",
        "\n",
        "# 主成分分析の実行\n",
        "pca = PCA()\n",
        "pca.fit(cls)\n",
        "# データを主成分空間に写像 = 次元圧縮\n",
        "feature = pca.transform(cls)\n",
        "# print(labels.shape)\n",
        "# print(colors)\n",
        "# print(labels[line_inf])\n",
        "# print(colors[labels])\n",
        "# 第一主成分と第二主成分でプロットする\n",
        "# plt.figure(figsize=(8,8))\n",
        "plt.scatter(cls[line_inf, 0], cls[line_inf, 1], c=colors[labels])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Fh2IwplXRoYQ",
        "outputId": "c5ff0d9a-9e23-4936-8737-cda31f28a964"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(cls[line_inf, 0], cls[line_inf, 1], c=\"b\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCNQ4wUAx9If",
        "outputId": "54718724-c6d9-49c8-c4f7-be8156923738"
      },
      "source": [
        "print(cls[0][1])\n",
        "print(cls[0][2])\n",
        "print(cls[0][188])\n",
        "print(cls[0][189])\n",
        "print(cls[0][190])\n",
        "print(cls[0][191])\n",
        "print()\n",
        "print(input_w2v[0]) # 指定されたseq番号の単語特徴量"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.716723\n",
            "0.5509514\n",
            "-0.42319265\n",
            "0.56826204\n",
            "-0.81666684\n",
            "-0.6045787\n",
            "\n",
            "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.\n",
            " 11. 12. 13. 14. 15. 16. 17. 18. 19. 13. 20. 21.  7. 22. 11. 23. 24. 13.\n",
            " 25. 26. 27. 13. 28. 29. 30. 31. 29. 32. 17. 33. 19. 13. 20. 34. 35. 29.\n",
            " 36. 13. 37. 38. 39. 40. 41. 42.  7. 43. 44. 45. 46. 11. 47. 48. 49. 50.\n",
            "  7. 51. 52. 53. 54. 13. 55. 38. 15. 16. 56. 57. 20.  8.  7.  9. 10. 58.\n",
            " 59. 17. 25. 27.  7. 28. 31. 20. 47. 60. 61. 19. 58. 62. 63. 64. 65. 66.\n",
            " 29. 67. 39. 17.  1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji_EVDCvop9b",
        "outputId": "5a13a812-2f6e-4cc0-cc4d-3280f89de7fb"
      },
      "source": [
        "# Embedding後の各単語の特徴量\n",
        "\n",
        "# print(cls[7][1])    # padding(7は適当)\n",
        "# print(cls[7][23])   # padding\n",
        "# print(cls[7][186])  # 普通の単語\n",
        "# print(cls.shape)\n",
        "\n",
        "# [-0.7045435  -1.9852313   1.2020841   3.497596    0.21511011  0.503387\n",
        "#   2.4495409   6.1879616   3.6204557  -0.60159874  1.5071025   0.31605506\n",
        "#  -0.01150608 -0.44549415  1.4367023   1.9446754 ]\n",
        "# [-0.7045435  -1.9852313   1.2020841   3.497596    0.21511011  0.503387\n",
        "#   2.4495409   6.1879616   3.6204557  -0.60159874  1.5071025   0.31605506\n",
        "#  -0.01150608 -0.44549415  1.4367023   1.9446754 ]\n",
        "# [-2.4816272   1.236034    0.271418    0.5273086   0.22198428 -0.29935843\n",
        "#  -2.7889354   2.3024821   0.29362816 -0.9283478  -1.0853043   0.08451311\n",
        "#  -1.841284    0.57333696 -1.3197367  -0.17667502]\n",
        "# (70, 238, 16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.7045435  -1.9852313   1.2020841   3.497596    0.21511011  0.503387\n",
            "  2.4495409   6.1879616   3.6204557  -0.60159874  1.5071025   0.31605506\n",
            " -0.01150608 -0.44549415  1.4367023   1.9446754 ]\n",
            "[-0.7045435  -1.9852313   1.2020841   3.497596    0.21511011  0.503387\n",
            "  2.4495409   6.1879616   3.6204557  -0.60159874  1.5071025   0.31605506\n",
            " -0.01150608 -0.44549415  1.4367023   1.9446754 ]\n",
            "[-2.4816272   1.236034    0.271418    0.5273086   0.22198428 -0.29935843\n",
            " -2.7889354   2.3024821   0.29362816 -0.9283478  -1.0853043   0.08451311\n",
            " -1.841284    0.57333696 -1.3197367  -0.17667502]\n",
            "(70, 238, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUX21ormibD4",
        "outputId": "83360850-e7de-4607-8684-3e021d30d87e"
      },
      "source": [
        "\n",
        "c_num = 10\n",
        "km = KMeans(n_clusters=c_num).fit(cls)\n",
        "\n",
        "labels = km.labels_\n",
        "distance = np.min(km.transform(cls), axis=1)\n",
        "\n",
        "path_pd = '/drive/MyDrive/clustering.csv'\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df.to_csv(path_pd, mode='w', index=False)\n",
        "\n",
        "for i in range(c_num):\n",
        "  df = pd.DataFrame(columns=['Cluster #'])\n",
        "  df = df.append({'Cluster # ': i}, ignore_index=True)\n",
        "  df.to_csv(path_pd, mode='a', index=False)\n",
        "\n",
        "  c_index = np.array(np.where(labels==i)).reshape(-1)\n",
        "  c_distance=distance[c_index]\n",
        "  sorted_c_index = c_index[np.argsort(c_distance)]\n",
        "  print(sorted_c_index)\n",
        "\n",
        "  df = pd.DataFrame(columns=['文書番号', 'クラスタ中心からの距離', '文書'])\n",
        "\n",
        "  for j in sorted_c_index:\n",
        "    df = df.append({'文書番号':j, 'クラスタ中心からの距離':distance[j], '文書':data[j]}, ignore_index=True)\n",
        "\n",
        "  df.to_csv(path_pd, mode='a', index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17  0 19]\n",
            "[34 30 28 56 31 65 23 55]\n",
            "[37 33 32 25 38]\n",
            "[53 50 41 40 48 46 49 45 39 52  9]\n",
            "[68 60 69 58]\n",
            "[67 59 66 57 63 62 64 61]\n",
            "[16 12 15  1 13]\n",
            "[10  4  5  3  6 20 11  7  8  2 18 21 14]\n",
            "[36 24 35 26 29 27 22]\n",
            "[43 54 42 44 51 47]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD8fq3icjkqp",
        "outputId": "85d9f777-a6bf-44d6-fe73-428dee0bbc73"
      },
      "source": [
        "\n",
        "sentence_no = 10\n",
        "\n",
        "layer_name = 'layer_6x'\n",
        "model_tiral = Model(inputs=LSTM_AE.input, outputs=LSTM_AE.get_layer(layer_name).output)\n",
        "att_vis = model_trial.predict([input_w2v])\n",
        "\n",
        "# attentionの特徴量を成形(次元削減＆正規化)\n",
        "from sklearn import preprocessing\n",
        "\n",
        "att_vis = np.squeeze(att_vis)\n",
        "\n",
        "ss = preprocessing.MinMaxScaler()\n",
        "att_vis = ss.fit_transform(att_vis)\n",
        "\n",
        "# 文書番号をsentence_noに格納するとAttentionで色付けして出力\n",
        "for i in range(len(vec_data[sentence_no])):\n",
        "  grey = 200 - int(200 * att_vis[sentence_no][i])\n",
        "  print(\"\\033[38;2;\" + str(grey) + \";\" + str(grey) + \";\" + str(grey) + \"m\" + dictionary[vec_data[sentence_no][i]] + \"\\033[0m\", end = ' ')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;2;155;155;155m先発\u001b[0m \u001b[38;2;32;32;32mの\u001b[0m \u001b[38;2;114;114;114m2\u001b[0m \u001b[38;2;172;172;172m年\u001b[0m \u001b[38;2;164;164;164m目\u001b[0m \u001b[38;2;0;0;0m清水\u001b[0m \u001b[38;2;37;37;37mは\u001b[0m \u001b[38;2;47;47;47m2\u001b[0m \u001b[38;2;167;167;167m回\u001b[0m \u001b[38;2;17;17;17mを\u001b[0m \u001b[38;2;19;19;19m無\u001b[0m \u001b[38;2;36;36;36m失点\u001b[0m \u001b[38;2;50;50;50m。\u001b[0m \u001b[38;2;81;81;81m3\u001b[0m \u001b[38;2;23;23;23m番手\u001b[0m \u001b[38;2;193;193;193mの\u001b[0m \u001b[38;2;125;125;125mドラフト\u001b[0m \u001b[38;2;4;4;4m4\u001b[0m \u001b[38;2;58;58;58m位\u001b[0m \u001b[38;2;55;55;55m大西\u001b[0m \u001b[38;2;150;150;150mは\u001b[0m \u001b[38;2;186;186;186m2\u001b[0m \u001b[38;2;9;9;9m回\u001b[0m \u001b[38;2;133;133;133mを\u001b[0m \u001b[38;2;194;194;194m完璧\u001b[0m \u001b[38;2;97;97;97mに\u001b[0m \u001b[38;2;143;143;143m抑え\u001b[0m \u001b[38;2;49;49;49mた\u001b[0m \u001b[38;2;183;183;183m。\u001b[0m \u001b[38;2;172;172;172m9\u001b[0m \u001b[38;2;101;101;101m回\u001b[0m \u001b[38;2;71;71;71mは\u001b[0m \u001b[38;2;90;90;90m、\u001b[0m \u001b[38;2;163;163;163mソフト\u001b[0m \u001b[38;2;31;31;31mバンク\u001b[0m \u001b[38;2;52;52;52mの\u001b[0m \u001b[38;2;82;82;82m育成\u001b[0m \u001b[38;2;142;142;142mから\u001b[0m \u001b[38;2;0;0;0m新\u001b[0m \u001b[38;2;11;11;11m加入\u001b[0m \u001b[38;2;95;95;95mし\u001b[0m \u001b[38;2;147;147;147mた\u001b[0m \u001b[38;2;197;197;197m左腕\u001b[0m \u001b[38;2;184;184;184m長谷川\u001b[0m \u001b[38;2;29;29;29mが\u001b[0m \u001b[38;2;44;44;44mわずか\u001b[0m \u001b[38;2;59;59;59m7\u001b[0m \u001b[38;2;50;50;50m球\u001b[0m \u001b[38;2;24;24;24mで\u001b[0m \u001b[38;2;94;94;94m3\u001b[0m \u001b[38;2;50;50;50m者\u001b[0m \u001b[38;2;109;109;109m凡退\u001b[0m \u001b[38;2;84;84;84m。\u001b[0m \u001b[38;2;189;189;189m\n",
            "\u001b[0m "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1hfiLj9YJF5",
        "outputId": "80da4677-b444-45fb-96d1-26ec0f41981d"
      },
      "source": [
        "\n",
        "# loss_list = []\n",
        "# loss_list.extend(result.history['loss'])\n",
        "print(loss_list)\n",
        "print(np.array(loss_list).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6.401927471160889, 6.2403106689453125, 6.249670028686523, 6.1136064529418945, 6.006901264190674, 5.844775676727295, 5.685949802398682, 5.563150405883789, 5.355846405029297, 5.271862030029297, 5.838228225708008, 5.78462553024292, 5.745098114013672, 5.640793800354004, 5.539499282836914, 5.452040195465088, 5.288724422454834, 5.2187089920043945, 4.987415790557861, 4.878882884979248, 5.788774013519287, 5.825491428375244, 5.784441947937012, 5.609563827514648, 5.561429977416992, 5.385857582092285, 5.309630870819092, 5.200871467590332, 5.007061004638672, 4.9111456871032715, 6.0971245765686035, 6.014912128448486, 5.90290641784668, 5.808718681335449, 5.729406833648682, 5.558047294616699, 5.390603542327881, 5.290581703186035, 5.1156229972839355, 4.956219673156738, 5.50606632232666, 5.456737995147705, 5.4428606033325195, 5.317091941833496, 5.219941139221191, 5.152580261230469, 4.976334095001221, 4.87598991394043, 4.766567707061768, 4.615355968475342, 6.036299228668213, 5.925957202911377, 5.859058856964111, 5.801535129547119, 5.697564125061035, 5.5607757568359375, 5.374264717102051, 5.311306476593018, 5.13429594039917, 5.061938762664795, 6.139221668243408, 6.045865058898926, 6.00897216796875, 5.8367838859558105, 5.745011806488037, 5.602537155151367, 5.561801433563232, 5.340614318847656, 5.2450270652771, 5.088486194610596, 5.836386680603027, 5.8351945877075195, 5.727233409881592, 5.575467586517334, 5.513097763061523, 5.420601844787598, 5.255895614624023, 5.121553421020508, 4.988361835479736, 4.899979591369629, 5.79635763168335, 5.730280876159668, 5.72457218170166, 5.665992736816406, 5.493012428283691, 5.4250030517578125, 5.32063102722168, 5.112367630004883, 5.033034324645996, 4.852021217346191, 6.059788703918457, 6.005484104156494, 5.9559526443481445, 5.7981061935424805, 5.648787498474121, 5.481705665588379, 5.392323017120361, 5.203197479248047, 5.029586315155029, 4.982234001159668]\n",
            "(100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf8nWDn1lhIk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01086ecf-0454-418b-9bef-2d2dffef0162"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "epoch=1000\n",
        "\n",
        "# out_loss_list = []\n",
        "# out_2_loss_list = []\n",
        "# out_acc_list = []\n",
        "# out_2_acc_list = []\n",
        "\n",
        "fig=plt.figure()\n",
        "plt.plot(range(1, epoch+1),\n",
        "         out_acc_list)\n",
        "plt.title('Sports acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "fig=plt.figure()\n",
        "plt.plot(range(1, epoch+1),\n",
        "         out_2_acc_list)\n",
        "plt.title('word acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "fig=plt.figure()\n",
        "plt.plot(range(1, epoch+1),\n",
        "         loss_list)\n",
        "plt.title('Total loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "fig=plt.figure()\n",
        "plt.plot(range(1, epoch+1),\n",
        "         out_loss_list)\n",
        "plt.title('Sports loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "fig=plt.figure()\n",
        "plt.plot(range(1, epoch+1),\n",
        "         out_2_loss_list)\n",
        "plt.title('word loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO-ZZB64YGXn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}